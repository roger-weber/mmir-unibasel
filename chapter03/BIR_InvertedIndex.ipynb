{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIR retrieval with inverted files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer & Set of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'simple', 'function', 'test'}\n",
      "{'a', 'test', 'for', 'this', 'is', 'simple', 'function'}\n"
     ]
    }
   ],
   "source": [
    "from utils import analyzer\n",
    "\n",
    "print(analyzer.set_of_words(\"this is a simple test for this function\", remove_stopwords = True))\n",
    "print(analyzer.set_of_words(\"this is a simple test for this function\", remove_stopwords = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "class Feedback:\n",
    "    \"\"\"\n",
    "        Collects feedback for documents and provides functions to check if \n",
    "        document is assessed, relevant or not relevant.\n",
    "    \"\"\"\n",
    "    def __init__(self, assessment_func: Callable[[int], bool] = None):\n",
    "        self.assessment_func = assessment_func\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.assessed = set()\n",
    "        self.relevant = set()\n",
    "    \n",
    "    def is_initial_step(self) -> bool:\n",
    "        return len(self.assessed) == 0\n",
    "\n",
    "    def assess(self, doc_id: int) -> None:\n",
    "        self.assessed.add(doc_id)\n",
    "        if not self.assessment_func or self.assessment_func(doc_id):\n",
    "            self.relevant.add(doc_id)\n",
    "        \n",
    "    def is_relevant(self, doc_id: int) -> bool:\n",
    "        return doc_id in self.relevant\n",
    "    \n",
    "    def is_assessed(self, doc_id: int) -> bool:\n",
    "        return doc_id in self.assessed\n",
    "    \n",
    "    def is_not_relevant(self, doc_id: int) -> bool:\n",
    "        return (doc_id in self.assessed) and (doc_id not in self.relevant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopKList class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappop, heappush\n",
    "\n",
    "class TopKList:\n",
    "    \"\"\"\n",
    "        Maintains a list of top-k documents. Initializer accepts\n",
    "        a list of tuples (term, weight) to provide information about\n",
    "        weights used by retrieval model. Implements the iter() interface.\n",
    "        Takes an optional predicate(doc_id: int) function to filter documents\n",
    "        before returning them. \n",
    "    \"\"\"\n",
    "    def __init__(self, k: int, term_weights: list[tuple[str,float]] = None, predicate: Callable[[int], bool] = None):\n",
    "        self.docs_heap = []\n",
    "        self.k = k\n",
    "        self.predicate = predicate\n",
    "        self.results = []\n",
    "        if term_weights:\n",
    "            self.term_weights = term_weights\n",
    "            self.terms = [term for term, _ in self.term_weights]\n",
    "            self.weights = dict(self.term_weights)\n",
    "    \n",
    "    def add(self, doc_id: int, score: float):\n",
    "        heappush(self.docs_heap, (-score, doc_id, {'id': doc_id, 'score': score}))\n",
    "        # optional (infrequent) pruning if heap grows too large\n",
    "\n",
    "    def __iter__(self):\n",
    "        # do we already have the results?\n",
    "        for entry in self.results:\n",
    "            yield entry\n",
    "        # produce more results (if necessary and available)\n",
    "        rank = len(self.results)\n",
    "        while rank < self.k and len(self.docs_heap) > 0:\n",
    "            entry = heappop(self.docs_heap)[2]\n",
    "            if self.predicate == None or self.predicate(entry['id']):\n",
    "                rank += 1\n",
    "                entry['rank'] = rank\n",
    "                self.results.append(entry)\n",
    "                yield entry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIR Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Base Retriever Class\n",
    "\n",
    "* `n_docs: int`: number of documents added to index\n",
    "* `documents dict[int, dict{'id', 'vector'}]`: collection of documents as dictionary with doc_id as key. Each document is a dictionary with the properties from the dataset and additional properties for the retrieval:\n",
    "  - `id` hold the document id as generated when loading the document; corresponds to the key in documents\n",
    "  - `vector` holds the term freqeuncies as dictionary (key=term, value=term frequency)\n",
    "* `vocabulary: dict[term, int]`: vocabularoy of collection with term as keys and document frequency as values\n",
    "* `index: dict[term, list[int]]`: inverted index mapping terms to postings. Postings contain doc_id sorted by doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class BIRModel:\n",
    "    \"\"\"\n",
    "        Generic class for the evaluation of the BIR model, inherited by the document-at-a-time (DAAT) and \n",
    "        term-at-a-time (TAAT) models. \n",
    "    \"\"\"\n",
    "    def __init__(self, collection: list[dict] = None, remove_stopwords: bool = True):\n",
    "        self.build_index(collection or [], remove_stopwords)\n",
    "    \n",
    "    def _add_document(self, doc: dict):\n",
    "        self.n_docs += 1\n",
    "        doc_id = doc['id'] = self.n_docs\n",
    "        self.documents[doc_id] = doc\n",
    "        # create vector from str-properties\n",
    "        text = ' '.join([value for key, value in doc.items() if type(value) == str])\n",
    "        vector = doc['vector'] = analyzer.set_of_words(text, remove_stopwords = self.remove_stopwords)\n",
    "        # add to vocabulary and postings\n",
    "        for term in vector:\n",
    "            self.vocabulary[term] += 1\n",
    "            self.index[term].append(doc_id)\n",
    "    \n",
    "    def build_index(self, collection: list[dict], remove_stopwords: bool = True):\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.n_docs = 0\n",
    "        self.documents = {}\n",
    "        self.index = defaultdict(list)\n",
    "        self.vocabulary = defaultdict(int)\n",
    "        # load all documents\n",
    "        for doc in collection:\n",
    "            self._add_document(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of c_j-weights\n",
    "Two variants for initial step and feedback step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BIRModel(BIRModel):\n",
    "    def cj_weight(self, term: str, feedback: Feedback):\n",
    "        doc_freq = len(self.index[term])\n",
    "        if feedback.is_initial_step():\n",
    "            rj = 0.5\n",
    "            nj = (doc_freq + 0.5) / (len(self.documents) + 1)\n",
    "        else:\n",
    "            # get postings as set to siplify calculations in Python\n",
    "            docs = set(self.index[term])\n",
    "            # number of assessed relevant documents which have the term\n",
    "            lj, L = len(feedback.relevant & docs), len(feedback.relevant)\n",
    "            # number of assessed documents which have the term\n",
    "            kj, K = len(feedback.assessed & docs), len(feedback.assessed)\n",
    "            # calculate rj and nj\n",
    "            rj = (lj + 0.5) / (L + 1)\n",
    "            nj = (kj - lj + 0.5) / (K - L + 1)\n",
    "        return math.log(rj / (1 - rj) * (1 - nj) / nj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term & document filtering with options\n",
    "Pruning of terms and documents based on the following settings:\n",
    "- `PRUNE_NEGATIVE_WEIGHTS: bool = False`, set this property to True to remove terms with negative weights\n",
    "- `PRUNE_WEIGHT_THRESHOLD: bool  = False`, set this property to remove terms with absolute weights smaller than this value\n",
    "- `PRUNE_TOPK: bool | int = False`, set this property to select top-k weights based on absolute values\n",
    "- `PRUNE_NON_RELEVANT: bool = False`, set this property to true to prune non-relevant documents from result list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel(BIRModel):\n",
    "    # set this property to True to remove terms with negative weights\n",
    "    PRUNE_NEGATIVE_WEIGHTS = False\n",
    "\n",
    "    # set this property to remove terms with absolute weights smaller than this value\n",
    "    PRUNE_WEIGHT_THRESHOLD  = False\n",
    "\n",
    "    # set this property to select top-k weights based on absolute values\n",
    "    PRUNE_TOPK = False\n",
    "\n",
    "    # set this property to true to prune non-relevant documents from result list\n",
    "    PRUNE_NON_RELEVANT = False\n",
    "\n",
    "    def query_weights(self, terms: set[str], feedback: Feedback) -> list[tuple[str,float]]:\n",
    "        # remove terms not in vocabulary\n",
    "        terms = list(filter(lambda t: t in self.vocabulary, terms))\n",
    "        # calculate weigths and produce tuples (term, weight)\n",
    "        term_weights = list(map(lambda t: (t, self.cj_weight(t, feedback)), terms))\n",
    "        # filter terms with negative weights\n",
    "        if self.PRUNE_NEGATIVE_WEIGHTS:\n",
    "            term_weights = list(filter(lambda t: t[1] >= 0, term_weights))\n",
    "        # filter terms with small absolute weights\n",
    "        if self.PRUNE_WEIGHT_THRESHOLD:\n",
    "            term_weights = list(filter(lambda t: abs(t[1]) > self.PRUNE_WEIGHT_THRESHOLD, term_weights))\n",
    "        # select top-k terms based on absolute values\n",
    "        if self.PRUNE_TOPK:\n",
    "            term_weights = sorted(term_weights, key = lambda t: (-abs(t[1]),len(self.index[t[0]]),t[0]))[:self.PRUNE_TOPK]\n",
    "        return term_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-at-a-time (DAAT) for BIR Model\n",
    "The implementation of DAAT for the BIR model uses sorted postings and processes postings in ascending order of the document IDs (see Or-implementation of Boolean model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel_DAAT(BIRModel):\n",
    "    \"\"\"\n",
    "        Implements the DAAT model for the BIR model using inverted index method.\n",
    "    \"\"\"\n",
    "    def search(self, query: str, k: int, feedback: Feedback, predicate: Callable[[int], bool] = None, selected_docs: set[int] = None) -> TopKList:\n",
    "        query_vector = analyzer.set_of_words(query)\n",
    "\n",
    "        # filter terms and obtain c_j-weights for terms in order of their importance \n",
    "        term_weights = self.query_weights(query_vector, feedback)\n",
    "        \n",
    "        # get iterators for each term and fetch first posting\n",
    "        iters = [iter(self.index[term]) for (term, _) in term_weights]\n",
    "        nexts = [next(iter, None) for iter in iters]\n",
    "\n",
    "        # keep track of all retrieved documents and their score; stored as tuples (doc_id, score)\n",
    "        topk = TopKList(k, term_weights, predicate)\n",
    "\n",
    "        # iterate through all streams and calculate score for smallest doc id\n",
    "        while not all(e is None for e in nexts):\n",
    "            # get smallest value from nexts, ignoring None values\n",
    "            smallest = min(nexts, key = lambda x: x or math.inf)\n",
    "            # if we have feedback, make sure document is either relevant or not assessed so far; if we have selected_docs, make sure document is in it\n",
    "            if not(self.PRUNE_NON_RELEVANT and feedback.is_not_relevant(smallest)) and (selected_docs == None or smallest in selected_docs):\n",
    "                # if so, add it to topk\n",
    "                score = sum([term_weights[i][1] for i in range(len(nexts)) if nexts[i] == smallest])\n",
    "                topk.add(smallest, score)\n",
    "            # for each entry in nexts, fetch next item if entry equals smallest\n",
    "            for i, e in enumerate(nexts):\n",
    "                if e is smallest:\n",
    "                    nexts[i] = next(iters[i], None)\n",
    "        \n",
    "        # finished, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-at-a-time for BIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel_TAAT(BIRModel):\n",
    "    \"\"\"\n",
    "        Implements the TAAT model for the BIR model using inverted index method.\n",
    "    \"\"\"\n",
    "    def search(self, query: str, k: int, feedback: Feedback, predicate: Callable[[int], bool] = None, selected_docs: set[int] = None) -> TopKList:\n",
    "        query_vector = analyzer.set_of_words(query)\n",
    "\n",
    "        # filter terms and obtain c_j-weights for terms in order of their importance \n",
    "        term_weights = self.query_weights(query_vector, feedback)\n",
    "        doc_scores = defaultdict(float)\n",
    "\n",
    "        # iterate over terms and fetch postings\n",
    "        for (term, weight) in term_weights:\n",
    "            for doc_id in self.index[term]:\n",
    "                # check if it is either not assessed or relevant; check if doc_id is selected_docs (if given)\n",
    "                if not(self.PRUNE_NON_RELEVANT and feedback.is_not_relevant(doc_id)) and (selected_docs == None or doc_id in selected_docs):\n",
    "                    doc_scores[doc_id] += weight\n",
    "\n",
    "        # we do not need a full sort of doc_scores, but can use the heap in TopKList\n",
    "        topk = TopKList(k, term_weights, predicate)\n",
    "        for doc_id, score in doc_scores.items():\n",
    "                topk.add(doc_id, score)\n",
    "        \n",
    "        # finisheds, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running some examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1b63fa4a0f436b96495dbe6566635d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(options=('animals', 'imdb movies'), value='animals'), Dropdown(options=('document-at-a…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "opt_dataset = widgets.Dropdown(options=['animals', 'imdb movies'])\n",
    "opt_strategy = widgets.Dropdown(options=['document-at-a-time', 'term-at-a-time'])\n",
    "display(widgets.HBox([opt_dataset, opt_strategy]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import table\n",
    "from datasets import animals as animals_docs, imdb as imdb_docs\n",
    "import random\n",
    "\n",
    "def build_index_for_selection(dataset, strategy):\n",
    "    global retriever, collection, queries, predicates, selections, assessments\n",
    " \n",
    "    # select the implementation of the retrieval model\n",
    "    if strategy == 'document-at-a-time':\n",
    "        retriever = BIRModel_DAAT()\n",
    "    else:\n",
    "        retriever = BIRModel_TAAT()\n",
    "\n",
    "    # select the dataset and define feedback function, queries, predicates, and selections\n",
    "    if dataset == 'animals':\n",
    "        collection = animals_docs\n",
    "        assessments = {\n",
    "            'random': lambda id: random.random() < 0.8,\n",
    "            'id < 20': lambda id: id < 20,\n",
    "        }\n",
    "        queries = [\n",
    "            'cat dog',\n",
    "            'horse bird',\n",
    "            'cat dog horse bird'\n",
    "        ]\n",
    "        predicates = {\n",
    "            'even doc ids': lambda id: id % 2 == 0,\n",
    "            'odd doc ids': lambda id: id % 2 == 1,\n",
    "        }\n",
    "        selections = {\n",
    "            'doc<10': list(range(10)),\n",
    "        }\n",
    "    elif dataset == 'imdb movies':\n",
    "        collection = imdb_docs\n",
    "        assessments = {\n",
    "            'top-100': lambda id: id < 100,\n",
    "            'star in title': lambda id: 'star' in retriever.documents[id]['title'].lower(),\n",
    "            'morgan in actor': lambda id: 'morgan' in retriever.documents[id]['actors'].lower(),\n",
    "            'comedy in genre': lambda id: 'comedy' in retriever.documents[id]['genre'].lower(),\n",
    "        }\n",
    "        queries = [\n",
    "            'star wars', \n",
    "            'drama morgan freeman', \n",
    "            'comedy'\n",
    "        ]\n",
    "        predicates = {\n",
    "            'year < 1990': lambda id: retriever.documents[id]['year'] < 1990,\n",
    "            'year >= 1990': lambda id: retriever.documents[id]['year'] >= 1990,\n",
    "        }\n",
    "        selections = {\n",
    "            'top-100': list(range(100)),\n",
    "            'top-250': list(range(250)),\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"to be implemented\")\n",
    "\n",
    "    # build index\n",
    "    retriever.build_index(collection.load())\n",
    "\n",
    "build_index_for_selection(opt_dataset.value, opt_strategy.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   id | text                                                                                                            |\n",
      "|------|-----------------------------------------------------------------------------------------------------------------|\n",
      "|    1 | ostrich                                                                                                         |\n",
      "|    2 | horse horse rabit rabit rabit rabit rabit bear bear bear bear bear lion lion donkey donkey donkey donkey donkey |\n",
      "|    3 | dog dog dog                                                                                                     |\n",
      "|    4 | ostrich ostrich ostrich ostrich ostrich bee bee bee bee bee ant ant ant snake snake snake                       |\n",
      "|    5 | dog dog dog dog dog cat cat cat donkey donkey donkey donkey                                                     |\n",
      "|    6 | bear bear bear bear bear wale wale                                                                              |\n",
      "|    7 | dog dog dog cat cat cat cat horse horse horse horse horse rabit rabit rabit bird bird bird bird bird wale wale  |\n",
      "|    8 | dog dog dog cat cat cat cat bird bird bird bird bird                                                            |\n",
      "|    9 | dog dog rabit rabit                                                                                             |\n",
      "|   10 | bird bird                                                                                                       |\n",
      "\n",
      "| term    |   df | posting                                                                                                             |\n",
      "|---------|------|---------------------------------------------------------------------------------------------------------------------|\n",
      "| dog     |   30 | [3, 5, 7, 8, 9, 12, 13, 19, 20, 26, 28, 40, 44, 45, 47, 54, 57, 62, 68, 72, 76, 77, 81, 85, 88, 89, 95, 96, 97, 98] |\n",
      "| cat     |   25 | [5, 7, 8, 11, 18, 22, 30, 35, 37, 40, 44, 48, 50, 60, 68, 69, 70, 72, 77, 81, 83, 87, 94, 97, 98]                   |\n",
      "| horse   |   22 | [2, 7, 13, 18, 21, 22, 23, 29, 35, 37, 42, 44, 52, 62, 67, 68, 69, 76, 79, 94, 95, 98]                              |\n",
      "| rabit   |   20 | [2, 7, 9, 14, 15, 27, 33, 50, 52, 54, 68, 72, 77, 79, 82, 86, 95, 97, 98, 99]                                       |\n",
      "| ostrich |   18 | [1, 4, 11, 14, 23, 45, 51, 52, 55, 61, 63, 66, 72, 81, 82, 84, 89, 90]                                              |\n",
      "| bear    |   17 | [2, 6, 13, 20, 23, 24, 33, 38, 41, 49, 53, 58, 59, 61, 76, 81, 83]                                                  |\n",
      "| tiger   |   15 | [17, 19, 20, 36, 39, 48, 51, 53, 61, 64, 66, 82, 90, 92, 97]                                                        |\n",
      "| lion    |   14 | [2, 15, 22, 33, 36, 37, 40, 59, 64, 85, 89, 93, 95, 96]                                                             |\n",
      "| bird    |   14 | [7, 8, 10, 17, 19, 32, 35, 48, 50, 58, 60, 75, 76, 91]                                                              |\n",
      "| donkey  |   13 | [2, 5, 20, 44, 48, 59, 62, 64, 65, 72, 75, 98, 100]                                                                 |\n",
      "| ant     |   12 | [4, 14, 15, 38, 41, 54, 56, 60, 67, 74, 79, 100]                                                                    |\n",
      "| bee     |   12 | [4, 34, 40, 52, 57, 68, 74, 79, 86, 87, 92, 95]                                                                     |\n",
      "| wale    |   11 | [6, 7, 22, 37, 45, 52, 56, 57, 68, 72, 97]                                                                          |\n",
      "| fly     |   11 | [14, 28, 31, 42, 52, 57, 64, 66, 79, 91, 94]                                                                        |\n",
      "| snake   |   10 | [4, 28, 46, 50, 53, 61, 65, 67, 71, 77]                                                                             |\n",
      "| fish    |    6 | [16, 25, 43, 73, 78, 80]                                                                                            |\n",
      "\n",
      "100 documents in collection\n",
      "16 distinct terms in collection\n",
      "250 postings\n"
     ]
    }
   ],
   "source": [
    "table.print([collection.format(doc) for doc in retriever.documents.values()], collection.headers(), max_rows = 10)\n",
    "table.print(sorted([[term, df, retriever.index[term]] for term, df in retriever.vocabulary.items()], key=lambda x: -x[1]), ['term', 'df', 'posting'], max_rows=20)\n",
    "\n",
    "print(f'{len(retriever.documents)} documents in collection')\n",
    "print(f'{len(retriever.vocabulary)} distinct terms in collection')\n",
    "print('{count} postings'.format(count=sum([len(postings) for postings in retriever.index.values()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty printing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feedback(feedback: Feedback, func: str, text: str = 'feedback'):\n",
    "    info = \", \".join([('+' if feedback.is_relevant(doc_id) else '-') + str(doc_id) for doc_id in sorted(feedback.assessed, key=lambda doc_id: (not feedback.is_relevant(doc_id), doc_id))])\n",
    "    print(f'{text} ({func}): {info}')\n",
    "\n",
    "def print_topk(topk: TopKList, feedback: Feedback):\n",
    "    list = []\n",
    "    for entry in topk:\n",
    "        list.append(collection.format(retriever.documents[entry['id']], [\n",
    "            '+' if feedback.is_relevant(entry['id']) else '-' if feedback.is_assessed(entry['id']) else ' ',\n",
    "            entry['rank'],\n",
    "            round(entry['score'], 2)\n",
    "        ]))\n",
    "    table.print(list, collection.headers('rel', 'rank', 'score'), max_rows=len(list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feedback(feedback, topk, n_feedback):\n",
    "    for entry in topk:\n",
    "        if n_feedback <= 0: return\n",
    "        if feedback.is_assessed(entry['id']): continue\n",
    "        feedback.assess(entry['id'])\n",
    "        n_feedback -= 1\n",
    "    for doc_id in filter(lambda doc_id: not feedback.is_assessed(doc_id), retriever.documents.keys()):\n",
    "        if n_feedback <= 0: return\n",
    "        if feedback.is_assessed(doc_id): continue\n",
    "        feedback.assess(doc_id)\n",
    "        n_feedback -= 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search with feedback iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3627b7adc6684a0f88d3cedde7a0f828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(options=('animals', 'imdb movies'), value='animals'), Dropdown(options=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbfe0cc0ada444629de0c768dac0ff24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid #eeeeee', border_left='1px solid #eeeeee', border_right='1px sol…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from functools import reduce\n",
    "\n",
    "feedback = Feedback()\n",
    "\n",
    "def run_query(query: str, k: int, assessment: str, predicate: str, selection: str, n_feedback: int):\n",
    "    global topk\n",
    "    feedback.assessment_func = assessments.get(assessment, None)\n",
    "    print_feedback(feedback, assessment)\n",
    "    print()\n",
    "    topk = retriever.search(query, k, feedback=feedback, predicate=predicates.get(predicate, None), selected_docs=selections.get(selection, None))\n",
    "    add_feedback(feedback, topk, n_feedback)\n",
    "    print_topk(topk, feedback)\n",
    "    for term in sorted(topk.weights.keys(), key = lambda term: -topk.weights[term]):\n",
    "        print(term.rjust(16), topk.weights[term])\n",
    "    print_feedback(feedback, assessment, \"\\nnext feedback\")\n",
    "\n",
    "def on_next(btn):\n",
    "    retriever.PRUNE_NEGATIVE_WEIGHTS = opt_neg.value\n",
    "    retriever.PRUNE_WEIGHT_THRESHOLD = opt_small.value and 0.5\n",
    "    retriever.PRUNE_TOPK = opt_topk.value and 10\n",
    "    retriever.PRUNE_NON_RELEVANT = opt_nonrel.value\n",
    "    if opt_expand.value:\n",
    "        query_text = f_query.value + ' ' + ' '.join(reduce(lambda terms, doc_id: terms | retriever.documents[doc_id]['vector'], feedback.relevant, set()))\n",
    "    else:\n",
    "        query_text = f_query.value\n",
    "    with out:\n",
    "        clear_output()\n",
    "        print(query_text)\n",
    "        run_query(query_text, 20, f_assessment.value, f_predicate.value, f_selection.value, f_feedback.value)\n",
    "\n",
    "# remove handlers if we re-execute this cell, need to do this before we overwrite function\n",
    "if 'on_start' in globals():\n",
    "    f_query.unobserve(on_start, 'value')\n",
    "def on_start(btn):\n",
    "    feedback.clear()\n",
    "    on_next(btn)\n",
    "\n",
    "# remove handlers if we re-execute this cell, need to do this before we overwrite function\n",
    "if 'rebuild_dataset' in globals():\n",
    "    opt_strategy.unobserve(rebuild_dataset, 'value')\n",
    "    opt_dataset.unobserve(rebuild_dataset, 'value')\n",
    "def rebuild_dataset(*args):\n",
    "    build_index_for_selection(opt_dataset.value, opt_strategy.value)\n",
    "    f_query.options = list(queries)\n",
    "    f_predicate.options = ['<none>'] + list(predicates.keys())\n",
    "    f_selection.options = ['<none>'] + list(selections.keys())\n",
    "    on_start(None)\n",
    "\n",
    "# build the dataset widgets\n",
    "form_data = widgets.HBox([opt_dataset, opt_strategy], layout = {'margin': '0px 0px 20px'})\n",
    "opt_dataset.observe(rebuild_dataset, 'value')\n",
    "opt_strategy.observe(rebuild_dataset, 'value')\n",
    "\n",
    "# buttons\n",
    "btn_start = widgets.Button(description=' start', icon='play')\n",
    "btn_start.on_click(on_start)\n",
    "btn_next = widgets.Button(description=' next', icon='step-forward')\n",
    "btn_next.on_click(on_next)\n",
    "buttons = widgets.HBox([btn_start, btn_next])\n",
    "\n",
    "# query left side\n",
    "f_query = widgets.Dropdown(description='query',options=list(queries))\n",
    "f_assessment = widgets.Dropdown(description='assessment',options=['<none>'] + list(assessments.keys()))\n",
    "f_feedback = widgets.IntSlider(description='feedback', min=5, max=50, step=5, value=5)\n",
    "f_predicate = widgets.Dropdown(description='predicate',options=['<none>'] + list(predicates.keys()))\n",
    "f_selection = widgets.Dropdown(description='selection',options=['<none>'] + list(selections.keys()))\n",
    "left = widgets.VBox([f_query, f_assessment, f_feedback, f_predicate, f_selection])\n",
    "f_query.observe(on_start, 'value')\n",
    "\n",
    "# options right side\n",
    "opt_neg = widgets.Checkbox(value=False, description='prune negative weights')\n",
    "opt_small = widgets.Checkbox(value=False, description='prune small weights (abs < 0.5)')\n",
    "opt_topk = widgets.Checkbox(value=False, description='keep top 10 weights')\n",
    "opt_nonrel = widgets.Checkbox(value=False, description='prune non relevant documents')\n",
    "opt_expand = widgets.Checkbox(value=False, description='expand query with feedback')\n",
    "right = widgets.VBox([opt_neg, opt_small, opt_topk, opt_nonrel, opt_expand])\n",
    "\n",
    "# display the dialog object\n",
    "display(widgets.VBox([form_data, buttons, widgets.HBox([left, right], layout={'margin': '20px'})]))\n",
    "\n",
    "# capture output with this widget\n",
    "out = widgets.Output(layout={'border': '1px solid #eeeeee', 'height': '500px', 'overflow': 'auto', 'padding': '0px 0px 0px 10px'})\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
