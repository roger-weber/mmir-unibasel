{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIR retrieval with inverted files\n",
    "\n",
    "## Helper functions for the BIRModel\n",
    "\n",
    "### Feedback class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedback:\n",
    "    \"\"\"\n",
    "        Collects feedback for documents and provides\n",
    "        functions to check if document is assessed,\n",
    "        relevant or not relevant.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.assessed = set()\n",
    "        self.relevant = set()\n",
    "    \n",
    "    def is_initial_step(self) -> bool:\n",
    "        return len(self.assessed) == 0\n",
    "\n",
    "    def add(self, doc: int, relevant: bool) -> None:\n",
    "        self.assessed.add(doc)\n",
    "        if relevant:\n",
    "            self.relevant.add(doc)\n",
    "        \n",
    "    def is_relevant(self, doc: int) -> bool:\n",
    "        return doc in self.relevant\n",
    "    \n",
    "    def is_assessed(self, doc: int) -> bool:\n",
    "        return doc in self.assessed\n",
    "    \n",
    "    def is_not_relevant(self, doc: int) -> bool:\n",
    "        return (doc in self.assessed) and (doc not in self.relevant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopKList class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappop, heappush, nsmallest\n",
    "from typing import Callable\n",
    "\n",
    "class TopKList:\n",
    "    \"\"\"\n",
    "        Maintains a list of top-k documents. Initializer accepts\n",
    "        a list of tuples (term, weight) to provide information about\n",
    "        weights used by retrieval model. Implements the iter() interface.\n",
    "        Takes an optional predicate(doc: int) function to filter documents\n",
    "        before returning them. For selective predicate, you may have to\n",
    "        adjust the pruning thresholds.\n",
    "    \"\"\"\n",
    "    # set this property to define the pruning threshold, None/False to turn pruning off\n",
    "    # pruning ensures that the number of elements in the heap ranges between\n",
    "    # first and second value of the tuple\n",
    "    PRUNING_THRESHOLDS = (1000, 2000)\n",
    "\n",
    "    def __init__(self, k: int, term_weights: list[tuple[str,float]] = None, predicate: Callable[[int], bool] = None):\n",
    "        self.docs_heap = []\n",
    "        self.k = k\n",
    "        self.predicate = predicate\n",
    "        if term_weights:\n",
    "            self.term_weights = term_weights\n",
    "            self.terms = [term for term, _ in self.term_weights]\n",
    "            self.weights = dict(self.term_weights)\n",
    "    \n",
    "    def add(self, doc: int, score: float):\n",
    "        heappush(self.docs_heap, (-score, doc, {'id': doc, 'score': score}))\n",
    "        # optional pruning if heap grows too large; be careful to not trigger each time for performance\n",
    "        if len(self.docs_heap) > TopKList.PRUNING_THRESHOLDS[1]:\n",
    "            self.docs_heap = nsmallest(TopKList.PRUNING_THRESHOLDS[0], self.docs_heap)\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = 0\n",
    "        while n < self.k and len(self.docs_heap) > 0:\n",
    "            doc = heappop(self.docs_heap)[2]\n",
    "            if self.predicate == None or self.predicate(doc['id']):\n",
    "                yield doc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this global variables o drive the examples\n",
    "DEBUG = False\n",
    "nDocs = 100\n",
    "index = {}\n",
    "documents = []\n",
    "feedback = topk = is_relevant = None\n",
    "\n",
    "def print_feedback(text = 'feedback'):\n",
    "    print(text + \":\", \", \".join([('+' if feedback.is_relevant(doc) else '-') + str(doc) for doc in sorted(feedback.assessed, key=lambda doc: (not feedback.is_relevant(doc), doc))]))\n",
    "\n",
    "# helper function to display result and get feedback\n",
    "def display_and_get_feedback(n = 8, all_docs = False):\n",
    "    n = topk.size if n < 0 else n\n",
    "    print(\"\\n  f  r  id score  document\\n-------------------------------------\")\n",
    "    for rank, doc in enumerate(topk):\n",
    "        # let's assume user provides feedback for n not yet assessed documents\n",
    "        if not feedback.is_assessed(doc['id']) and n > 0:\n",
    "            n -= 1\n",
    "            feedback.add(doc['id'], is_relevant(doc['id']))\n",
    "        if rank < 10:\n",
    "            relevancy = '+' if feedback.is_relevant(doc['id']) else '-' if feedback.is_assessed(doc['id']) else ' '\n",
    "            print(\"  {relevancy}{rank: >3d} {id: >3d} ({score:.2f})\".format(rank = rank + 1, relevancy = relevancy, **doc), documents[doc['id']])\n",
    "    if all_docs:\n",
    "        for doc in filter(lambda doc: not feedback.is_assessed(doc), range(1, nDocs + 1)):\n",
    "            if n > 0:\n",
    "                n -= 1\n",
    "                feedback.add(doc, is_relevant(doc))\n",
    "    print()\n",
    "    for term in sorted(topk.weights.keys(), key = lambda term: -topk.weights[term]):\n",
    "        print(term.rjust(16), topk.weights[term])\n",
    "    print_feedback(\"\\nnext feedback\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIR Model Implementation\n",
    "\n",
    "### Base class\n",
    "\n",
    "- pruning of terms\n",
    "- setting weigths based on feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BIRModel:\n",
    "    \"\"\"\n",
    "        Generic class for the evaluation of the BIR model, inherited by the document-at-a-time (DAAT) and \n",
    "        term-at-a-time (TAAT) models. This superclass defines the cj-weights including filtering the most\n",
    "        important terms.\n",
    "    \"\"\"\n",
    "    # set this property to True to remove terms with negative weights\n",
    "    PRUNE_NEGATIVE_WEIGHTS = False\n",
    "\n",
    "    # set this property to remove terms with absolute weights smaller than this value\n",
    "    PRUNE_WEIGHT_THRESHOLD  = False\n",
    "\n",
    "    # set this property to select top-k weights based on absolute values\n",
    "    PRUNE_TOPK = False\n",
    "\n",
    "    # set this property to true to prune non-relevant documents from result list\n",
    "    PRUNE_NON_RELEVANT = False\n",
    "\n",
    "    @staticmethod\n",
    "    def cj_weight(term: str, feedback: Feedback):\n",
    "        docFreq = len(index[term])\n",
    "        if feedback.is_initial_step():\n",
    "            rj = 0.5\n",
    "            nj = (docFreq + 0.5) / (len(documents) + 1)\n",
    "            if DEBUG:\n",
    "                print(term, \"rj=\", rj, \"nj=\", nj, \"cj=\", math.log(rj / (1 - rj) * (1 - nj) / nj))\n",
    "        else:\n",
    "            # get postings as set to siplify calculations in Python\n",
    "            docs = set(index[term])\n",
    "            # number of assessed relevant documents which have the term\n",
    "            lj, L = len(feedback.relevant & docs), len(feedback.relevant)\n",
    "            # number of assessed documents which have the term\n",
    "            kj, K = len(feedback.assessed & docs), len(feedback.assessed)\n",
    "            # calculate rj and nj\n",
    "            rj = (lj + 0.5) / (L + 1)\n",
    "            nj = (kj - lj + 0.5) / (K - L + 1)\n",
    "            if DEBUG:\n",
    "                print(term, \"l=\", lj, \"/\", L, \"k=\", kj, \"/\", K, \"rj=\", rj, \"nj=\", nj, \"cj=\", math.log(rj / (1 - rj) * (1 - nj) / nj))\n",
    "        return math.log(rj / (1 - rj) * (1 - nj) / nj)\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_terms(terms: list[str], feedback: Feedback) -> list[tuple[str,float]]:\n",
    "        # remove terms not in vocabulary\n",
    "        terms = list(filter(lambda t: t in vocabulary, terms))\n",
    "        # calculate weigths and produce tuples (term, weight)\n",
    "        term_weights = list(map(lambda t: (t, BIRModel.cj_weight(t, feedback)), terms))\n",
    "        # filter terms with negative weights\n",
    "        if BIRModel.PRUNE_NEGATIVE_WEIGHTS:\n",
    "            print('pruning negative weights')\n",
    "            term_weights = list(filter(lambda t: t[1] >= 0, term_weights))\n",
    "        # filter terms with small absolute weights\n",
    "        if BIRModel.PRUNE_WEIGHT_THRESHOLD:\n",
    "            print('pruning small weights')\n",
    "            term_weights = list(filter(lambda t: abs(t[1]) > BIRModel.PRUNE_WEIGHT_THRESHOLD, term_weights))\n",
    "        # select top-k terms based on absolute values\n",
    "        if BIRModel.PRUNE_TOPK:\n",
    "            print('pruning top-k weights')\n",
    "            term_weights = sorted(term_weights, key = lambda t: (-abs(t[1]),len(index[t[0]]),t[0]))[:BIRModel.PRUNE_TOPK]\n",
    "        return term_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-at-a-time for BIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel_DAAT(BIRModel):\n",
    "    \"\"\"\n",
    "        Implements the DAAT model for the BIR model using inverted index method.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def query(terms: list[str], k: int, feedback: Feedback, predicate: Callable[[int], bool] = None):\n",
    "        # filter terms and obtain weights for terms in order of their importance \n",
    "        term_weights = BIRModel.filter_terms(terms, feedback)\n",
    "        \n",
    "        # get iterators for each term and fetch first posting\n",
    "        iters = [iter(index[term]) for (term, _) in term_weights]\n",
    "        nexts = [next(iter, None) for iter in iters]\n",
    "\n",
    "        # keep track of all retrieved documents and their score; stored as tuples (doc_id, score)\n",
    "        topk = TopKList(k, term_weights, predicate)\n",
    "        while not all(e is None for e in nexts):\n",
    "            # get smallest value from nexts, ignoring None values\n",
    "            smallest = min(nexts, key = lambda x: x if x is not None else float('inf'))\n",
    "            # if we have feedback, make sure document is either relevant or not assessed so far\n",
    "            if not(BIRModel.PRUNE_NON_RELEVANT and feedback.is_not_relevant(smallest)):\n",
    "                # if so, add it to topk\n",
    "                score = sum([term_weights[i][1] for i in range(len(nexts)) if nexts[i] == smallest])\n",
    "                topk.add(smallest, score)\n",
    "            # for each entry in nexts, fetch next item if entry equals smallest\n",
    "            for i, e in enumerate(nexts):\n",
    "                if e is smallest:\n",
    "                    nexts[i] = next(iters[i], None)\n",
    "        \n",
    "        # finsihed, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-at-a-time for BIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel_TAAT(BIRModel):\n",
    "    \"\"\"\n",
    "        Implements the TAAT model for the BIR model using inverted index method.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def query(terms: list[str], k: int, feedback: Feedback, predicate: Callable[[int], bool] = None):\n",
    "        # filter terms and obtain weights for terms in order of their importance \n",
    "        term_weights = BIRModel.filter_terms(terms, feedback)\n",
    "        doc_scores = {}\n",
    "\n",
    "        # iterate over terms and fetch postings\n",
    "        for (term, weight) in term_weights:\n",
    "            for posting in index[term]:\n",
    "                # if document is not already in doc_scores, add it\n",
    "                if posting not in doc_scores:\n",
    "                    doc_scores[posting] = 0\n",
    "                # add weight to document score\n",
    "                doc_scores[posting] += weight\n",
    "\n",
    "        # we do not need a full sort of doc_scores, but can use the heap in TopKList\n",
    "        topk = TopKList(k, term_weights, predicate)\n",
    "        for doc, score in doc_scores.items():\n",
    "            topk.add(doc, score)\n",
    "        \n",
    "        # finsihed, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random data example\n",
    "### Create inverted index\n",
    "The next section generates random inverted index postings for a set of terms. It simulates the indexing process for Boolean retrieval by associating random document IDs with each term. The `vocabulary` dictionary defines terms and their desired document frequencies (as a %-figure). The generated postings are stored in the `index` dictionary, with each term mapped to a set of corresponding document IDs.\n",
    "\n",
    "* `nDocs = 100`: Defines the total number of documents (document IDs) as 100.\n",
    "* `index = {}`: Initializes an empty dictionary to store the postings for each term.\n",
    "* `DEBUG = False`: A debug flag (we use it later to illustrate code execution).\n",
    "* `vocabulary`: Defines a dictionary where each term is associated with its desired document frequency (expressed as a percentage).\n",
    "* `documents`: List of all documents with each entry holding the set of terms in the document\n",
    "\n",
    "`create_postings(term: str, docFreq: int = None)` takes a term (string) and an optional document frequency (docFreq, integer) as arguments. It generates random postings for the term by creating a set of document IDs. If docFreq is not provided, it generates a random document frequency between 1 and nDocs. The for-loop iterates through each term in the vocabulary dictionary and calls the create_postings function. For each term, it fetches the desired document frequency from the vocabulary (values are percentages) and passes it to create_postings.\n",
    "\n",
    "`is_relevant(doc: int)` returns True if document is relevant ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "DEBUG = False\n",
    "nDocs = 100\n",
    "index = {}\n",
    "documents = []\n",
    "vocabulary = {}\n",
    "\n",
    "# helper function to rate if newly encountered document is relevant\n",
    "def is_relevant(doc):\n",
    "    return random.random() < 0.8\n",
    "\n",
    "# helper function to create random postings with given document frequency\n",
    "def create_postings(term: str, docFreq: int = None):\n",
    "    # create sets with random ids\n",
    "    index[term] = sorted(random.sample(range(1, nDocs + 1), docFreq))\n",
    "    vocabulary[term] = docFreq\n",
    "    # extend feature vectors for documents\n",
    "    for doc in index[term]:\n",
    "        documents[doc].add(term)\n",
    "\n",
    "# set all feature vectors of documents to empty. We use sets since BIR uses set-of-word model\n",
    "for doc in range(nDocs + 1):\n",
    "    documents.append(set())\n",
    "\n",
    "# we use some animal terms to create random documents\n",
    "terms = ['dog', 'cat', 'horse', 'rabit', 'ostrich', 'bear', 'tiger', 'lion', 'bird']\n",
    "\n",
    "# call create_postings for each entry in vocabulary to create the inverted index\n",
    "for term in terms:\n",
    "    create_postings(term, random.randint(nDocs // 10, nDocs // 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the postings for each term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog        35   [7, 10, 11, 12, 14, 15, 16, 17, 22, 23, 31, 32, 34, 35, 50, 55, 58, 62, 65, 67, 69, 71, 72, 73, 76]\n",
      "cat        29   [2, 9, 14, 15, 16, 21, 23, 29, 30, 33, 39, 43, 44, 47, 48, 49, 51, 53, 58, 64, 66, 67, 71, 77, 78]\n",
      "horse      44   [2, 5, 6, 8, 10, 14, 20, 21, 22, 23, 24, 25, 26, 29, 34, 35, 37, 41, 43, 46, 47, 50, 52, 55, 58]\n",
      "rabit      11   [36, 46, 50, 52, 53, 67, 71, 74, 83, 84, 97]\n",
      "ostrich    27   [7, 8, 9, 10, 16, 22, 32, 40, 41, 43, 44, 48, 51, 54, 57, 58, 62, 65, 67, 71, 75, 78, 79, 81, 86]\n",
      "bear       48   [1, 3, 9, 12, 14, 15, 16, 18, 20, 23, 25, 28, 30, 31, 34, 36, 38, 39, 42, 43, 44, 45, 49, 53, 54]\n",
      "tiger      22   [2, 3, 4, 6, 7, 8, 17, 23, 26, 38, 42, 44, 49, 54, 55, 56, 66, 80, 83, 91, 93, 100]\n",
      "lion       15   [14, 18, 22, 31, 33, 37, 41, 42, 43, 44, 50, 80, 84, 86, 100]\n",
      "bird       31   [6, 7, 8, 9, 12, 17, 22, 25, 26, 28, 31, 32, 35, 36, 37, 40, 42, 43, 46, 49, 51, 56, 62, 64, 74]\n"
     ]
    }
   ],
   "source": [
    "# print postings with term and list of documents\n",
    "for term, posting in index.items():\n",
    "    print(term.ljust(10), str(len(posting)).ljust(4), sorted(posting[:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'bear'}\n",
      "2 {'horse', 'tiger', 'cat'}\n",
      "3 {'tiger', 'bear'}\n",
      "4 {'tiger'}\n",
      "5 {'horse'}\n",
      "6 {'horse', 'bird', 'tiger'}\n",
      "7 {'ostrich', 'bird', 'tiger', 'dog'}\n",
      "8 {'horse', 'bird', 'ostrich', 'tiger'}\n",
      "9 {'ostrich', 'bird', 'bear', 'cat'}\n",
      "10 {'horse', 'ostrich', 'dog'}\n",
      "11 {'dog'}\n",
      "12 {'bird', 'dog', 'bear'}\n",
      "13 set()\n",
      "14 {'lion', 'cat', 'dog', 'bear', 'horse'}\n",
      "15 {'bear', 'dog', 'cat'}\n",
      "16 {'ostrich', 'bear', 'dog', 'cat'}\n",
      "17 {'bird', 'tiger', 'dog'}\n",
      "18 {'lion', 'bear'}\n",
      "19 set()\n",
      "20 {'horse', 'bear'}\n"
     ]
    }
   ],
   "source": [
    "# print a few documents\n",
    "for doc in range(20):\n",
    "    print(doc + 1, documents[doc + 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Initial step without feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird horse\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1   6 (1.06) {'horse', 'bird', 'tiger'}\n",
      "  +  2   8 (1.06) {'horse', 'bird', 'ostrich', 'tiger'}\n",
      "  +  3  22 (1.06) {'bird', 'lion', 'dog', 'ostrich', 'horse'}\n",
      "  +  4  25 (1.06) {'horse', 'bird', 'bear'}\n",
      "  +  5  26 (1.06) {'horse', 'bird', 'tiger'}\n",
      "  -  6  35 (1.06) {'horse', 'bird', 'dog'}\n",
      "  +  7  37 (1.06) {'horse', 'bird', 'lion'}\n",
      "  +  8  43 (1.06) {'bird', 'lion', 'cat', 'ostrich', 'bear', 'horse'}\n",
      "     9  46 (1.06) {'horse', 'bird', 'rabit'}\n",
      "    10  62 (1.06) {'horse', 'bird', 'ostrich', 'dog'}\n",
      "\n",
      "            bird 0.8056251639866356\n",
      "           horse 0.25629575863111026\n",
      "\n",
      "next feedback: +6, +8, +22, +25, +26, +37, +43, -35\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# initial step for \"cat dog\"\n",
    "query = ['bird', 'horse']\n",
    "k = 20\n",
    "feedback = Feedback()\n",
    "print(' '.join(query))\n",
    "\n",
    "# set pruning behavior\n",
    "BIRModel.PRUNE_NEGATIVE_WEIGHTS     = False\n",
    "BIRModel.PRUNE_WEIGHT_THRESHOLD     = False\n",
    "BIRModel.PRUNE_TOPK                 = False\n",
    "BIRModel.PRUNE_NON_RELEVANT         = True\n",
    "\n",
    "# (optional) enable a predicate for the filtering step\n",
    "predicate = None\n",
    "# predicate = lambda doc: doc % 2 == 0\n",
    "# predicate = lambda doc: doc % 2 == 1\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback, predicate)\n",
    "display_and_get_feedback()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Feedback step\n",
    "\n",
    "Adjust weights with feedback. Repeat runs (`Ctrl+Enter` to stay on cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird horse\n",
      "feedback: +2, +5, +6, +7, +8, +9, +14, +17, +20, +21, +22, +23, +24, +25, +26, +28, +29, +31, +34, +36, +37, +40, +41, +42, +43, +46, +47, +49, +51, +52, +55, +59, +64, +65, +69, +71, +76, +77, +78, +80, +82, +85, +86, +87, +94, -10, -12, -32, -35, -50, -58, -62, -63, -90, -98, -99\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1   7 (0.04) {'ostrich', 'bird', 'tiger', 'dog'}\n",
      "  +  2   9 (0.04) {'ostrich', 'bird', 'bear', 'cat'}\n",
      "  +  3  17 (0.04) {'bird', 'tiger', 'dog'}\n",
      "  +  4  28 (0.04) {'bird', 'bear'}\n",
      "  +  5  31 (0.04) {'bird', 'lion', 'dog', 'bear'}\n",
      "  +  6  36 (0.04) {'bird', 'rabit', 'bear'}\n",
      "  +  7  40 (0.04) {'ostrich', 'bird'}\n",
      "  +  8  42 (0.04) {'bird', 'tiger', 'lion', 'bear'}\n",
      "  +  9  49 (0.04) {'bird', 'tiger', 'bear', 'cat'}\n",
      "  + 10  51 (0.04) {'ostrich', 'bird', 'cat'}\n",
      "\n",
      "            bird 0.03643390224610241\n",
      "           horse -0.11684362741444752\n",
      "\n",
      "next feedback: +2, +5, +6, +7, +8, +9, +14, +17, +20, +21, +22, +23, +24, +25, +26, +28, +29, +31, +34, +36, +37, +40, +41, +42, +43, +46, +47, +49, +51, +52, +55, +56, +59, +64, +65, +69, +71, +74, +76, +77, +78, +80, +81, +82, +85, +86, +87, +92, +94, -10, -12, -32, -35, -50, -58, -62, -63, -89, -90, -98, -99\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(query))\n",
    "print_feedback()\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback, predicate)\n",
    "display_and_get_feedback()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small document example for DAAT and TAAT\n",
    "### Create inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from the exercise\n",
    "nDocs = 0\n",
    "index = {}\n",
    "documents = [set()]\n",
    "vocabulary = {}\n",
    "stopwords = set([\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
    "    'i', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was',\n",
    "    'were', 'will', 'with'\n",
    "])\n",
    "\n",
    "# helper function to rate if newly encountered document is relevant\n",
    "def is_relevant(doc):\n",
    "    return doc < 6\n",
    "\n",
    "def add_document(text: str):\n",
    "    global nDocs\n",
    "    nDocs += 1\n",
    "    terms = set()\n",
    "    for term in set(text.lower().split(' ')):\n",
    "        if term in stopwords:\n",
    "            continue\n",
    "        terms.add(term)\n",
    "        if term not in vocabulary:\n",
    "            index[term] = [nDocs]\n",
    "            vocabulary[term] = 1\n",
    "        else:\n",
    "            index[term].append(nDocs)\n",
    "            vocabulary[term] += 1\n",
    "    documents.append(terms)\n",
    "\n",
    "add_document(\"Human machine interface for Lab ABC computer applications\")\n",
    "add_document(\"A survey of user opinion of computer system response time\")\n",
    "add_document(\"The EPS user interface management system\")\n",
    "add_document(\"System and human system engineering testing of EPS\")\n",
    "add_document(\"Relation of user perceived response time to error measurement\")\n",
    "\n",
    "add_document(\"The generation of random binary unordered trees\")\n",
    "add_document(\"The intersection graph of paths in trees\")\n",
    "add_document(\"Graph minors IV Widths of trees and well quasi ordering\")\n",
    "add_document(\"Graph minors a survey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc            1    [1]\n",
      "machine        1    [1]\n",
      "lab            1    [1]\n",
      "human          2    [1, 4]\n",
      "computer       2    [1, 2]\n",
      "applications   1    [1]\n",
      "interface      2    [1, 3]\n",
      "user           3    [2, 3, 5]\n",
      "response       2    [2, 5]\n",
      "survey         2    [2, 9]\n",
      "system         3    [2, 3, 4]\n",
      "time           2    [2, 5]\n",
      "opinion        1    [2]\n",
      "eps            2    [3, 4]\n",
      "management     1    [3]\n",
      "engineering    1    [4]\n",
      "testing        1    [4]\n",
      "relation       1    [5]\n",
      "error          1    [5]\n",
      "perceived      1    [5]\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice \n",
    "\n",
    "# print postings with term and list of documents\n",
    "for term, posting in islice(index.items(), 20):\n",
    "    print(term.ljust(14), str(len(posting)).ljust(4), sorted(posting[:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 {'abc', 'machine', 'lab', 'human', 'computer', 'applications', 'interface'}\n",
      "2 {'response', 'user', 'survey', 'system', 'time', 'computer', 'opinion'}\n",
      "3 {'eps', 'user', 'system', 'management', 'interface'}\n",
      "4 {'engineering', 'eps', 'testing', 'human', 'system'}\n",
      "5 {'response', 'user', 'relation', 'time', 'error', 'perceived', 'measurement'}\n",
      "6 {'random', 'trees', 'binary', 'unordered', 'generation'}\n",
      "7 {'trees', 'intersection', 'paths', 'graph'}\n",
      "8 {'trees', 'graph', 'quasi', 'iv', 'well', 'widths', 'ordering', 'minors'}\n",
      "9 {'graph', 'survey', 'minors'}\n"
     ]
    }
   ],
   "source": [
    "# print all documents\n",
    "print()\n",
    "for doc in range(len(documents) - 1):\n",
    "    print(doc + 1, documents[doc + 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human computer interaction\n",
      "pruning negative weights\n",
      "pruning top-k weights\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1   1 (2.45) {'abc', 'machine', 'lab', 'human', 'computer', 'applications', 'interface'}\n",
      "  +  2   2 (1.22) {'response', 'user', 'survey', 'system', 'time', 'computer', 'opinion'}\n",
      "  +  3   4 (1.22) {'engineering', 'eps', 'testing', 'human', 'system'}\n",
      "\n",
      "        computer 1.2237754316221157\n",
      "           human 1.2237754316221157\n",
      "\n",
      "next feedback: +1, +2, +3, +4, +5, -6, -7, -8, -9\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# initial step for \"cat dog\"\n",
    "query = ['human', 'computer', 'interaction']\n",
    "k = 9\n",
    "feedback = Feedback()\n",
    "print(' '.join(query))\n",
    "\n",
    "# set behavior\n",
    "BIRModel.PRUNE_NEGATIVE_WEIGHTS     = True\n",
    "BIRModel.PRUNE_WEIGHT_THRESHOLD     = False\n",
    "BIRModel.PRUNE_TOPK                 = 5\n",
    "BIRModel.PRUNE_NON_RELEVANT         = False\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback)\n",
    "display_and_get_feedback(k, all_docs = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback step\n",
    "\n",
    "Adjust weights with feedback. Repeat runs (`Ctrl+Enter` to stay on cell) + query expansion with terms from relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc applications computer engineering eps error human interaction interface lab machine management measurement opinion perceived relation response survey system testing time user\n",
      "feedback: +1, +2, +3, +4, +5, -6, -7, -8, -9\n",
      "pruning negative weights\n",
      "pruning top-k weights\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1   2 (6.93) {'response', 'user', 'survey', 'system', 'time', 'computer', 'opinion'}\n",
      "  +  2   3 (6.93) {'eps', 'user', 'system', 'management', 'interface'}\n",
      "  +  3   4 (6.26) {'engineering', 'eps', 'testing', 'human', 'system'}\n",
      "  +  4   1 (3.72) {'abc', 'machine', 'lab', 'human', 'computer', 'applications', 'interface'}\n",
      "  +  5   5 (2.53) {'response', 'user', 'relation', 'time', 'error', 'perceived', 'measurement'}\n",
      "\n",
      "          system 2.5336968139574325\n",
      "            user 2.5336968139574325\n",
      "        computer 1.8607523407150066\n",
      "             eps 1.8607523407150066\n",
      "           human 1.8607523407150066\n",
      "\n",
      "next feedback: +1, +2, +3, +4, +5, -6, -7, -8, -9\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "print(' '.join(query))\n",
    "print_feedback()\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback)\n",
    "display_and_get_feedback(k)\n",
    "\n",
    "query = sorted(set(query) | reduce(lambda terms, doc: terms | documents[doc], feedback.relevant, set()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
