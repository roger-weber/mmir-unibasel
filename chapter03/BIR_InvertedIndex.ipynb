{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIR retrieval with inverted files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer & Set of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test', 'function', 'simple'}\n",
      "{'test', 'function', 'simple', 'a', 'this', 'is', 'for'}\n"
     ]
    }
   ],
   "source": [
    "from utils import analyzer\n",
    "\n",
    "print(analyzer.set_of_words(\"this is a simple test for this function\", remove_stopwords = True))\n",
    "print(analyzer.set_of_words(\"this is a simple test for this function\", remove_stopwords = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "class Feedback:\n",
    "    \"\"\"\n",
    "        Collects feedback for documents and provides functions to check if \n",
    "        document is assessed, relevant or not relevant.\n",
    "    \"\"\"\n",
    "    def __init__(self, assessment_func: Callable[[int], bool] = None):\n",
    "        self.assessment_func = assessment_func\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.assessed = set()\n",
    "        self.relevant = set()\n",
    "    \n",
    "    def is_initial_step(self) -> bool:\n",
    "        return len(self.assessed) == 0\n",
    "\n",
    "    def assess(self, doc_id: int) -> None:\n",
    "        self.assessed.add(doc_id)\n",
    "        if not self.assessment_func or self.assessment_func(doc_id):\n",
    "            self.relevant.add(doc_id)\n",
    "        \n",
    "    def is_relevant(self, doc_id: int) -> bool:\n",
    "        return doc_id in self.relevant\n",
    "    \n",
    "    def is_assessed(self, doc_id: int) -> bool:\n",
    "        return doc_id in self.assessed\n",
    "    \n",
    "    def is_not_relevant(self, doc_id: int) -> bool:\n",
    "        return (doc_id in self.assessed) and (doc_id not in self.relevant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopKList class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappop, heappush\n",
    "\n",
    "class TopKList:\n",
    "    \"\"\"\n",
    "        Maintains a list of top-k documents. Initializer accepts\n",
    "        a list of tuples (term, weight) to provide information about\n",
    "        weights used by retrieval model. Implements the iter() interface.\n",
    "        Takes an optional predicate(doc_id: int) function to filter documents\n",
    "        before returning them. \n",
    "    \"\"\"\n",
    "    def __init__(self, k: int, term_weights: list[tuple[str,float]] = None, predicate: Callable[[int], bool] = None):\n",
    "        self.docs_heap = []\n",
    "        self.k = k\n",
    "        self.predicate = predicate\n",
    "        self.results = []\n",
    "        if term_weights:\n",
    "            self.term_weights = term_weights\n",
    "            self.terms = [term for term, _ in self.term_weights]\n",
    "            self.weights = dict(self.term_weights)\n",
    "    \n",
    "    def add(self, doc_id: int, score: float):\n",
    "        heappush(self.docs_heap, (-score, doc_id, {'id': doc_id, 'score': score}))\n",
    "        # optional (infrequent) pruning if heap grows too large\n",
    "\n",
    "    def __iter__(self):\n",
    "        # do we already have the results?\n",
    "        for entry in self.results:\n",
    "            yield entry\n",
    "        # produce more results (if necessary and available)\n",
    "        rank = len(self.results)\n",
    "        while rank < self.k and len(self.docs_heap) > 0:\n",
    "            entry = heappop(self.docs_heap)[2]\n",
    "            if self.predicate == None or self.predicate(entry['id']):\n",
    "                rank += 1\n",
    "                entry['rank'] = rank\n",
    "                self.results.append(entry)\n",
    "                yield entry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIR Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Base Retriever Class\n",
    "\n",
    "* `n_docs: int`: number of documents added to index\n",
    "* `documents dict[int, dict{'id', 'vector'}]`: collection of documents as dictionary with doc_id as key. Each document is a dictionary with the properties from the dataset and additional properties for the retrieval:\n",
    "  - `id` hold the document id as generated when loading the document; corresponds to the key in documents\n",
    "  - `vector` holds the term freqeuncies as dictionary (key=term, value=term frequency)\n",
    "* `vocabulary: dict[term, int]`: vocabularoy of collection with term as keys and document frequency as values\n",
    "* `index: dict[term, list[int]]`: inverted index mapping terms to postings. Postings contain doc_id sorted by doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel:\n",
    "    \"\"\"\n",
    "        Generic class for the evaluation of the BIR model, inherited by the document-at-a-time (DAAT) and \n",
    "        term-at-a-time (TAAT) models. \n",
    "    \"\"\"\n",
    "    def __init__(self, collection: list[dict] = None, remove_stopwords: bool = True):\n",
    "        self.build_index(collection or [], remove_stopwords)\n",
    "    \n",
    "    def _add_document(self, doc: dict):\n",
    "        self.n_docs += 1\n",
    "        doc_id = doc['id'] = self.n_docs\n",
    "        self.documents[doc_id] = doc\n",
    "        # create vector from str-properties\n",
    "        text = ' '.join([value for key, value in doc.items() if type(value) == str])\n",
    "        vector = doc['vector'] = analyzer.set_of_words(text, remove_stopwords = self.remove_stopwords)\n",
    "        # add to vocabulary and postings\n",
    "        for term in vector:\n",
    "            self.vocabulary[term] = self.vocabulary.get(term, 0) + 1\n",
    "            self.index.setdefault(term, []).append(doc_id)\n",
    "    \n",
    "    def build_index(self, collection: list[dict], remove_stopwords: bool = True):\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.n_docs = 0\n",
    "        self.documents = {}\n",
    "        self.index = {}\n",
    "        self.vocabulary = {}\n",
    "        # load all documents\n",
    "        for doc in collection:\n",
    "            self._add_document(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of c_j-weights\n",
    "Two variants for initial step and feedback step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BIRModel(BIRModel):\n",
    "    def cj_weight(self, term: str, feedback: Feedback):\n",
    "        doc_freq = len(self.index[term])\n",
    "        if feedback.is_initial_step():\n",
    "            rj = 0.5\n",
    "            nj = (doc_freq + 0.5) / (len(self.documents) + 1)\n",
    "        else:\n",
    "            # get postings as set to siplify calculations in Python\n",
    "            docs = set(self.index[term])\n",
    "            # number of assessed relevant documents which have the term\n",
    "            lj, L = len(feedback.relevant & docs), len(feedback.relevant)\n",
    "            # number of assessed documents which have the term\n",
    "            kj, K = len(feedback.assessed & docs), len(feedback.assessed)\n",
    "            # calculate rj and nj\n",
    "            rj = (lj + 0.5) / (L + 1)\n",
    "            nj = (kj - lj + 0.5) / (K - L + 1)\n",
    "        return math.log(rj / (1 - rj) * (1 - nj) / nj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term & document filtering with options\n",
    "Pruning of terms and documents based on the following settings:\n",
    "- `PRUNE_NEGATIVE_WEIGHTS: bool = False`, set this property to True to remove terms with negative weights\n",
    "- `PRUNE_WEIGHT_THRESHOLD: bool  = False`, set this property to remove terms with absolute weights smaller than this value\n",
    "- `PRUNE_TOPK: bool | int = False`, set this property to select top-k weights based on absolute values\n",
    "- `PRUNE_NON_RELEVANT: bool = False`, set this property to true to prune non-relevant documents from result list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel(BIRModel):\n",
    "    # set this property to True to remove terms with negative weights\n",
    "    PRUNE_NEGATIVE_WEIGHTS = False\n",
    "\n",
    "    # set this property to remove terms with absolute weights smaller than this value\n",
    "    PRUNE_WEIGHT_THRESHOLD  = False\n",
    "\n",
    "    # set this property to select top-k weights based on absolute values\n",
    "    PRUNE_TOPK = False\n",
    "\n",
    "    # set this property to true to prune non-relevant documents from result list\n",
    "    PRUNE_NON_RELEVANT = False\n",
    "\n",
    "    def query_weights(self, terms: set[str], feedback: Feedback) -> list[tuple[str,float]]:\n",
    "        # remove terms not in vocabulary\n",
    "        terms = list(filter(lambda t: t in self.vocabulary, terms))\n",
    "        # calculate weigths and produce tuples (term, weight)\n",
    "        term_weights = list(map(lambda t: (t, self.cj_weight(t, feedback)), terms))\n",
    "        # filter terms with negative weights\n",
    "        if self.PRUNE_NEGATIVE_WEIGHTS:\n",
    "            term_weights = list(filter(lambda t: t[1] >= 0, term_weights))\n",
    "        # filter terms with small absolute weights\n",
    "        if self.PRUNE_WEIGHT_THRESHOLD:\n",
    "            term_weights = list(filter(lambda t: abs(t[1]) > self.PRUNE_WEIGHT_THRESHOLD, term_weights))\n",
    "        # select top-k terms based on absolute values\n",
    "        if self.PRUNE_TOPK:\n",
    "            term_weights = sorted(term_weights, key = lambda t: (-abs(t[1]),len(self.index[t[0]]),t[0]))[:self.PRUNE_TOPK]\n",
    "        return term_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-at-a-time (DAAT) for BIR Model\n",
    "The implementation of DAAT for the BIR model uses sorted postings and processes postings in ascending order of the document IDs (see Or-implementation of Boolean model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel_DAAT(BIRModel):\n",
    "    \"\"\"\n",
    "        Implements the DAAT model for the BIR model using inverted index method.\n",
    "    \"\"\"\n",
    "    def search(self, query: str, k: int, feedback: Feedback, predicate: Callable[[int], bool] = None, selected_docs: set[int] = None) -> TopKList:\n",
    "        query_vector = analyzer.set_of_words(query)\n",
    "\n",
    "        # filter terms and obtain c_j-weights for terms in order of their importance \n",
    "        term_weights = self.query_weights(query_vector, feedback)\n",
    "        \n",
    "        # get iterators for each term and fetch first posting\n",
    "        iters = [iter(self.index[term]) for (term, _) in term_weights]\n",
    "        nexts = [next(iter, None) for iter in iters]\n",
    "\n",
    "        # keep track of all retrieved documents and their score; stored as tuples (doc_id, score)\n",
    "        topk = TopKList(k, term_weights, predicate)\n",
    "\n",
    "        # iterate through all streams and calculate score for smallest doc id\n",
    "        while not all(e is None for e in nexts):\n",
    "            # get smallest value from nexts, ignoring None values\n",
    "            smallest = min(nexts, key = lambda x: x or math.inf)\n",
    "            # if we have feedback, make sure document is either relevant or not assessed so far; if we have selected_docs, make sure document is in it\n",
    "            if not(self.PRUNE_NON_RELEVANT and feedback.is_not_relevant(smallest)) and (selected_docs == None or smallest in selected_docs):\n",
    "                # if so, add it to topk\n",
    "                score = sum([term_weights[i][1] for i in range(len(nexts)) if nexts[i] == smallest])\n",
    "                topk.add(smallest, score)\n",
    "            # for each entry in nexts, fetch next item if entry equals smallest\n",
    "            for i, e in enumerate(nexts):\n",
    "                if e is smallest:\n",
    "                    nexts[i] = next(iters[i], None)\n",
    "        \n",
    "        # finished, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-at-a-time for BIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel_TAAT(BIRModel):\n",
    "    \"\"\"\n",
    "        Implements the TAAT model for the BIR model using inverted index method.\n",
    "    \"\"\"\n",
    "    def search(self, query: str, k: int, feedback: Feedback, predicate: Callable[[int], bool] = None, selected_docs: set[int] = None) -> TopKList:\n",
    "        query_vector = analyzer.set_of_words(query)\n",
    "\n",
    "        # filter terms and obtain c_j-weights for terms in order of their importance \n",
    "        term_weights = self.query_terms(query_vector, feedback)\n",
    "        doc_scores = {}\n",
    "\n",
    "        # iterate over terms and fetch postings\n",
    "        for (term, weight) in term_weights:\n",
    "            for posting in self.index[term]:\n",
    "                # check if it is either not assessed or relevant; check if posting is selected_docs (if given)\n",
    "                if not(self.PRUNE_NON_RELEVANT and feedback.is_not_relevant(posting)) and (selected_docs == None or posting in selected_docs):\n",
    "                    doc_scores[posting] = doc_scores.get(posting, 0) + weight\n",
    "\n",
    "        # we do not need a full sort of doc_scores, but can use the heap in TopKList\n",
    "        topk = TopKList(k, term_weights, predicate)\n",
    "        for doc_id, score in doc_scores.items():\n",
    "                topk.add(doc_id, score)\n",
    "        \n",
    "        # finisheds, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running some examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f800e3d04f4cbf963407c0bf169a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(options=('document-at-a-time', 'term-at-a-time'), value='document-at-a-time')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920b8a0b17244449b9ea4bb99bd29451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(options=('random', 'imdb movies'), value='random')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "opt_implementation = widgets.Dropdown(options=['document-at-a-time', 'term-at-a-time'])\n",
    "opt_dataset = widgets.Dropdown(options=['random', 'imdb movies'])\n",
    "display(opt_implementation)\n",
    "display(opt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import table\n",
    "from datasets import random as random_docs, imdb as imdb_docs\n",
    "import random\n",
    "\n",
    "# select the implementation of the retrieval model\n",
    "if opt_implementation.value == 'document-at-a-time':\n",
    "    retriever = BIRModel_DAAT()\n",
    "else:\n",
    "    retriever = BIRModel_TAAT()\n",
    "\n",
    "# select the dataset and define feedback function, queries, predicates, and selections\n",
    "if opt_dataset.value == 'random':\n",
    "    collection = random_docs\n",
    "    assessments = {\n",
    "        'random': lambda id: random.random() < 0.8,\n",
    "        'id < 20': lambda id: id < 20,\n",
    "    }\n",
    "    queries = [\n",
    "        'cat dog',\n",
    "        'horse bird',\n",
    "        'cat dog horse bird'\n",
    "    ]\n",
    "    predicates = {\n",
    "        'even doc ids': lambda id: id % 2 == 0,\n",
    "        'odd doc ids': lambda id: id % 2 == 1,\n",
    "    }\n",
    "    selections = {\n",
    "        'doc<10': list(range(10)),\n",
    "    }\n",
    "elif opt_dataset.value == 'imdb movies':\n",
    "    collection = imdb_docs\n",
    "    assessments = {\n",
    "        'top-100': lambda id: id < 100,\n",
    "        'star in title': lambda id: 'star' in retriever.documents[id]['title'].lower(),\n",
    "        'morgan in actor': lambda id: 'morgan' in retriever.documents[id]['actors'].lower(),\n",
    "        'comedy in genre': lambda id: 'comedy' in retriever.documents[id]['genre'].lower(),\n",
    "    }\n",
    "    queries = [\n",
    "        'star wars', \n",
    "        'drama morgan freeman', \n",
    "        'comedy'\n",
    "    ]\n",
    "    predicates = {\n",
    "        'year < 1990': lambda id: retriever.documents[id]['year'] < 1990,\n",
    "        'year >= 1990': lambda id: retriever.documents[id]['year'] >= 1990,\n",
    "    }\n",
    "    selections = {\n",
    "        'top-100': list(range(100)),\n",
    "        'top-250': list(range(250)),\n",
    "    }\n",
    "else:\n",
    "    raise ValueError(\"to be implemented\")\n",
    "\n",
    "# build index\n",
    "retriever.build_index(collection.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   id | text                                                           |\n",
      "|------|----------------------------------------------------------------|\n",
      "|    1 | dog dog dog ostrich                                            |\n",
      "|    2 | tiger tiger tiger tiger tiger                                  |\n",
      "|    3 | dog dog cat cat cat cat horse                                  |\n",
      "|    4 | dog rabit rabit lion ant wale wale wale wale                   |\n",
      "|    5 | wale wale                                                      |\n",
      "|    6 | tiger tiger wale                                               |\n",
      "|    7 | rabit rabit rabit rabit tiger tiger snake snake                |\n",
      "|    8 | dog bird bird bird bee bee bee bee bee snake snake snake snake |\n",
      "|    9 | horse horse horse horse bear bear snake snake snake snake      |\n",
      "|   10 | horse horse horse horse rabit rabit bear bear bear ant ant ant |\n",
      "\n",
      "| term    |   df | posting                                                                                                               |\n",
      "|---------|------|-----------------------------------------------------------------------------------------------------------------------|\n",
      "| dog     |   30 | [1, 3, 4, 8, 11, 13, 17, 20, 21, 27, 28, 38, 40, 43, 44, 51, 52, 55, 60, 61, 63, 68, 71, 76, 78, 81, 85, 88, 93, 100] |\n",
      "| cat     |   25 | [3, 11, 23, 26, 30, 32, 34, 37, 39, 45, 47, 54, 56, 65, 68, 76, 78, 81, 82, 83, 87, 88, 89, 90, 96]                   |\n",
      "| horse   |   22 | [3, 9, 10, 17, 19, 23, 25, 29, 32, 35, 48, 51, 52, 59, 67, 69, 73, 75, 87, 88, 94, 96]                                |\n",
      "| rabit   |   20 | [4, 7, 10, 16, 22, 29, 31, 32, 40, 45, 48, 50, 52, 54, 57, 59, 84, 87, 92, 99]                                        |\n",
      "| ostrich |   18 | [1, 16, 18, 20, 23, 30, 32, 33, 36, 41, 52, 55, 63, 64, 65, 74, 77, 91]                                               |\n",
      "| bear    |   17 | [9, 10, 11, 21, 22, 31, 38, 39, 40, 54, 59, 66, 68, 79, 85, 89, 92]                                                   |\n",
      "| tiger   |   15 | [2, 6, 7, 15, 16, 21, 35, 46, 49, 55, 56, 57, 62, 72, 93]                                                             |\n",
      "| lion    |   14 | [4, 17, 28, 32, 36, 40, 41, 43, 47, 50, 52, 55, 62, 98]                                                               |\n",
      "| bird    |   14 | [8, 18, 21, 22, 36, 41, 46, 70, 73, 76, 91, 96, 98, 99]                                                               |\n",
      "| donkey  |   13 | [16, 28, 36, 40, 46, 56, 60, 61, 64, 66, 80, 90, 97]                                                                  |\n",
      "| ant     |   12 | [4, 10, 11, 16, 32, 35, 41, 55, 67, 90, 91, 100]                                                                      |\n",
      "| bee     |   12 | [8, 11, 37, 45, 52, 59, 63, 73, 76, 84, 88, 90]                                                                       |\n",
      "| wale    |   11 | [4, 5, 6, 12, 15, 21, 23, 26, 71, 83, 93]                                                                             |\n",
      "| fly     |   11 | [11, 20, 28, 33, 34, 38, 53, 54, 78, 83, 98]                                                                          |\n",
      "| snake   |   10 | [7, 8, 9, 25, 30, 51, 55, 57, 65, 93]                                                                                 |\n",
      "| fish    |    6 | [14, 24, 42, 58, 86, 95]                                                                                              |\n",
      "\n",
      "100 documents in collection\n",
      "16 distinct terms in collection\n",
      "250 postings\n"
     ]
    }
   ],
   "source": [
    "table.print([collection.format(doc) for doc in retriever.documents.values()], collection.headers(), max_rows = 10)\n",
    "table.print(sorted([[term, df, retriever.index[term]] for term, df in retriever.vocabulary.items()], key=lambda x: -x[1]), ['term', 'df', 'posting'], max_rows=20)\n",
    "\n",
    "print(f'{len(retriever.documents)} documents in collection')\n",
    "print(f'{len(retriever.vocabulary)} distinct terms in collection')\n",
    "print('{count} postings'.format(count=sum([len(postings) for postings in retriever.index.values()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty printing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feedback(feedback: Feedback, func: str, text: str = 'feedback'):\n",
    "    info = \", \".join([('+' if feedback.is_relevant(doc_id) else '-') + str(doc_id) for doc_id in sorted(feedback.assessed, key=lambda doc_id: (not feedback.is_relevant(doc_id), doc_id))])\n",
    "    print(f'{text} ({func}): {info}')\n",
    "\n",
    "def print_topk(topk: TopKList, feedback: Feedback):\n",
    "    list = []\n",
    "    for entry in topk:\n",
    "        list.append(collection.format(retriever.documents[entry['id']], [\n",
    "            '+' if feedback.is_relevant(entry['id']) else '-' if feedback.is_assessed(entry['id']) else ' ',\n",
    "            entry['rank'],\n",
    "            round(entry['score'], 2)\n",
    "        ]))\n",
    "    table.print(list, collection.headers('rel', 'rank', 'score'), max_rows=len(list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feedback(feedback, topk, n_feedback):\n",
    "    for entry in topk:\n",
    "        if n_feedback <= 0: return\n",
    "        if feedback.is_assessed(entry['id']): continue\n",
    "        feedback.assess(entry['id'])\n",
    "        n_feedback -= 1\n",
    "    for doc_id in filter(lambda doc_id: not feedback.is_assessed(doc_id), retriever.documents.keys()):\n",
    "        if n_feedback <= 0: return\n",
    "        if feedback.is_assessed(doc_id): continue\n",
    "        feedback.assess(doc_id)\n",
    "        n_feedback -= 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search with feedback iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a420ad1251c41d688f603da9cd79787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(description=' start', icon='play', style=ButtonStyle()), Button(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9214365e42a94043a11aed257dfc48c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid #eeeeee', border_left='1px solid #eeeeee', border_right='1px sol…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from functools import reduce\n",
    "\n",
    "feedback = Feedback()\n",
    "\n",
    "def run_query(query: str, k: int, assessment: str, predicate: str, selection: str, n_feedback: int):\n",
    "    global topk\n",
    "    feedback.assessment_func = assessments.get(assessment, None)\n",
    "    print_feedback(feedback, assessment)\n",
    "    print()\n",
    "    topk = retriever.search(query, k, feedback=feedback, predicate=predicates.get(predicate, None), selected_docs=selections.get(selection, None))\n",
    "    add_feedback(feedback, topk, n_feedback)\n",
    "    print_topk(topk, feedback)\n",
    "    for term in sorted(topk.weights.keys(), key = lambda term: -topk.weights[term]):\n",
    "        print(term.rjust(16), topk.weights[term])\n",
    "    print_feedback(feedback, assessment, \"\\nnext feedback\")\n",
    "\n",
    "def on_next(btn):\n",
    "    retriever.PRUNE_NEGATIVE_WEIGHTS = opt_neg.value\n",
    "    retriever.PRUNE_WEIGHT_THRESHOLD = opt_small.value and 0.5\n",
    "    retriever.PRUNE_TOPK = opt_topk.value and 10\n",
    "    retriever.PRUNE_NON_RELEVANT = opt_nonrel.value\n",
    "    if opt_expand.value:\n",
    "        query_text = query.value + ' ' + ' '.join(reduce(lambda terms, doc_id: terms | retriever.documents[doc_id]['vector'], feedback.relevant, set()))\n",
    "    else:\n",
    "        query_text = query.value\n",
    "    with out:\n",
    "        clear_output()\n",
    "        print(query_text)\n",
    "        run_query(query_text, 20, assessment.value, predicate.value, selection.value, n_feedback.value)\n",
    "\n",
    "def on_start(btn):\n",
    "    feedback.clear()\n",
    "    on_next(btn)\n",
    "\n",
    "# buttons\n",
    "btn_start = widgets.Button(description=' start', icon='play')\n",
    "btn_start.on_click(on_start)\n",
    "btn_next = widgets.Button(description=' next', icon='step-forward')\n",
    "btn_next.on_click(on_next)\n",
    "buttons = widgets.HBox([btn_start, btn_next])\n",
    "\n",
    "# query left side\n",
    "query=widgets.Dropdown(description='query',options=list(queries))\n",
    "assessment=widgets.Dropdown(description='assessment',options=['<none>'] + list(assessments.keys()))\n",
    "n_feedback=widgets.IntSlider(description='feedback', min=5, max=50, step=5, value=5)\n",
    "predicate=widgets.Dropdown(description='predicate',options=['<none>'] + list(predicates.keys()))\n",
    "selection=widgets.Dropdown(description='selection',options=['<none>'] + list(selections.keys()))\n",
    "left = widgets.VBox([query, assessment, n_feedback, predicate, selection])\n",
    "\n",
    "# options right side\n",
    "opt_neg = widgets.Checkbox(value=False, description='prune negative weights')\n",
    "opt_small = widgets.Checkbox(value=False, description='prune small weights (abs < 0.5)')\n",
    "opt_topk = widgets.Checkbox(value=False, description='keep top 10 weights')\n",
    "opt_nonrel = widgets.Checkbox(value=False, description='prune non relevant documents')\n",
    "opt_expand = widgets.Checkbox(value=False, description='expand query with feedback')\n",
    "right = widgets.VBox([opt_neg, opt_small, opt_topk, opt_nonrel, opt_expand])\n",
    "\n",
    "# display the dialog object\n",
    "display(widgets.VBox([buttons, widgets.HBox([left, right], layout={'margin': '20px'})]))\n",
    "\n",
    "# capture output with this widget\n",
    "out = widgets.Output(layout={'border': '1px solid #eeeeee', 'height': '500px', 'overflow': 'auto', 'padding': '0px 0px 0px 10px'})\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
