{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIR retrieval with inverted files\n",
    "\n",
    "## Helper functions for the BIRModel\n",
    "\n",
    "### Feedback class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedback:\n",
    "    \"\"\"\n",
    "        Collects feedback for documents and provides\n",
    "        functions to check if document is assessed,\n",
    "        relevant or not relevant.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.assessed = set()\n",
    "        self.relevant = set()\n",
    "    \n",
    "    def is_initial_step(self) -> bool:\n",
    "        return len(self.assessed) == 0\n",
    "\n",
    "    def add(self, doc: int, relevant: bool) -> None:\n",
    "        self.assessed.add(doc)\n",
    "        if relevant:\n",
    "            self.relevant.add(doc)\n",
    "        \n",
    "    def is_relevant(self, doc: int) -> bool:\n",
    "        return doc in self.relevant\n",
    "    \n",
    "    def is_assessed(self, doc: int) -> bool:\n",
    "        return doc in self.assessed\n",
    "    \n",
    "    def is_not_relevant(self, doc: int) -> bool:\n",
    "        return (doc in self.assessed) and (doc not in self.relevant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopKList class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappop, heappush, nsmallest\n",
    "from typing import Callable\n",
    "\n",
    "class TopKList:\n",
    "    \"\"\"\n",
    "        Maintains a list of top-k documents. Initializer accepts\n",
    "        a list of tuples (term, weight) to provide information about\n",
    "        weights used by retrieval model. Implements the iter() interface.\n",
    "        Takes an optional predicate(doc: int) function to filter documents\n",
    "        before returning them. For selective predicate, you may have to\n",
    "        adjust the pruning thresholds.\n",
    "    \"\"\"\n",
    "    # set this property to define the pruning threshold, None/False to turn pruning off\n",
    "    # pruning ensures that the number of elements in the heap ranges between\n",
    "    # first and second value of the tuple\n",
    "    PRUNING_THRESHOLDS = (1000, 2000)\n",
    "\n",
    "    def __init__(self, k: int, term_weights: list[tuple[str,float]] = None, predicate: Callable[[int], bool] = None):\n",
    "        self.docs_heap = []\n",
    "        self.k = k\n",
    "        self.predicate = predicate\n",
    "        if term_weights:\n",
    "            self.term_weights = term_weights\n",
    "            self.terms = [term for term, _ in self.term_weights]\n",
    "            self.weights = dict(self.term_weights)\n",
    "    \n",
    "    def add(self, doc: int, score: float):\n",
    "        heappush(self.docs_heap, (-score, doc, {'id': doc, 'score': score}))\n",
    "        # optional pruning if heap grows too large; be careful to not trigger each time for performance\n",
    "        if len(self.docs_heap) > TopKList.PRUNING_THRESHOLDS[1]:\n",
    "            self.docs_heap = nsmallest(TopKList.PRUNING_THRESHOLDS[0], self.docs_heap)\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = 0\n",
    "        while n < self.k and len(self.docs_heap) > 0:\n",
    "            doc = heappop(self.docs_heap)[2]\n",
    "            if self.predicate == None or self.predicate(doc['id']):\n",
    "                yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this global variables o drive the examples\n",
    "DEBUG = False\n",
    "nDocs = 100\n",
    "index = {}\n",
    "documents = []\n",
    "vocabulary = {}\n",
    "\n",
    "def print_feedback(feedback, text = 'feedback'):\n",
    "    print(text + \":\", \", \".join([('+' if feedback.is_relevant(doc) else '-') + str(doc) for doc in sorted(feedback.assessed, key=lambda doc: (not feedback.is_relevant(doc), doc))]))\n",
    "\n",
    "# helper function to display result and get feedback\n",
    "def print_topk_and_get_feedback(topk: TopKList, feedback: Feedback, is_relevant: Callable[[int], bool], nFeedback = 5, extra_feedback: bool = False):\n",
    "    print(\"\\n  f  r  id score  document\\n-------------------------------------\")\n",
    "    for rank, doc in enumerate(topk):\n",
    "        # let's assume user provides feedback for n not yet assessed documents\n",
    "        if not feedback.is_assessed(doc['id']) and nFeedback > 0:\n",
    "            nFeedback -= 1\n",
    "            feedback.add(doc['id'], is_relevant(doc['id']))\n",
    "        if rank < 10:\n",
    "            relevancy = '+' if feedback.is_relevant(doc['id']) else '-' if feedback.is_assessed(doc['id']) else ' '\n",
    "            print(\"  {relevancy}{rank: >3d} {id: >3d} ({score:.2f})\".format(rank = rank + 1, relevancy = relevancy, **doc), documents[doc['id']])\n",
    "    if extra_feedback:\n",
    "        for doc in filter(lambda doc: not feedback.is_assessed(doc), range(1, nDocs + 1)):\n",
    "            if nFeedback <= 0:\n",
    "                break\n",
    "            nFeedback -= 1\n",
    "            feedback.add(doc, is_relevant(doc))\n",
    "    print()\n",
    "    for term in sorted(topk.weights.keys(), key = lambda term: -topk.weights[term]):\n",
    "        print(term.rjust(16), topk.weights[term])\n",
    "    print_feedback(feedback, \"\\nnext feedback\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for pretty printing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIR Model Implementation\n",
    "\n",
    "### Base class\n",
    "\n",
    "- pruning of terms\n",
    "- setting weigths based on feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BIRModel:\n",
    "    \"\"\"\n",
    "        Generic class for the evaluation of the BIR model, inherited by the document-at-a-time (DAAT) and \n",
    "        term-at-a-time (TAAT) models. This superclass defines the cj-weights including filtering the most\n",
    "        important terms.\n",
    "    \"\"\"\n",
    "    # set this property to True to remove terms with negative weights\n",
    "    PRUNE_NEGATIVE_WEIGHTS = False\n",
    "\n",
    "    # set this property to remove terms with absolute weights smaller than this value\n",
    "    PRUNE_WEIGHT_THRESHOLD  = False\n",
    "\n",
    "    # set this property to select top-k weights based on absolute values\n",
    "    PRUNE_TOPK = False\n",
    "\n",
    "    # set this property to true to prune non-relevant documents from result list\n",
    "    PRUNE_NON_RELEVANT = False\n",
    "\n",
    "    @staticmethod\n",
    "    def cj_weight(term: str, feedback: Feedback):\n",
    "        docFreq = len(index[term])\n",
    "        if feedback.is_initial_step():\n",
    "            rj = 0.5\n",
    "            nj = (docFreq + 0.5) / (len(documents) + 1)\n",
    "            if DEBUG:\n",
    "                print(term, \"rj=\", rj, \"nj=\", nj, \"cj=\", math.log(rj / (1 - rj) * (1 - nj) / nj))\n",
    "        else:\n",
    "            # get postings as set to siplify calculations in Python\n",
    "            docs = set(index[term])\n",
    "            # number of assessed relevant documents which have the term\n",
    "            lj, L = len(feedback.relevant & docs), len(feedback.relevant)\n",
    "            # number of assessed documents which have the term\n",
    "            kj, K = len(feedback.assessed & docs), len(feedback.assessed)\n",
    "            # calculate rj and nj\n",
    "            rj = (lj + 0.5) / (L + 1)\n",
    "            nj = (kj - lj + 0.5) / (K - L + 1)\n",
    "            if DEBUG:\n",
    "                print(term, \"l=\", lj, \"/\", L, \"k=\", kj, \"/\", K, \"rj=\", rj, \"nj=\", nj, \"cj=\", math.log(rj / (1 - rj) * (1 - nj) / nj))\n",
    "        return math.log(rj / (1 - rj) * (1 - nj) / nj)\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_terms(terms: list[str], feedback: Feedback) -> list[tuple[str,float]]:\n",
    "        # remove terms not in vocabulary\n",
    "        terms = list(filter(lambda t: t in vocabulary, terms))\n",
    "        # calculate weigths and produce tuples (term, weight)\n",
    "        term_weights = list(map(lambda t: (t, BIRModel.cj_weight(t, feedback)), terms))\n",
    "        # filter terms with negative weights\n",
    "        if BIRModel.PRUNE_NEGATIVE_WEIGHTS:\n",
    "            print('pruning negative weights')\n",
    "            term_weights = list(filter(lambda t: t[1] >= 0, term_weights))\n",
    "        # filter terms with small absolute weights\n",
    "        if BIRModel.PRUNE_WEIGHT_THRESHOLD:\n",
    "            print('pruning small weights')\n",
    "            term_weights = list(filter(lambda t: abs(t[1]) > BIRModel.PRUNE_WEIGHT_THRESHOLD, term_weights))\n",
    "        # select top-k terms based on absolute values\n",
    "        if BIRModel.PRUNE_TOPK:\n",
    "            print('pruning top-k weights')\n",
    "            term_weights = sorted(term_weights, key = lambda t: (-abs(t[1]),len(index[t[0]]),t[0]))[:BIRModel.PRUNE_TOPK]\n",
    "        return term_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-at-a-time for BIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel_DAAT(BIRModel):\n",
    "    \"\"\"\n",
    "        Implements the DAAT model for the BIR model using inverted index method.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def query(terms: list[str], k: int, feedback: Feedback, predicate: Callable[[int], bool] = None, selected_docs: set[int] = None):\n",
    "        # filter terms and obtain weights for terms in order of their importance \n",
    "        term_weights = BIRModel.filter_terms(terms, feedback)\n",
    "        \n",
    "        # get iterators for each term and fetch first posting\n",
    "        iters = [iter(index[term]) for (term, _) in term_weights]\n",
    "        nexts = [next(iter, None) for iter in iters]\n",
    "\n",
    "        # keep track of all retrieved documents and their score; stored as tuples (doc_id, score)\n",
    "        topk = TopKList(k, term_weights, predicate)\n",
    "        while not all(e is None for e in nexts):\n",
    "            # get smallest value from nexts, ignoring None values\n",
    "            smallest = min(nexts, key = lambda x: x if x is not None else float('inf'))\n",
    "            # if we have feedback, make sure document is either relevant or not assessed so far; if we have selected_docs, make sure document is in it\n",
    "            if not(BIRModel.PRUNE_NON_RELEVANT and feedback.is_not_relevant(smallest)) and (selected_docs == None or smallest in selected_docs):\n",
    "                # if so, add it to topk\n",
    "                score = sum([term_weights[i][1] for i in range(len(nexts)) if nexts[i] == smallest])\n",
    "                topk.add(smallest, score)\n",
    "            # for each entry in nexts, fetch next item if entry equals smallest\n",
    "            for i, e in enumerate(nexts):\n",
    "                if e is smallest:\n",
    "                    nexts[i] = next(iters[i], None)\n",
    "        \n",
    "        # finsihed, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-at-a-time for BIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel_TAAT(BIRModel):\n",
    "    \"\"\"\n",
    "        Implements the TAAT model for the BIR model using inverted index method.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def query(terms: list[str], k: int, feedback: Feedback, predicate: Callable[[int], bool] = None):\n",
    "        # filter terms and obtain weights for terms in order of their importance \n",
    "        term_weights = BIRModel.filter_terms(terms, feedback)\n",
    "        doc_scores = {}\n",
    "\n",
    "        # iterate over terms and fetch postings\n",
    "        for (term, weight) in term_weights:\n",
    "            for posting in index[term]:\n",
    "                # if document is not already in doc_scores, add it\n",
    "                if posting not in doc_scores:\n",
    "                    doc_scores[posting] = 0\n",
    "                # add weight to document score\n",
    "                doc_scores[posting] += weight\n",
    "\n",
    "        # we do not need a full sort of doc_scores, but can use the heap in TopKList\n",
    "        topk = TopKList(k, term_weights, predicate)\n",
    "        for doc, score in doc_scores.items():\n",
    "            topk.add(doc, score)\n",
    "        \n",
    "        # finsihed, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random data example\n",
    "### Create inverted index\n",
    "The next section generates random inverted index postings for a set of terms. It simulates the indexing process for Boolean retrieval by associating random document IDs with each term. The `vocabulary` dictionary defines terms and their desired document frequencies (as a %-figure). The generated postings are stored in the `index` dictionary, with each term mapped to a set of corresponding document IDs.\n",
    "\n",
    "* `nDocs = 100`: Defines the total number of documents (document IDs) as 100.\n",
    "* `index = {}`: Initializes an empty dictionary to store the postings for each term.\n",
    "* `DEBUG = False`: A debug flag (we use it later to illustrate code execution).\n",
    "* `vocabulary`: Defines a dictionary where each term is associated with its desired document frequency (expressed as a percentage).\n",
    "* `documents`: List of all documents with each entry holding the set of terms in the document\n",
    "\n",
    "`create_postings(term: str, docFreq: int = None)` takes a term (string) and an optional document frequency (docFreq, integer) as arguments. It generates random postings for the term by creating a set of document IDs. If docFreq is not provided, it generates a random document frequency between 1 and nDocs. The for-loop iterates through each term in the vocabulary dictionary and calls the create_postings function. For each term, it fetches the desired document frequency from the vocabulary (values are percentages) and passes it to create_postings.\n",
    "\n",
    "`is_relevant(doc: int)` returns True if document is relevant ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "DEBUG = False\n",
    "nDocs = 100\n",
    "index = {}\n",
    "documents = []\n",
    "vocabulary = {}\n",
    "\n",
    "# helper function to rate if newly encountered document is relevant\n",
    "def is_relevant(doc):\n",
    "    return random.random() < 0.8\n",
    "\n",
    "# helper function to create random postings with given document frequency\n",
    "def create_postings(term: str, docFreq: int = None):\n",
    "    # create sets with random ids\n",
    "    index[term] = sorted(random.sample(range(1, nDocs + 1), docFreq))\n",
    "    vocabulary[term] = docFreq\n",
    "    # extend feature vectors for documents\n",
    "    for doc in index[term]:\n",
    "        documents[doc].add(term)\n",
    "\n",
    "# set all feature vectors of documents to empty. We use sets since BIR uses set-of-word model\n",
    "for doc in range(nDocs + 1):\n",
    "    documents.append(set())\n",
    "\n",
    "# we use some animal terms to create random documents\n",
    "terms = ['dog', 'cat', 'horse', 'rabit', 'ostrich', 'bear', 'tiger', 'lion', 'bird']\n",
    "\n",
    "# call create_postings for each entry in vocabulary to create the inverted index\n",
    "for term in terms:\n",
    "    create_postings(term, random.randint(nDocs // 10, nDocs // 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the postings for each term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog        28   [2, 6, 8, 10, 13, 20, 23, 26, 27, 30, 32, 39, 44, 45, 51, 53, 55, 58, 59, 62, 65, 67, 73, 82, 84]\n",
      "cat        26   [2, 6, 11, 12, 15, 16, 22, 24, 26, 28, 32, 36, 38, 45, 47, 51, 57, 58, 68, 69, 70, 76, 79, 85, 88]\n",
      "horse      46   [5, 8, 9, 10, 12, 13, 15, 16, 17, 20, 24, 30, 34, 36, 37, 41, 42, 43, 44, 45, 46, 47, 48, 53, 54]\n",
      "rabit      40   [2, 4, 6, 7, 13, 15, 17, 21, 25, 28, 29, 30, 31, 38, 41, 43, 45, 46, 47, 48, 49, 51, 60, 61, 63]\n",
      "ostrich    34   [4, 11, 12, 13, 16, 18, 21, 23, 24, 25, 27, 32, 33, 35, 42, 44, 51, 52, 54, 55, 56, 59, 69, 70, 73]\n",
      "bear       50   [4, 5, 6, 7, 8, 12, 14, 15, 18, 20, 21, 22, 24, 30, 34, 35, 37, 38, 41, 42, 43, 45, 47, 48, 50]\n",
      "tiger      14   [9, 11, 18, 21, 32, 54, 60, 61, 63, 74, 75, 77, 88, 95]\n",
      "lion       49   [2, 4, 5, 6, 11, 12, 13, 19, 20, 25, 28, 30, 31, 32, 34, 36, 37, 40, 46, 47, 48, 50, 53, 54, 55]\n",
      "bird       50   [3, 5, 6, 7, 9, 10, 12, 16, 17, 21, 22, 27, 30, 33, 34, 35, 36, 37, 38, 42, 44, 45, 46, 50, 51]\n"
     ]
    }
   ],
   "source": [
    "# print postings with term and list of documents\n",
    "for term, posting in index.items():\n",
    "    print(term.ljust(10), str(len(posting)).ljust(4), sorted(posting[:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 set()\n",
      "2 {'dog', 'cat', 'lion', 'rabit'}\n",
      "3 {'bird'}\n",
      "4 {'bear', 'ostrich', 'rabit', 'lion'}\n",
      "5 {'horse', 'lion', 'bird', 'bear'}\n",
      "6 {'dog', 'rabit', 'bird', 'bear', 'lion', 'cat'}\n",
      "7 {'bear', 'rabit', 'bird'}\n",
      "8 {'horse', 'dog', 'bear'}\n",
      "9 {'horse', 'bird', 'tiger'}\n",
      "10 {'horse', 'dog', 'bird'}\n",
      "11 {'ostrich', 'cat', 'lion', 'tiger'}\n",
      "12 {'bird', 'bear', 'lion', 'horse', 'ostrich', 'cat'}\n",
      "13 {'dog', 'rabit', 'lion', 'horse', 'ostrich'}\n",
      "14 {'bear'}\n",
      "15 {'horse', 'cat', 'rabit', 'bear'}\n",
      "16 {'horse', 'ostrich', 'cat', 'bird'}\n",
      "17 {'horse', 'rabit', 'bird'}\n",
      "18 {'bear', 'ostrich', 'tiger'}\n",
      "19 {'lion'}\n",
      "20 {'horse', 'dog', 'lion', 'bear'}\n"
     ]
    }
   ],
   "source": [
    "# print a few documents\n",
    "for doc in range(20):\n",
    "    print(doc + 1, documents[doc + 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Initial step without feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird horse\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1   5 (0.20) {'horse', 'lion', 'bird', 'bear'}\n",
      "  +  2   9 (0.20) {'horse', 'bird', 'tiger'}\n",
      "  +  3  10 (0.20) {'horse', 'dog', 'bird'}\n",
      "  +  4  12 (0.20) {'bird', 'bear', 'lion', 'horse', 'ostrich', 'cat'}\n",
      "  -  5  16 (0.20) {'horse', 'ostrich', 'cat', 'bird'}\n",
      "     6  17 (0.20) {'horse', 'rabit', 'bird'}\n",
      "     7  30 (0.20) {'dog', 'rabit', 'bird', 'bear', 'lion', 'horse'}\n",
      "     8  34 (0.20) {'horse', 'lion', 'bird', 'bear'}\n",
      "     9  36 (0.20) {'horse', 'cat', 'lion', 'bird'}\n",
      "    10  37 (0.20) {'horse', 'lion', 'bird', 'bear'}\n",
      "\n",
      "           horse 0.17693070815907844\n",
      "            bird 0.019608471388376337\n",
      "\n",
      "next feedback: +5, +9, +10, +12, -16\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# initial step for \"bird horse\"\n",
    "query = ['bird', 'horse']\n",
    "k = 20\n",
    "feedback = Feedback()\n",
    "print(' '.join(query))\n",
    "\n",
    "# set pruning behavior\n",
    "BIRModel.PRUNE_NEGATIVE_WEIGHTS     = False\n",
    "BIRModel.PRUNE_WEIGHT_THRESHOLD     = False\n",
    "BIRModel.PRUNE_TOPK                 = False\n",
    "BIRModel.PRUNE_NON_RELEVANT         = True\n",
    "\n",
    "# (optional) enable a predicate for the filtering step\n",
    "predicate = None\n",
    "# predicate = lambda doc: doc % 2 == 0\n",
    "# predicate = lambda doc: doc % 2 == 1\n",
    "selected_docs = None\n",
    "# selected_docs = list(range(10))\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback, predicate, selected_docs=selected_docs)\n",
    "print_topk_and_get_feedback(topk, feedback, is_relevant, nFeedback = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Feedback step\n",
    "\n",
    "Adjust weights with feedback. Repeat runs (`Ctrl+Enter` to stay on cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird horse\n",
      "feedback: +3, +5, +7, +8, +9, +10, +12, +13, +15, +17, +20, +21, +24, +27, +30, +33, +34, +35, +36, +37, +38, +41, +42, +44, +45, +46, +47, +51, +53, +54, +55, +57, +58, +60, +63, +67, +69, +71, +72, +73, +74, +76, +77, +83, +84, +86, +88, +89, +92, +95, +97, -6, -16, -22, -43, -48, -50, -56, -62, -66, -70, -75, -78, -80, -94\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1   5 (1.12) {'horse', 'lion', 'bird', 'bear'}\n",
      "  +  2   9 (1.12) {'horse', 'bird', 'tiger'}\n",
      "  +  3  10 (1.12) {'horse', 'dog', 'bird'}\n",
      "  +  4  12 (1.12) {'bird', 'bear', 'lion', 'horse', 'ostrich', 'cat'}\n",
      "  +  5  17 (1.12) {'horse', 'rabit', 'bird'}\n",
      "  +  6  30 (1.12) {'dog', 'rabit', 'bird', 'bear', 'lion', 'horse'}\n",
      "  +  7  34 (1.12) {'horse', 'lion', 'bird', 'bear'}\n",
      "  +  8  36 (1.12) {'horse', 'cat', 'lion', 'bird'}\n",
      "  +  9  37 (1.12) {'horse', 'lion', 'bird', 'bear'}\n",
      "  + 10  42 (1.12) {'horse', 'ostrich', 'bird', 'bear'}\n",
      "\n",
      "            bird 0.7124112323752194\n",
      "           horse 0.41049445651316646\n",
      "\n",
      "next feedback: +3, +5, +7, +8, +9, +10, +12, +13, +15, +17, +20, +21, +24, +27, +30, +33, +34, +35, +36, +37, +38, +41, +42, +44, +45, +46, +47, +51, +53, +54, +55, +57, +58, +60, +63, +67, +69, +71, +72, +73, +74, +76, +77, +82, +83, +84, +85, +86, +88, +89, +90, +92, +95, +97, +100, -6, -16, -22, -43, -48, -50, -56, -62, -66, -70, -75, -78, -80, -93, -94\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(query))\n",
    "print_feedback(feedback)\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback, predicate, selected_docs=selected_docs)\n",
    "print_topk_and_get_feedback(topk, feedback, is_relevant, nFeedback = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small document example for DAAT and TAAT\n",
    "### Create inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from the exercise\n",
    "nDocs = 0\n",
    "index = {}\n",
    "documents = [set()]\n",
    "vocabulary = {}\n",
    "stopwords = set([\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
    "    'i', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was',\n",
    "    'were', 'will', 'with'\n",
    "])\n",
    "\n",
    "# helper function to rate if newly encountered document is relevant\n",
    "def is_relevant(doc):\n",
    "    return doc < 6\n",
    "\n",
    "def add_document(text: str):\n",
    "    global nDocs\n",
    "    nDocs += 1\n",
    "    terms = set()\n",
    "    for term in set(text.lower().split(' ')):\n",
    "        if term in stopwords:\n",
    "            continue\n",
    "        terms.add(term)\n",
    "        if term not in vocabulary:\n",
    "            index[term] = [nDocs]\n",
    "            vocabulary[term] = 1\n",
    "        else:\n",
    "            index[term].append(nDocs)\n",
    "            vocabulary[term] += 1\n",
    "    documents.append(terms)\n",
    "\n",
    "add_document(\"Human machine interface for Lab ABC computer applications\")\n",
    "add_document(\"A survey of user opinion of computer system response time\")\n",
    "add_document(\"The EPS user interface management system\")\n",
    "add_document(\"System and human system engineering testing of EPS\")\n",
    "add_document(\"Relation of user perceived response time to error measurement\")\n",
    "\n",
    "add_document(\"The generation of random binary unordered trees\")\n",
    "add_document(\"The intersection graph of paths in trees\")\n",
    "add_document(\"Graph minors IV Widths of trees and well quasi ordering\")\n",
    "add_document(\"Graph minors a survey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer       2    [1, 2]\n",
      "human          2    [1, 4]\n",
      "machine        1    [1]\n",
      "applications   1    [1]\n",
      "interface      2    [1, 3]\n",
      "lab            1    [1]\n",
      "abc            1    [1]\n",
      "survey         2    [2, 9]\n",
      "response       2    [2, 5]\n",
      "opinion        1    [2]\n",
      "user           3    [2, 3, 5]\n",
      "time           2    [2, 5]\n",
      "system         3    [2, 3, 4]\n",
      "eps            2    [3, 4]\n",
      "management     1    [3]\n",
      "engineering    1    [4]\n",
      "testing        1    [4]\n",
      "error          1    [5]\n",
      "perceived      1    [5]\n",
      "relation       1    [5]\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice \n",
    "\n",
    "# print postings with term and list of documents\n",
    "for term, posting in islice(index.items(), 20):\n",
    "    print(term.ljust(14), str(len(posting)).ljust(4), sorted(posting[:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 {'computer', 'human', 'applications', 'machine', 'interface', 'lab', 'abc'}\n",
      "2 {'survey', 'computer', 'response', 'opinion', 'user', 'time', 'system'}\n",
      "3 {'interface', 'user', 'eps', 'management', 'system'}\n",
      "4 {'engineering', 'human', 'eps', 'system', 'testing'}\n",
      "5 {'error', 'response', 'perceived', 'relation', 'user', 'time', 'measurement'}\n",
      "6 {'binary', 'trees', 'generation', 'random', 'unordered'}\n",
      "7 {'trees', 'paths', 'graph', 'intersection'}\n",
      "8 {'minors', 'iv', 'ordering', 'widths', 'graph', 'quasi', 'trees', 'well'}\n",
      "9 {'minors', 'graph', 'survey'}\n"
     ]
    }
   ],
   "source": [
    "# print all documents\n",
    "print()\n",
    "for doc in range(len(documents) - 1):\n",
    "    print(doc + 1, documents[doc + 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human computer interaction\n",
      "pruning negative weights\n",
      "pruning top-k weights\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1   1 (2.45) {'computer', 'human', 'applications', 'machine', 'interface', 'lab', 'abc'}\n",
      "  +  2   2 (1.22) {'survey', 'computer', 'response', 'opinion', 'user', 'time', 'system'}\n",
      "  +  3   4 (1.22) {'engineering', 'human', 'eps', 'system', 'testing'}\n",
      "\n",
      "        computer 1.2237754316221157\n",
      "           human 1.2237754316221157\n",
      "\n",
      "next feedback: +1, +2, +4\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# initial step for \"cat dog\"\n",
    "query = ['human', 'computer', 'interaction']\n",
    "k = 9\n",
    "feedback = Feedback()\n",
    "print(' '.join(query))\n",
    "\n",
    "# set behavior\n",
    "BIRModel.PRUNE_NEGATIVE_WEIGHTS     = True\n",
    "BIRModel.PRUNE_WEIGHT_THRESHOLD     = False\n",
    "BIRModel.PRUNE_TOPK                 = 5\n",
    "BIRModel.PRUNE_NON_RELEVANT         = False\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback)\n",
    "print_topk_and_get_feedback(topk, feedback, is_relevant, nFeedback = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback step\n",
    "\n",
    "Adjust weights with feedback. Repeat runs (`Ctrl+Enter` to stay on cell) + query expansion with terms from relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc applications computer engineering eps error human interaction interface lab machine management measurement opinion perceived relation response survey system testing time user\n",
      "feedback: +1, +2, +3, +4, +5, -6, -7, -8, -9\n",
      "pruning negative weights\n",
      "pruning top-k weights\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1   2 (6.93) {'survey', 'computer', 'response', 'opinion', 'user', 'time', 'system'}\n",
      "  +  2   3 (6.93) {'interface', 'user', 'eps', 'management', 'system'}\n",
      "  +  3   4 (6.26) {'engineering', 'human', 'eps', 'system', 'testing'}\n",
      "  +  4   1 (3.72) {'computer', 'human', 'applications', 'machine', 'interface', 'lab', 'abc'}\n",
      "  +  5   5 (2.53) {'error', 'response', 'perceived', 'relation', 'user', 'time', 'measurement'}\n",
      "\n",
      "          system 2.5336968139574325\n",
      "            user 2.5336968139574325\n",
      "        computer 1.8607523407150066\n",
      "             eps 1.8607523407150066\n",
      "           human 1.8607523407150066\n",
      "\n",
      "next feedback: +1, +2, +3, +4, +5, -6, -7, -8, -9\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "print(' '.join(query))\n",
    "print_feedback(feedback)\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback)\n",
    "print_topk_and_get_feedback(topk, feedback, is_relevant, nFeedback = 5, extra_feedback = True)\n",
    "\n",
    "query = sorted(set(query) | reduce(lambda terms, doc: terms | documents[doc], feedback.relevant, set()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
