{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIR retrieval with inverted files\n",
    "\n",
    "## Helper functions for the BIRModel\n",
    "\n",
    "### Feedback class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedback:\n",
    "    \"\"\"\n",
    "        Collects feedback for documents and provides\n",
    "        functions to check if document is assessed,\n",
    "        relevant or not relevant.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.assessed = set()\n",
    "        self.relevant = set()\n",
    "    \n",
    "    def is_initial_step(self) -> bool:\n",
    "        return len(self.assessed) == 0\n",
    "\n",
    "    def add(self, doc_id: int, relevant: bool) -> None:\n",
    "        self.assessed.add(doc_id)\n",
    "        if relevant:\n",
    "            self.relevant.add(doc_id)\n",
    "        \n",
    "    def is_relevant(self, doc_id: int) -> bool:\n",
    "        return doc_id in self.relevant\n",
    "    \n",
    "    def is_assessed(self, doc_id: int) -> bool:\n",
    "        return doc_id in self.assessed\n",
    "    \n",
    "    def is_not_relevant(self, doc_id: int) -> bool:\n",
    "        return (doc_id in self.assessed) and (doc_id not in self.relevant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopKList class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappop, heappush, nsmallest\n",
    "from typing import Callable\n",
    "\n",
    "class TopKList:\n",
    "    \"\"\"\n",
    "        Maintains a list of top-k documents. Initializer accepts\n",
    "        a list of tuples (term, weight) to provide information about\n",
    "        weights used by retrieval model. Implements the iter() interface.\n",
    "        Takes an optional predicate(doc_id: int) function to filter documents\n",
    "        before returning them. \n",
    "    \"\"\"\n",
    "    def __init__(self, k: int, term_weights: list[tuple[str,float]] = None, predicate: Callable[[int], bool] = None):\n",
    "        self.docs_heap = []\n",
    "        self.k = k\n",
    "        self.predicate = predicate\n",
    "        if term_weights:\n",
    "            self.term_weights = term_weights\n",
    "            self.terms = [term for term, _ in self.term_weights]\n",
    "            self.weights = dict(self.term_weights)\n",
    "    \n",
    "    def add(self, doc_id: int, score: float):\n",
    "        heappush(self.docs_heap, (-score, doc_id, {'id': doc_id, 'score': score}))\n",
    "        # optional (infrequent) pruning if heap grows too large\n",
    "\n",
    "    def __iter__(self):\n",
    "        rank = 0\n",
    "        while rank < self.k and len(self.docs_heap) > 0:\n",
    "            entry = heappop(self.docs_heap)[2]\n",
    "            if self.predicate == None or self.predicate(entry['id']):\n",
    "                rank += 1\n",
    "                entry['rank'] = rank\n",
    "                yield entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this global variables o drive the examples\n",
    "DEBUG = False\n",
    "nDocs = 100\n",
    "index = {}\n",
    "documents = {}\n",
    "vocabulary = {}\n",
    "\n",
    "def print_feedback(feedback, text = 'feedback'):\n",
    "    print(text + \":\", \", \".join([('+' if feedback.is_relevant(doc_id) else '-') + str(doc_id) for doc_id in sorted(feedback.assessed, key=lambda doc_id: (not feedback.is_relevant(doc_id), doc_id))]))\n",
    "\n",
    "# helper function to display result and get feedback\n",
    "def print_topk_and_get_feedback(topk: TopKList, feedback: Feedback, is_relevant: Callable[[int], bool], nFeedback = 5, extra_feedback: bool = False):\n",
    "    print(\"\\n  f  r  id score  document\\n-------------------------------------\")\n",
    "    for entry in topk:\n",
    "        # let's assume user provides feedback for n not yet assessed documents\n",
    "        if not feedback.is_assessed(entry['id']) and nFeedback > 0:\n",
    "            nFeedback -= 1\n",
    "            feedback.add(entry['id'], is_relevant(entry['id']))\n",
    "        relevancy = '+' if feedback.is_relevant(entry['id']) else '-' if feedback.is_assessed(entry['id']) else ' '\n",
    "        print(\"  {relevancy}{rank:>3d} {id:>3d} ({score:.2f})\".format(relevancy = relevancy, **entry), documents[entry['id']])\n",
    "    if extra_feedback:\n",
    "        for doc_id in filter(lambda doc_id: not feedback.is_assessed(doc_id), documents.keys()):\n",
    "            if nFeedback <= 0:\n",
    "                break\n",
    "            nFeedback -= 1\n",
    "            feedback.add(doc_id, is_relevant(doc_id))\n",
    "    print()\n",
    "    for term in sorted(topk.weights.keys(), key = lambda term: -topk.weights[term]):\n",
    "        print(term.rjust(16), topk.weights[term])\n",
    "    print_feedback(feedback, \"\\nnext feedback\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for pretty printing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIR Model Implementation\n",
    "\n",
    "### Base class\n",
    "\n",
    "- pruning of terms\n",
    "- setting weigths based on feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BIRModel:\n",
    "    \"\"\"\n",
    "        Generic class for the evaluation of the BIR model, inherited by the document-at-a-time (DAAT) and \n",
    "        term-at-a-time (TAAT) models. This superclass defines the cj-weights including filtering the most\n",
    "        important terms.\n",
    "    \"\"\"\n",
    "    # set this property to True to remove terms with negative weights\n",
    "    PRUNE_NEGATIVE_WEIGHTS = False\n",
    "\n",
    "    # set this property to remove terms with absolute weights smaller than this value\n",
    "    PRUNE_WEIGHT_THRESHOLD  = False\n",
    "\n",
    "    # set this property to select top-k weights based on absolute values\n",
    "    PRUNE_TOPK = False\n",
    "\n",
    "    # set this property to true to prune non-relevant documents from result list\n",
    "    PRUNE_NON_RELEVANT = False\n",
    "\n",
    "    @staticmethod\n",
    "    def cj_weight(term: str, feedback: Feedback):\n",
    "        docFreq = len(index[term])\n",
    "        if feedback.is_initial_step():\n",
    "            rj = 0.5\n",
    "            nj = (docFreq + 0.5) / (len(documents) + 1)\n",
    "            if DEBUG:\n",
    "                print(term, \"rj=\", rj, \"nj=\", nj, \"cj=\", math.log(rj / (1 - rj) * (1 - nj) / nj))\n",
    "        else:\n",
    "            # get postings as set to siplify calculations in Python\n",
    "            docs = set(index[term])\n",
    "            # number of assessed relevant documents which have the term\n",
    "            lj, L = len(feedback.relevant & docs), len(feedback.relevant)\n",
    "            # number of assessed documents which have the term\n",
    "            kj, K = len(feedback.assessed & docs), len(feedback.assessed)\n",
    "            # calculate rj and nj\n",
    "            rj = (lj + 0.5) / (L + 1)\n",
    "            nj = (kj - lj + 0.5) / (K - L + 1)\n",
    "            if DEBUG:\n",
    "                print(term, \"l=\", lj, \"/\", L, \"k=\", kj, \"/\", K, \"rj=\", rj, \"nj=\", nj, \"cj=\", math.log(rj / (1 - rj) * (1 - nj) / nj))\n",
    "        return math.log(rj / (1 - rj) * (1 - nj) / nj)\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_terms(terms: list[str], feedback: Feedback) -> list[tuple[str,float]]:\n",
    "        # remove terms not in vocabulary\n",
    "        terms = list(filter(lambda t: t in vocabulary, terms))\n",
    "        # calculate weigths and produce tuples (term, weight)\n",
    "        term_weights = list(map(lambda t: (t, BIRModel.cj_weight(t, feedback)), terms))\n",
    "        # filter terms with negative weights\n",
    "        if BIRModel.PRUNE_NEGATIVE_WEIGHTS:\n",
    "            print('pruning negative weights')\n",
    "            term_weights = list(filter(lambda t: t[1] >= 0, term_weights))\n",
    "        # filter terms with small absolute weights\n",
    "        if BIRModel.PRUNE_WEIGHT_THRESHOLD:\n",
    "            print('pruning small weights')\n",
    "            term_weights = list(filter(lambda t: abs(t[1]) > BIRModel.PRUNE_WEIGHT_THRESHOLD, term_weights))\n",
    "        # select top-k terms based on absolute values\n",
    "        if BIRModel.PRUNE_TOPK:\n",
    "            print('pruning top-k weights')\n",
    "            term_weights = sorted(term_weights, key = lambda t: (-abs(t[1]),len(index[t[0]]),t[0]))[:BIRModel.PRUNE_TOPK]\n",
    "        return term_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-at-a-time for BIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel_DAAT(BIRModel):\n",
    "    \"\"\"\n",
    "        Implements the DAAT model for the BIR model using inverted index method.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def query(terms: list[str], k: int, feedback: Feedback, predicate: Callable[[int], bool] = None, selected_docs: set[int] = None):\n",
    "        # filter terms and obtain weights for terms in order of their importance \n",
    "        term_weights = BIRModel.filter_terms(terms, feedback)\n",
    "        \n",
    "        # get iterators for each term and fetch first posting\n",
    "        iters = [iter(index[term]) for (term, _) in term_weights]\n",
    "        nexts = [next(iter, None) for iter in iters]\n",
    "\n",
    "        # keep track of all retrieved documents and their score; stored as tuples (doc_id, score)\n",
    "        topk = TopKList(k, term_weights, predicate)\n",
    "        while not all(e is None for e in nexts):\n",
    "            # get smallest value from nexts, ignoring None values\n",
    "            smallest = min(nexts, key = lambda x: x if x is not None else float('inf'))\n",
    "            # if we have feedback, make sure document is either relevant or not assessed so far; if we have selected_docs, make sure document is in it\n",
    "            if not(BIRModel.PRUNE_NON_RELEVANT and feedback.is_not_relevant(smallest)) and (selected_docs == None or smallest in selected_docs):\n",
    "                # if so, add it to topk\n",
    "                score = sum([term_weights[i][1] for i in range(len(nexts)) if nexts[i] == smallest])\n",
    "                topk.add(smallest, score)\n",
    "            # for each entry in nexts, fetch next item if entry equals smallest\n",
    "            for i, e in enumerate(nexts):\n",
    "                if e is smallest:\n",
    "                    nexts[i] = next(iters[i], None)\n",
    "        \n",
    "        # finsihed, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-at-a-time for BIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel_TAAT(BIRModel):\n",
    "    \"\"\"\n",
    "        Implements the TAAT model for the BIR model using inverted index method.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def query(terms: list[str], k: int, feedback: Feedback, predicate: Callable[[int], bool] = None):\n",
    "        # filter terms and obtain weights for terms in order of their importance \n",
    "        term_weights = BIRModel.filter_terms(terms, feedback)\n",
    "        doc_scores = {}\n",
    "\n",
    "        # iterate over terms and fetch postings\n",
    "        for (term, weight) in term_weights:\n",
    "            for posting in index[term]:\n",
    "                # if document is not already in doc_scores, add it\n",
    "                if posting not in doc_scores:\n",
    "                    doc_scores[posting] = 0\n",
    "                # add weight to document score\n",
    "                doc_scores[posting] += weight\n",
    "\n",
    "        # we do not need a full sort of doc_scores, but can use the heap in TopKList\n",
    "        topk = TopKList(k, term_weights, predicate)\n",
    "        for doc_id, score in doc_scores.items():\n",
    "            topk.add(doc_id, score)\n",
    "        \n",
    "        # finsihed, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random data example\n",
    "### Create inverted index\n",
    "The next section generates random inverted index postings for a set of terms. It simulates the indexing process for Boolean retrieval by associating random document IDs with each term. The `vocabulary` dictionary defines terms and their desired document frequencies (as a %-figure). The generated postings are stored in the `index` dictionary, with each term mapped to a set of corresponding document IDs.\n",
    "\n",
    "* `nDocs = 100`: Defines the total number of documents (document IDs) as 100.\n",
    "* `index = {}`: Initializes an empty dictionary to store the postings for each term.\n",
    "* `DEBUG = False`: A debug flag (we use it later to illustrate code execution).\n",
    "* `vocabulary`: Defines a dictionary where each term is associated with its desired document frequency (expressed as a percentage).\n",
    "* `documents`: List of all documents with each entry holding the set of terms in the document\n",
    "\n",
    "`create_postings(term: str, docFreq: int = None)` takes a term (string) and an optional document frequency (docFreq, integer) as arguments. It generates random postings for the term by creating a set of document IDs. If docFreq is not provided, it generates a random document frequency between 1 and nDocs. The for-loop iterates through each term in the vocabulary dictionary and calls the create_postings function. For each term, it fetches the desired document frequency from the vocabulary (values are percentages) and passes it to create_postings.\n",
    "\n",
    "`is_relevant(doc_id: int)` returns True if document is relevant ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "DEBUG = False\n",
    "nDocs = 100\n",
    "index = {}\n",
    "documents = {}\n",
    "vocabulary = {}\n",
    "\n",
    "# helper function to rate if newly encountered document is relevant\n",
    "def is_relevant(doc_id):\n",
    "    return random.random() < 0.8\n",
    "\n",
    "# helper function to create random postings with given document frequency\n",
    "def create_postings(term: str, docFreq: int = None):\n",
    "    # create sets with random ids\n",
    "    index[term] = sorted(random.sample(range(1, nDocs + 1), docFreq))\n",
    "    vocabulary[term] = docFreq\n",
    "    # extend feature vectors for documents\n",
    "    for doc_id in index[term]:\n",
    "        documents[doc_id].add(term)\n",
    "\n",
    "# set all feature vectors of documents to empty. We use sets since BIR uses set-of-word model\n",
    "for doc_id in range(1, nDocs + 1):\n",
    "    documents[doc_id] = set()\n",
    "\n",
    "# we use some animal terms to create random documents\n",
    "terms = ['dog', 'cat', 'horse', 'rabit', 'ostrich', 'bear', 'tiger', 'lion', 'bird']\n",
    "\n",
    "# call create_postings for each entry in vocabulary to create the inverted index\n",
    "for term in terms:\n",
    "    create_postings(term, random.randint(nDocs // 10, nDocs // 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the postings for each term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog        11   [17, 34, 37, 49, 59, 70, 77, 83, 88, 91, 94]\n",
      "cat        17   [9, 12, 15, 27, 31, 34, 37, 41, 51, 62, 63, 66, 69, 73, 76, 85, 88]\n",
      "horse      11   [1, 2, 11, 23, 24, 32, 56, 60, 66, 69, 98]\n",
      "rabit      16   [5, 8, 11, 16, 27, 32, 37, 52, 58, 61, 65, 67, 73, 76, 87, 97]\n",
      "ostrich    41   [3, 5, 6, 8, 11, 16, 17, 19, 21, 22, 24, 25, 27, 29, 31, 33, 35, 40, 41, 45, 49, 51, 52, 55, 57]\n",
      "bear       30   [3, 12, 22, 23, 25, 28, 30, 33, 41, 42, 43, 46, 53, 55, 59, 61, 62, 63, 64, 68, 69, 73, 79, 81, 82]\n",
      "tiger      50   [1, 8, 9, 11, 12, 17, 18, 19, 21, 22, 24, 25, 31, 35, 36, 38, 41, 42, 43, 46, 47, 48, 49, 50, 51]\n",
      "lion       26   [3, 5, 10, 16, 19, 23, 29, 32, 35, 40, 46, 55, 57, 62, 65, 67, 72, 73, 74, 75, 76, 79, 89, 90, 99]\n",
      "bird       39   [2, 4, 5, 6, 10, 11, 13, 16, 17, 19, 24, 25, 26, 29, 34, 35, 36, 37, 43, 46, 50, 53, 57, 58, 60]\n"
     ]
    }
   ],
   "source": [
    "# print postings with term and list of documents\n",
    "for term, posting in index.items():\n",
    "    print(term.ljust(10), str(len(posting)).ljust(4), sorted(posting[:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 {'tiger', 'horse'}\n",
      "  2 {'bird', 'horse'}\n",
      "  3 {'bear', 'ostrich', 'lion'}\n",
      "  4 {'bird'}\n",
      "  5 {'bird', 'ostrich', 'rabit', 'lion'}\n",
      "  6 {'bird', 'ostrich'}\n",
      "  7 set()\n",
      "  8 {'tiger', 'ostrich', 'rabit'}\n",
      "  9 {'cat', 'tiger'}\n",
      " 10 {'bird', 'lion'}\n",
      " 11 {'bird', 'horse', 'tiger', 'rabit', 'ostrich'}\n",
      " 12 {'cat', 'bear', 'tiger'}\n",
      " 13 {'bird'}\n",
      " 14 set()\n",
      " 15 {'cat'}\n",
      " 16 {'bird', 'ostrich', 'rabit', 'lion'}\n",
      " 17 {'bird', 'tiger', 'ostrich', 'dog'}\n",
      " 18 {'tiger'}\n",
      " 19 {'bird', 'ostrich', 'tiger', 'lion'}\n",
      " 20 set()\n"
     ]
    }
   ],
   "source": [
    "# print a few documents\n",
    "for doc_id in range(1, 21):\n",
    "    print('{id:>3d} {terms}'.format(id=doc_id, terms=documents[doc_id]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Initial step without feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird horse\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  -  1   2 (2.49) {'bird', 'horse'}\n",
      "  +  2  11 (2.49) {'bird', 'horse', 'tiger', 'rabit', 'ostrich'}\n",
      "  +  3  24 (2.49) {'bird', 'ostrich', 'tiger', 'horse'}\n",
      "  -  4  60 (2.49) {'bird', 'horse'}\n",
      "  +  5  66 (2.49) {'cat', 'bird', 'tiger', 'horse'}\n",
      "     6  98 (2.49) {'bird', 'ostrich', 'tiger', 'horse'}\n",
      "     7   1 (2.05) {'tiger', 'horse'}\n",
      "     8  23 (2.05) {'bear', 'lion', 'horse'}\n",
      "     9  32 (2.05) {'rabit', 'lion', 'horse'}\n",
      "    10  56 (2.05) {'tiger', 'horse'}\n",
      "\n",
      "           horse 2.0518915899116053\n",
      "            bird 0.4427365029053959\n",
      "\n",
      "next feedback: +11, +24, +66, -2, -60\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# initial step for \"bird horse\"\n",
    "query = ['bird', 'horse']\n",
    "k = 10\n",
    "feedback = Feedback()\n",
    "print(' '.join(query))\n",
    "\n",
    "# set pruning behavior\n",
    "BIRModel.PRUNE_NEGATIVE_WEIGHTS     = False\n",
    "BIRModel.PRUNE_WEIGHT_THRESHOLD     = False\n",
    "BIRModel.PRUNE_TOPK                 = False\n",
    "BIRModel.PRUNE_NON_RELEVANT         = True\n",
    "\n",
    "# (optional) enable a predicate for the filtering step\n",
    "predicate = None\n",
    "# predicate = lambda doc_id: doc_id % 2 == 0\n",
    "# predicate = lambda doc_id: doc_id % 2 == 1\n",
    "selected_docs = None\n",
    "# selected_docs = list(range(10))\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback, predicate, selected_docs=selected_docs)\n",
    "print_topk_and_get_feedback(topk, feedback, is_relevant, nFeedback = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Feedback step\n",
    "\n",
    "Adjust weights with feedback. Repeat runs (`Ctrl+Enter` to stay on cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird horse\n",
      "feedback: +1, +4, +6, +11, +23, +24, +32, +56, +66, +69, +98, -2, -5, -60\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1   1 (0.82) {'tiger', 'horse'}\n",
      "  +  2  23 (0.82) {'bear', 'lion', 'horse'}\n",
      "  +  3  32 (0.82) {'rabit', 'lion', 'horse'}\n",
      "  +  4  56 (0.82) {'tiger', 'horse'}\n",
      "  +  5  69 (0.82) {'horse', 'bear', 'tiger', 'cat', 'ostrich'}\n",
      "  +  6  11 (-0.95) {'bird', 'horse', 'tiger', 'rabit', 'ostrich'}\n",
      "  +  7  24 (-0.95) {'bird', 'ostrich', 'tiger', 'horse'}\n",
      "  +  8  66 (-0.95) {'cat', 'bird', 'tiger', 'horse'}\n",
      "  +  9  98 (-0.95) {'bird', 'ostrich', 'tiger', 'horse'}\n",
      "  + 10   4 (-1.78) {'bird'}\n",
      "\n",
      "           horse 0.8241754429663491\n",
      "            bird -1.7788560643921472\n",
      "\n",
      "next feedback: +1, +4, +6, +11, +23, +24, +32, +56, +66, +69, +98, -2, -5, -60\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(query))\n",
    "print_feedback(feedback)\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback, predicate, selected_docs=selected_docs)\n",
    "print_topk_and_get_feedback(topk, feedback, is_relevant, nFeedback = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small document example\n",
    "### Create inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from the exercise\n",
    "nDocs = 0\n",
    "index = {}\n",
    "documents = {}\n",
    "vocabulary = {}\n",
    "stopwords = set([\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
    "    'i', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was',\n",
    "    'were', 'will', 'with'\n",
    "])\n",
    "\n",
    "# helper function to rate if newly encountered document is relevant\n",
    "def is_relevant(doc_id):\n",
    "    return doc_id < 6\n",
    "\n",
    "def add_document(text: str):\n",
    "    global nDocs\n",
    "    nDocs += 1\n",
    "    terms = set()\n",
    "    for term in set(text.lower().split(' ')):\n",
    "        if term in stopwords:\n",
    "            continue\n",
    "        terms.add(term)\n",
    "        if term not in vocabulary:\n",
    "            index[term] = [nDocs]\n",
    "            vocabulary[term] = 1\n",
    "        else:\n",
    "            index[term].append(nDocs)\n",
    "            vocabulary[term] += 1\n",
    "    documents[nDocs] = terms\n",
    "\n",
    "add_document(\"Human machine interface for Lab ABC computer applications\")\n",
    "add_document(\"A survey of user opinion of computer system response time\")\n",
    "add_document(\"The EPS user interface management system\")\n",
    "add_document(\"System and human system engineering testing of EPS\")\n",
    "add_document(\"Relation of user perceived response time to error measurement\")\n",
    "\n",
    "add_document(\"The generation of random binary unordered trees\")\n",
    "add_document(\"The intersection graph of paths in trees\")\n",
    "add_document(\"Graph minors IV Widths of trees and well quasi ordering\")\n",
    "add_document(\"Graph minors a survey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab            1    [1]\n",
      "applications   1    [1]\n",
      "interface      2    [1, 3]\n",
      "human          2    [1, 4]\n",
      "machine        1    [1]\n",
      "computer       2    [1, 2]\n",
      "abc            1    [1]\n",
      "time           2    [2, 5]\n",
      "response       2    [2, 5]\n",
      "user           3    [2, 3, 5]\n",
      "system         3    [2, 3, 4]\n",
      "survey         2    [2, 9]\n",
      "opinion        1    [2]\n",
      "eps            2    [3, 4]\n",
      "management     1    [3]\n",
      "engineering    1    [4]\n",
      "testing        1    [4]\n",
      "perceived      1    [5]\n",
      "measurement    1    [5]\n",
      "error          1    [5]\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice \n",
    "\n",
    "# print postings with term and list of documents\n",
    "for term, posting in islice(index.items(), 20):\n",
    "    print(term.ljust(14), str(len(posting)).ljust(4), sorted(posting[:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 {'applications', 'lab', 'interface', 'human', 'machine', 'computer', 'abc'}\n",
      "  2 {'time', 'response', 'user', 'system', 'computer', 'survey', 'opinion'}\n",
      "  3 {'eps', 'interface', 'user', 'system', 'management'}\n",
      "  4 {'eps', 'engineering', 'human', 'system', 'testing'}\n",
      "  5 {'perceived', 'time', 'response', 'user', 'measurement', 'error', 'relation'}\n",
      "  6 {'random', 'binary', 'unordered', 'generation', 'trees'}\n",
      "  7 {'graph', 'paths', 'intersection', 'trees'}\n",
      "  8 {'graph', 'iv', 'well', 'minors', 'widths', 'ordering', 'quasi', 'trees'}\n",
      "  9 {'survey', 'graph', 'minors'}\n"
     ]
    }
   ],
   "source": [
    "# print all documents\n",
    "for doc_id, terms in documents.items():\n",
    "    print('{id:>3d} {terms}'.format(id=doc_id, terms=terms))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human computer interaction\n",
      "pruning negative weights\n",
      "pruning top-k weights\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1   1 (2.20) {'applications', 'lab', 'interface', 'human', 'machine', 'computer', 'abc'}\n",
      "  +  2   2 (1.10) {'time', 'response', 'user', 'system', 'computer', 'survey', 'opinion'}\n",
      "  +  3   4 (1.10) {'eps', 'engineering', 'human', 'system', 'testing'}\n",
      "\n",
      "        computer 1.0986122886681098\n",
      "           human 1.0986122886681098\n",
      "\n",
      "next feedback: +1, +2, +4\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# initial step for \"cat dog\"\n",
    "query = ['human', 'computer', 'interaction']\n",
    "k = 9\n",
    "feedback = Feedback()\n",
    "print(' '.join(query))\n",
    "\n",
    "# set behavior\n",
    "BIRModel.PRUNE_NEGATIVE_WEIGHTS     = True\n",
    "BIRModel.PRUNE_WEIGHT_THRESHOLD     = False\n",
    "BIRModel.PRUNE_TOPK                 = 5\n",
    "BIRModel.PRUNE_NON_RELEVANT         = False\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback)\n",
    "print_topk_and_get_feedback(topk, feedback, is_relevant, nFeedback = 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback step\n",
    "\n",
    "Adjust weights with feedback. Repeat runs (`Ctrl+Enter` to stay on cell) + query expansion with terms from relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc applications computer engineering eps error human interaction interface lab machine management measurement opinion perceived relation response survey system testing time user\n",
      "feedback: +1, +2, +3, +4, +5, -6, -7, -8, -9\n",
      "pruning negative weights\n",
      "pruning top-k weights\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1   2 (6.93) {'time', 'response', 'user', 'system', 'computer', 'survey', 'opinion'}\n",
      "  +  2   3 (6.93) {'eps', 'interface', 'user', 'system', 'management'}\n",
      "  +  3   4 (6.26) {'eps', 'engineering', 'human', 'system', 'testing'}\n",
      "  +  4   1 (3.72) {'applications', 'lab', 'interface', 'human', 'machine', 'computer', 'abc'}\n",
      "  +  5   5 (2.53) {'perceived', 'time', 'response', 'user', 'measurement', 'error', 'relation'}\n",
      "\n",
      "          system 2.5336968139574325\n",
      "            user 2.5336968139574325\n",
      "        computer 1.8607523407150066\n",
      "             eps 1.8607523407150066\n",
      "           human 1.8607523407150066\n",
      "\n",
      "next feedback: +1, +2, +3, +4, +5, -6, -7, -8, -9\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "print(' '.join(query))\n",
    "print_feedback(feedback)\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback)\n",
    "print_topk_and_get_feedback(topk, feedback, is_relevant, nFeedback = 5, extra_feedback = True)\n",
    "\n",
    "query = sorted(set(query) | reduce(lambda terms, doc_id: terms | documents[doc_id], feedback.relevant, set()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
