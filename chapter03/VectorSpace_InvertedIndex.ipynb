{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space retrieval with inverted files\n",
    "\n",
    "## Helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopKList class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappop, heappush, nsmallest\n",
    "from typing import Callable\n",
    "\n",
    "class TopKList:\n",
    "    \"\"\"\n",
    "        Maintains a list of top-k documents. Initializer accepts\n",
    "        a list of tuples (term, weight) to provide information about\n",
    "        weights used by retrieval model. Implements the iter() interface.\n",
    "        Takes an optional predicate(doc: int) function to filter documents\n",
    "        before returning them. For selective predicate, you may have to\n",
    "        adjust the pruning thresholds.\n",
    "    \"\"\"\n",
    "    # set this property to define the pruning threshold, None/False to turn pruning off\n",
    "    # pruning ensures that the number of elements in the heap ranges between\n",
    "    # first and second value of the tuple\n",
    "    PRUNING_THRESHOLDS = (1000, 2000)\n",
    "\n",
    "    def __init__(self, k: int, term_weights: list[tuple[str,float]] = None, predicate: Callable[[int], bool] = None):\n",
    "        self.docs_heap = []\n",
    "        self.k = k\n",
    "        self.predicate = predicate\n",
    "        if term_weights:\n",
    "            self.term_weights = term_weights\n",
    "            self.terms = [term for term, _ in self.term_weights]\n",
    "            self.weights = dict(self.term_weights)\n",
    "    \n",
    "    def add(self, doc: int, score: float):\n",
    "        heappush(self.docs_heap, (-score, doc, {'id': doc, 'score': score}))\n",
    "        # optional pruning if heap grows too large; be careful to not trigger each time for performance\n",
    "        if len(self.docs_heap) > TopKList.PRUNING_THRESHOLDS[1]:\n",
    "            self.docs_heap = nsmallest(TopKList.PRUNING_THRESHOLDS[0], self.docs_heap)\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = 0\n",
    "        while n < self.k and len(self.docs_heap) > 0:\n",
    "            doc = heappop(self.docs_heap)[2]\n",
    "            if self.predicate == None or self.predicate(doc['id']):\n",
    "                yield doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def idf(doc_freq: int, num_docs: int) -> float:\n",
    "    return math.log((num_docs + 1) / (doc_freq + 1))\n",
    "\n",
    "def idf_bm25(doc_freq: int, num_docs: int) -> float:\n",
    "    return math.log((num_docs - doc_freq + 0.5) / (doc_freq + 0.5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this global variables o drive the examples\n",
    "DEBUG = False\n",
    "nDocs = 100\n",
    "index = {}\n",
    "documents = []\n",
    "topk = None\n",
    "\n",
    "# helper function to display result and get feedback\n",
    "def display_and_get_feedback(n = 8, all_docs = False):\n",
    "    n = topk.size if n < 0 else n\n",
    "    print(\"\\n  f  r  id score  document\\n-------------------------------------\")\n",
    "    for rank, doc in enumerate(topk):\n",
    "        print(\"  {rank: >3d} {id: >3d} ({score:.2f})\".format(rank = rank + 1, **doc), documents[doc['id']])\n",
    "    print()\n",
    "    for term in sorted(topk.weights.keys(), key = lambda term: -topk.weights[term]):\n",
    "        print(term.rjust(16), topk.weights[term])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Model Implementation\n",
    "\n",
    "### Base class\n",
    "\n",
    "- scoring functions\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BIRModel:\n",
    "    \"\"\"\n",
    "        Generic class for the evaluation of the BIR model, inherited by the document-at-a-time (DAAT) and \n",
    "        term-at-a-time (TAAT) models. This superclass defines the cj-weights including filtering the most\n",
    "        important terms.\n",
    "    \"\"\"\n",
    "    # set this property to True to remove terms with negative weights\n",
    "    PRUNE_NEGATIVE_WEIGHTS = False\n",
    "\n",
    "    # set this property to remove terms with absolute weights smaller than this value\n",
    "    PRUNE_WEIGHT_THRESHOLD  = False\n",
    "\n",
    "    # set this property to select top-k weights based on absolute values\n",
    "    PRUNE_TOPK = False\n",
    "\n",
    "    # set this property to true to prune non-relevant documents from result list\n",
    "    PRUNE_NON_RELEVANT = False\n",
    "\n",
    "    @staticmethod\n",
    "    def cj_weight(term: str, feedback: Feedback):\n",
    "        docFreq = len(index[term])\n",
    "        if feedback.is_initial_step():\n",
    "            rj = 0.5\n",
    "            nj = (docFreq + 0.5) / (len(documents) + 1)\n",
    "            if DEBUG:\n",
    "                print(term, \"rj=\", rj, \"nj=\", nj, \"cj=\", math.log(rj / (1 - rj) * (1 - nj) / nj))\n",
    "        else:\n",
    "            # get postings as set to siplify calculations in Python\n",
    "            docs = set(index[term])\n",
    "            # number of assessed relevant documents which have the term\n",
    "            lj, L = len(feedback.relevant & docs), len(feedback.relevant)\n",
    "            # number of assessed documents which have the term\n",
    "            kj, K = len(feedback.assessed & docs), len(feedback.assessed)\n",
    "            # calculate rj and nj\n",
    "            rj = (lj + 0.5) / (L + 1)\n",
    "            nj = (kj - lj + 0.5) / (K - L + 1)\n",
    "            if DEBUG:\n",
    "                print(term, \"l=\", lj, \"/\", L, \"k=\", kj, \"/\", K, \"rj=\", rj, \"nj=\", nj, \"cj=\", math.log(rj / (1 - rj) * (1 - nj) / nj))\n",
    "        return math.log(rj / (1 - rj) * (1 - nj) / nj)\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_terms(terms: list[str], feedback: Feedback) -> list[tuple[str,float]]:\n",
    "        # remove terms not in vocabulary\n",
    "        terms = list(filter(lambda t: t in vocabulary, terms))\n",
    "        # calculate weigths and produce tuples (term, weight)\n",
    "        term_weights = list(map(lambda t: (t, BIRModel.cj_weight(t, feedback)), terms))\n",
    "        # filter terms with negative weights\n",
    "        if BIRModel.PRUNE_NEGATIVE_WEIGHTS:\n",
    "            print('pruning negative weights')\n",
    "            term_weights = list(filter(lambda t: t[1] >= 0, term_weights))\n",
    "        # filter terms with small absolute weights\n",
    "        if BIRModel.PRUNE_WEIGHT_THRESHOLD:\n",
    "            print('pruning small weights')\n",
    "            term_weights = list(filter(lambda t: abs(t[1]) > BIRModel.PRUNE_WEIGHT_THRESHOLD, term_weights))\n",
    "        # select top-k terms based on absolute values\n",
    "        if BIRModel.PRUNE_TOPK:\n",
    "            print('pruning top-k weights')\n",
    "            term_weights = sorted(term_weights, key = lambda t: (-abs(t[1]),len(index[t[0]]),t[0]))[:BIRModel.PRUNE_TOPK]\n",
    "        return term_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-at-a-time for BIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel_DAAT(BIRModel):\n",
    "    \"\"\"\n",
    "        Implements the DAAT model for the BIR model using inverted index method.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def query(terms: list[str], k: int, feedback: Feedback, predicate: Callable[[int], bool] = None):\n",
    "        # filter terms and obtain weights for terms in order of their importance \n",
    "        term_weights = BIRModel.filter_terms(terms, feedback)\n",
    "        \n",
    "        # get iterators for each term and fetch first posting\n",
    "        iters = [iter(index[term]) for (term, _) in term_weights]\n",
    "        nexts = [next(iter, None) for iter in iters]\n",
    "\n",
    "        # keep track of all retrieved documents and their score; stored as tuples (doc_id, score)\n",
    "        topk = TopKList(k, term_weights, predicate)\n",
    "        while not all(e is None for e in nexts):\n",
    "            # get smallest value from nexts, ignoring None values\n",
    "            smallest = min(nexts, key = lambda x: x if x is not None else float('inf'))\n",
    "            # if we have feedback, make sure document is either relevant or not assessed so far\n",
    "            if not(BIRModel.PRUNE_NON_RELEVANT and feedback.is_not_relevant(smallest)):\n",
    "                # if so, add it to topk\n",
    "                score = sum([term_weights[i][1] for i in range(len(nexts)) if nexts[i] == smallest])\n",
    "                topk.add(smallest, score)\n",
    "            # for each entry in nexts, fetch next item if entry equals smallest\n",
    "            for i, e in enumerate(nexts):\n",
    "                if e is smallest:\n",
    "                    nexts[i] = next(iters[i], None)\n",
    "        \n",
    "        # finsihed, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-at-a-time for BIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel_TAAT(BIRModel):\n",
    "    \"\"\"\n",
    "        Implements the TAAT model for the BIR model using inverted index method.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def query(terms: list[str], k: int, feedback: Feedback, predicate: Callable[[int], bool] = None):\n",
    "        # filter terms and obtain weights for terms in order of their importance \n",
    "        term_weights = BIRModel.filter_terms(terms, feedback)\n",
    "        doc_scores = {}\n",
    "\n",
    "        # iterate over terms and fetch postings\n",
    "        for (term, weight) in term_weights:\n",
    "            for posting in index[term]:\n",
    "                # if document is not already in doc_scores, add it\n",
    "                if posting not in doc_scores:\n",
    "                    doc_scores[posting] = 0\n",
    "                # add weight to document score\n",
    "                doc_scores[posting] += weight\n",
    "\n",
    "        # we do not need a full sort of doc_scores, but can use the heap in TopKList\n",
    "        topk = TopKList(k, term_weights, predicate)\n",
    "        for doc, score in doc_scores.items():\n",
    "            topk.add(doc, score)\n",
    "        \n",
    "        # finsihed, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random data example\n",
    "### Create inverted index\n",
    "The next section generates random inverted index postings for a set of terms. It simulates the indexing process for Boolean retrieval by associating random document IDs with each term. The `vocabulary` dictionary defines terms and their desired document frequencies (as a %-figure). The generated postings are stored in the `index` dictionary, with each term mapped to a set of corresponding document IDs.\n",
    "\n",
    "* `nDocs = 100`: Defines the total number of documents (document IDs) as 100.\n",
    "* `index = {}`: Initializes an empty dictionary to store the postings for each term.\n",
    "* `DEBUG = False`: A debug flag (we use it later to illustrate code execution).\n",
    "* `vocabulary`: Defines a dictionary where each term is associated with its desired document frequency (expressed as a percentage).\n",
    "* `documents`: List of all documents with each entry holding a dictionary {vector: dict, len: float, norm: float}\n",
    "  - vector holds the term freqeuncies as dictionary (key=term, value=term frequency)\n",
    "  - len is the number of terms in the document (its length)\n",
    "  - norm is the \n",
    "\n",
    "`create_postings(term: str, docFreq: int = None)` takes a term (string) and an optional document frequency (docFreq, integer) as arguments. It generates random postings for the term by creating a set of document IDs. If docFreq is not provided, it generates a random document frequency between 1 and nDocs. The for-loop iterates through each term in the vocabulary dictionary and calls the create_postings function. For each term, it fetches the desired document frequency from the vocabulary (values are percentages) and passes it to create_postings.\n",
    "\n",
    "`is_relevant(doc: int)` returns True if document is relevant ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "DEBUG = False\n",
    "nDocs = 40\n",
    "index = {}\n",
    "documents = []\n",
    "vocabulary = {}\n",
    "\n",
    "# helper function to create random postings with given document frequency\n",
    "def create_postings(term: str, docFreq: int = None):\n",
    "    # create sets with random ids\n",
    "    index[term] = []\n",
    "    vocabulary[term] = {'df': docFreq, 'idf': 0}\n",
    "    # extend feature vectors for documents with a random term frequency\n",
    "    for doc in sorted(random.sample(range(1, nDocs + 1), docFreq)):\n",
    "        # select a random term frequency for the term\n",
    "        tf = random.randint(1, 10)\n",
    "        index[term].append((doc, tf))\n",
    "        documents[doc]['terms'][term] = tf\n",
    "\n",
    "# set all feature vectors of documents to empty. We use sets since BIR uses set-of-word model\n",
    "for doc in range(nDocs + 1):\n",
    "    documents.append({'terms': {}, 'len': 0, 'norm': 0})\n",
    "\n",
    "# we use some animal terms to create random documents\n",
    "terms = ['dog', 'cat', 'horse', 'rabit', 'ostrich', 'bear', 'tiger', 'lion', 'bird']\n",
    "\n",
    "# call create_postings for each entry in vocabulary to create the inverted index\n",
    "for term in terms:\n",
    "    create_postings(term, random.randint(nDocs // 10, nDocs // 2))\n",
    "\n",
    "# now calculate the idf for each term and the norm for each document\n",
    "for item in vocabulary.values():\n",
    "    item['idf'] = idf(item['df'], nDocs)\n",
    "for doc in documents:\n",
    "    doc['len'] = sum([tf for _, tf in doc['terms'].items()])\n",
    "    doc['norm'] = sum([(tf * vocabulary[term]['idf']) ** 2 for term, tf in doc['terms'].items()]) ** 0.5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the postings for each term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog        5    1.92    [(2, 4), (11, 6), (17, 7), (30, 2), (32, 9)]\n",
      "cat        20   0.67    [(4, 7), (8, 6), (10, 8), (11, 1), (12, 9), (13, 9), (14, 4), (15, 4), (18, 2), (21, 1), (26, 4), (28, 9), (30, 5), (32, 6), (33, 6), (36, 10), (37, 4), (38, 10), (39, 8), (40, 5)]\n",
      "horse      13   1.07    [(7, 4), (9, 6), (10, 8), (11, 5), (13, 8), (15, 8), (19, 2), (23, 2), (27, 3), (34, 6), (38, 8), (39, 2), (40, 1)]\n",
      "rabit      17   0.82    [(1, 6), (4, 8), (8, 8), (9, 7), (15, 6), (16, 10), (17, 9), (19, 5), (23, 7), (25, 6), (27, 3), (28, 7), (29, 10), (31, 9), (32, 6), (33, 3), (34, 7)]\n",
      "ostrich    17   0.82    [(1, 2), (5, 4), (7, 3), (9, 7), (10, 10), (13, 8), (14, 3), (15, 9), (17, 8), (19, 3), (22, 8), (26, 10), (29, 4), (30, 5), (31, 9), (37, 4), (40, 6)]\n",
      "bear       17   0.82    [(2, 6), (5, 1), (7, 6), (9, 3), (10, 9), (11, 10), (12, 9), (13, 8), (14, 8), (18, 7), (27, 10), (31, 9), (32, 7), (35, 4), (38, 9), (39, 9), (40, 8)]\n",
      "tiger      20   0.67    [(3, 9), (5, 8), (6, 3), (7, 2), (11, 1), (14, 8), (17, 10), (18, 7), (20, 4), (21, 1), (24, 5), (25, 4), (26, 3), (27, 7), (29, 4), (30, 1), (32, 1), (36, 1), (38, 7), (39, 4)]\n",
      "lion       11   1.23    [(6, 6), (9, 5), (10, 9), (14, 4), (17, 2), (20, 7), (22, 5), (24, 6), (25, 2), (26, 9), (40, 3)]\n",
      "bird       14   1.01    [(2, 4), (3, 2), (4, 4), (7, 10), (8, 9), (11, 6), (15, 10), (17, 6), (18, 7), (22, 8), (27, 7), (29, 5), (33, 9), (40, 1)]\n"
     ]
    }
   ],
   "source": [
    "# print vocabulary with df and idf\n",
    "for term, item in vocabulary.items():\n",
    "    print(\"{term:10} {df:<4d} {idf:<7.2f} {postings}\".format(term=term.ljust(10), df=item['df'], idf=item['idf'], postings=index[term]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 | 8     5.21    {'rabit': 6, 'ostrich': 2}\n",
      "   2 | 14    9.98    {'dog': 4, 'bear': 6, 'bird': 4}\n",
      "   3 | 11    6.35    {'tiger': 9, 'bird': 2}\n",
      "   4 | 19    9.03    {'cat': 7, 'rabit': 8, 'bird': 4}\n",
      "   5 | 13    6.34    {'ostrich': 4, 'bear': 1, 'tiger': 8}\n",
      "   6 | 9     7.64    {'tiger': 3, 'lion': 6}\n",
      "   7 | 25    12.32   {'horse': 4, 'ostrich': 3, 'bear': 6, 'tiger': 2, 'bird': 10}\n",
      "   8 | 23    11.89   {'cat': 6, 'rabit': 8, 'bird': 9}\n",
      "   9 | 28    12.32   {'horse': 6, 'rabit': 7, 'ostrich': 7, 'bear': 3, 'lion': 5}\n",
      "  10 | 44    18.64   {'cat': 8, 'horse': 8, 'ostrich': 10, 'bear': 9, 'lion': 9}\n",
      "  11 | 29    16.34   {'dog': 6, 'cat': 1, 'horse': 5, 'bear': 10, 'tiger': 1, 'bird': 6}\n",
      "  12 | 18    9.55    {'cat': 9, 'bear': 9}\n",
      "  13 | 33    14.03   {'cat': 9, 'horse': 8, 'ostrich': 8, 'bear': 8}\n",
      "  14 | 27    10.46   {'cat': 4, 'ostrich': 3, 'bear': 8, 'tiger': 8, 'lion': 4}\n",
      "  15 | 37    16.17   {'cat': 4, 'horse': 8, 'rabit': 6, 'ostrich': 9, 'bird': 10}\n",
      "  16 | 10    8.23    {'rabit': 10}\n",
      "  17 | 42    19.14   {'dog': 7, 'rabit': 9, 'ostrich': 8, 'tiger': 10, 'lion': 2, 'bird': 6}\n",
      "  18 | 23    10.32   {'cat': 2, 'bear': 7, 'tiger': 7, 'bird': 7}\n",
      "  19 | 10    5.26    {'horse': 2, 'rabit': 5, 'ostrich': 3}\n",
      "  20 | 11    9.01    {'tiger': 4, 'lion': 7}\n"
     ]
    }
   ],
   "source": [
    "# print a few documents\n",
    "for doc in range(20):\n",
    "    print(\"{id:>4} | {len:<5d} {norm:<7.2f} {terms}\".format(id=doc + 1, len=documents[doc + 1]['len'], norm=documents[doc + 1]['norm'], terms=str(documents[doc + 1]['terms'])))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Initial step without feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird horse\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1  78 (2.36) {'horse', 'bird'}\n",
      "  +  2  80 (2.36) {'horse', 'rabit', 'bird'}\n",
      "  +  3  34 (1.51) {'lion', 'horse'}\n",
      "  +  4  38 (1.51) {'horse'}\n",
      "  +  5  44 (1.51) {'ostrich', 'horse', 'rabit', 'cat'}\n",
      "  +  6  58 (1.51) {'ostrich', 'tiger', 'dog', 'horse', 'cat'}\n",
      "  +  7  68 (1.51) {'ostrich', 'horse'}\n",
      "  -  8  82 (1.51) {'ostrich', 'horse'}\n",
      "     9  96 (1.51) {'horse', 'cat'}\n",
      "    10  12 (0.85) {'dog', 'ostrich', 'bird'}\n",
      "\n",
      "           horse 1.5070758997725306\n",
      "            bird 0.8519707660865959\n",
      "\n",
      "next feedback: +34, +38, +44, +58, +68, +78, +80, -82\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# initial step for \"cat dog\"\n",
    "query = ['bird', 'horse']\n",
    "k = 20\n",
    "feedback = Feedback()\n",
    "print(' '.join(query))\n",
    "\n",
    "# set pruning behavior\n",
    "BIRModel.PRUNE_NEGATIVE_WEIGHTS     = False\n",
    "BIRModel.PRUNE_WEIGHT_THRESHOLD     = False\n",
    "BIRModel.PRUNE_TOPK                 = False\n",
    "BIRModel.PRUNE_NON_RELEVANT         = True\n",
    "\n",
    "# (optional) enable a predicate for the filtering step\n",
    "predicate = None\n",
    "# predicate = lambda doc: doc % 2 == 0\n",
    "# predicate = lambda doc: doc % 2 == 1\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback, predicate)\n",
    "display_and_get_feedback()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Feedback step\n",
    "\n",
    "Adjust weights with feedback. Repeat runs (`Ctrl+Enter` to stay on cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird horse\n",
      "feedback: +12, +34, +36, +38, +44, +48, +52, +58, +68, +76, +78, +80, +98, -28, -60, -82, -96\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1  78 (0.58) {'horse', 'bird'}\n",
      "  +  2  80 (0.58) {'horse', 'rabit', 'bird'}\n",
      "  +  3  12 (0.44) {'dog', 'ostrich', 'bird'}\n",
      "  +  4  36 (0.44) {'ostrich', 'bird', 'dog', 'lion', 'cat'}\n",
      "  +  5  48 (0.44) {'dog', 'ostrich', 'bird', 'cat'}\n",
      "  +  6  52 (0.44) {'bird'}\n",
      "  +  7  76 (0.44) {'ostrich', 'bear', 'bird', 'lion', 'rabit'}\n",
      "  +  8  98 (0.44) {'ostrich', 'bird'}\n",
      "  +  9  34 (0.14) {'lion', 'horse'}\n",
      "  + 10  38 (0.14) {'horse'}\n",
      "\n",
      "            bird 0.4353180712578454\n",
      "           horse 0.14310084364067324\n",
      "\n",
      "next feedback: +12, +34, +36, +38, +44, +48, +52, +58, +68, +76, +78, +80, +98, -28, -60, -82, -96\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(query))\n",
    "print_feedback()\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback, predicate)\n",
    "display_and_get_feedback()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small document example for DAAT and TAAT\n",
    "### Create inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from the exercise\n",
    "nDocs = 0\n",
    "index = {}\n",
    "documents = [set()]\n",
    "vocabulary = {}\n",
    "stopwords = set([\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
    "    'i', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was',\n",
    "    'were', 'will', 'with'\n",
    "])\n",
    "\n",
    "# helper function to rate if newly encountered document is relevant\n",
    "def is_relevant(doc):\n",
    "    return doc < 6\n",
    "\n",
    "def add_document(text: str):\n",
    "    global nDocs\n",
    "    nDocs += 1\n",
    "    terms = set()\n",
    "    for term in set(text.lower().split(' ')):\n",
    "        if term in stopwords:\n",
    "            continue\n",
    "        terms.add(term)\n",
    "        if term not in vocabulary:\n",
    "            index[term] = [nDocs]\n",
    "            vocabulary[term] = 1\n",
    "        else:\n",
    "            index[term].append(nDocs)\n",
    "            vocabulary[term] += 1\n",
    "    documents.append(terms)\n",
    "\n",
    "add_document(\"Human machine interface for Lab ABC computer applications\")\n",
    "add_document(\"A survey of user opinion of computer system response time\")\n",
    "add_document(\"The EPS user interface management system\")\n",
    "add_document(\"System and human system engineering testing of EPS\")\n",
    "add_document(\"Relation of user perceived response time to error measurement\")\n",
    "\n",
    "add_document(\"The generation of random binary unordered trees\")\n",
    "add_document(\"The intersection graph of paths in trees\")\n",
    "add_document(\"Graph minors IV Widths of trees and well quasi ordering\")\n",
    "add_document(\"Graph minors a survey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interface      2    [1, 3]\n",
      "human          2    [1, 4]\n",
      "lab            1    [1]\n",
      "machine        1    [1]\n",
      "computer       2    [1, 2]\n",
      "applications   1    [1]\n",
      "abc            1    [1]\n",
      "opinion        1    [2]\n",
      "user           3    [2, 3, 5]\n",
      "system         3    [2, 3, 4]\n",
      "time           2    [2, 5]\n",
      "response       2    [2, 5]\n",
      "survey         2    [2, 9]\n",
      "management     1    [3]\n",
      "eps            2    [3, 4]\n",
      "engineering    1    [4]\n",
      "testing        1    [4]\n",
      "measurement    1    [5]\n",
      "relation       1    [5]\n",
      "error          1    [5]\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice \n",
    "\n",
    "# print postings with term and list of documents\n",
    "for term, posting in islice(index.items(), 20):\n",
    "    print(term.ljust(14), str(len(posting)).ljust(4), sorted(posting[:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 {'interface', 'human', 'machine', 'lab', 'computer', 'applications', 'abc'}\n",
      "2 {'opinion', 'computer', 'user', 'system', 'time', 'response', 'survey'}\n",
      "3 {'interface', 'management', 'user', 'system', 'eps'}\n",
      "4 {'human', 'engineering', 'testing', 'system', 'eps'}\n",
      "5 {'measurement', 'relation', 'user', 'time', 'error', 'response', 'perceived'}\n",
      "6 {'trees', 'unordered', 'random', 'binary', 'generation'}\n",
      "7 {'intersection', 'graph', 'paths', 'trees'}\n",
      "8 {'widths', 'well', 'trees', 'graph', 'minors', 'quasi', 'iv', 'ordering'}\n",
      "9 {'graph', 'minors', 'survey'}\n"
     ]
    }
   ],
   "source": [
    "# print all documents\n",
    "print()\n",
    "for doc in range(len(documents) - 1):\n",
    "    print(doc + 1, documents[doc + 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human computer interaction\n",
      "pruning negative weights\n",
      "pruning top-k weights\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1   1 (2.45) {'interface', 'human', 'machine', 'lab', 'computer', 'applications', 'abc'}\n",
      "  +  2   2 (1.22) {'opinion', 'computer', 'user', 'system', 'time', 'response', 'survey'}\n",
      "  +  3   4 (1.22) {'human', 'engineering', 'testing', 'system', 'eps'}\n",
      "\n",
      "        computer 1.2237754316221157\n",
      "           human 1.2237754316221157\n",
      "\n",
      "next feedback: +1, +2, +3, +4, +5, -6, -7, -8, -9\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# initial step for \"cat dog\"\n",
    "query = ['human', 'computer', 'interaction']\n",
    "k = 9\n",
    "feedback = Feedback()\n",
    "print(' '.join(query))\n",
    "\n",
    "# set behavior\n",
    "BIRModel.PRUNE_NEGATIVE_WEIGHTS     = True\n",
    "BIRModel.PRUNE_WEIGHT_THRESHOLD     = False\n",
    "BIRModel.PRUNE_TOPK                 = 5\n",
    "BIRModel.PRUNE_NON_RELEVANT         = False\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback)\n",
    "display_and_get_feedback(k, all_docs = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback step\n",
    "\n",
    "Adjust weights with feedback. Repeat runs (`Ctrl+Enter` to stay on cell) + query expansion with terms from relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m reduce\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(query))\n\u001b[1;32m      3\u001b[0m print_feedback()\n\u001b[1;32m      5\u001b[0m \u001b[39m# run query, display result, and get feedback\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "print(' '.join(query))\n",
    "print_feedback()\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback)\n",
    "display_and_get_feedback(k)\n",
    "\n",
    "query = sorted(set(query) | reduce(lambda terms, doc: terms | documents[doc], feedback.relevant, set()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(10, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
