{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space retrieval with inverted files\n",
    "\n",
    "## Helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopKList class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappop, heappush, nsmallest\n",
    "from typing import Callable\n",
    "\n",
    "class TopKList:\n",
    "    \"\"\"\n",
    "        Maintains a list of top-k documents. Initializer accepts\n",
    "        a list of tuples (term, weight) to provide information about\n",
    "        weights used by retrieval model. Implements the iter() interface.\n",
    "        Takes an optional predicate(doc_id: int) function to filter documents\n",
    "        before returning them. \n",
    "    \"\"\"\n",
    "    def __init__(self, k: int, term_weights: list[tuple[str,float]] = None, predicate: Callable[[int], bool] = None):\n",
    "        self.docs_heap = []\n",
    "        self.k = k\n",
    "        self.predicate = predicate\n",
    "        if term_weights:\n",
    "            self.term_weights = term_weights\n",
    "            self.terms = [term for term, _ in self.term_weights]\n",
    "            self.weights = dict(self.term_weights)\n",
    "    \n",
    "    def add(self, doc_id: int, score: float):\n",
    "        heappush(self.docs_heap, (-score, doc_id, {'id': doc_id, 'score': score}))\n",
    "        # optional (infrequent) pruning if heap grows too large\n",
    "\n",
    "    def __iter__(self):\n",
    "        rank = 0\n",
    "        while rank < self.k and len(self.docs_heap) > 0:\n",
    "            entry = heappop(self.docs_heap)[2]\n",
    "            if self.predicate == None or self.predicate(entry['id']):\n",
    "                rank += 1\n",
    "                entry['rank'] = rank\n",
    "                yield entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def idf(doc_freq: int, num_docs: int) -> float:\n",
    "    return math.log((num_docs + 1) / (doc_freq + 1))\n",
    "\n",
    "def idf_bm25(doc_freq: int, num_docs: int) -> float:\n",
    "    return math.log((num_docs - doc_freq + 0.5) / (doc_freq + 0.5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this global variables o drive the examples\n",
    "DEBUG = False\n",
    "nDocs = 100\n",
    "index = {}\n",
    "documents = []\n",
    "vocabulary = {}\n",
    "\n",
    "# helper function to display result and get feedback\n",
    "def print_topk(topk):\n",
    "    print(\"\\n    r  id score  document\\n-------------------------------------\")\n",
    "    for entry in topk:\n",
    "        print(\"  {rank: >3d} {id: >3d} ({score:.2f})\".format(**entry), documents[entry['id']])\n",
    "    print()\n",
    "    for term in sorted(topk.weights.keys(), key = lambda term: -topk.weights[term]):\n",
    "        print(term.rjust(16), topk.weights[term])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Model Implementation\n",
    "\n",
    "### Scoring functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VSMeasure: pass\n",
    "\n",
    "# implements the cosine measure\n",
    "class CosineMeasure(VSMeasure):\n",
    "    def __init__(self, query_vector: dict[str, int]):\n",
    "        self.query_vector_normalized = {}\n",
    "        self.term_weights = []\n",
    "        self.query_norm = 0\n",
    "        for term in query_vector.keys():\n",
    "            if term in vocabulary:\n",
    "                idf_2 = vocabulary[term]['idf'] ** 2\n",
    "                self.query_vector_normalized[term] = query_vector[term] * idf_2\n",
    "                self.term_weights.append((term, vocabulary[term]['idf']))\n",
    "                self.query_norm += idf_2 * query_vector[term] ** 2\n",
    "        self.query_norm = self.query_norm ** 0.5\n",
    "        for term in self.query_vector_normalized.keys():\n",
    "            self.query_vector_normalized[term] /= self.query_norm\n",
    "    \n",
    "    def similarity(self, doc_id: int, doc_vector: dict[str, int] = None):\n",
    "        if not doc_vector: \n",
    "            doc_vector = documents[doc_id]['vector']\n",
    "        dot_product = sum([doc_vector.get(term, 0) * q for (term, q) in self.query_vector_normalized.items()])\n",
    "        return dot_product / documents[doc_id]['norm']\n",
    "\n",
    "# implements the dot product\n",
    "class DotProduct(VSMeasure):\n",
    "    def __init__(self, query_vector: dict[str, int]):\n",
    "        self.query_vector_idf2 = {}\n",
    "        self.term_weights = []\n",
    "        for term in query_vector.keys():\n",
    "            if term in vocabulary:\n",
    "                idf = vocabulary[term]['idf']\n",
    "                self.query_vector_idf2[term] = query_vector[term] * idf ** 2\n",
    "                self.term_weights.append((term, idf))\n",
    "    \n",
    "    def similarity(self, doc_id: int, doc_vector: dict[str, int] = None):\n",
    "        if not doc_vector:\n",
    "            doc_vector = documents[doc_id]['vector']\n",
    "        dot_product = sum([doc_vector.get(term, 0) * q for (term, q) in self.query_vector_idf2.items()])\n",
    "        return dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class VSModel:\n",
    "    \"\"\"\n",
    "        Generic class for the evaluation of the Vector Space model, inherited by the document-at-a-time (DAAT) and \n",
    "        term-at-a-time (TAAT) implementation. This superclass defines the idf-weights including filtering the most\n",
    "        important terms.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def get_similarity_measure(measure: str, query_vector: dict[str, int]) -> VSMeasure:\n",
    "        return {\n",
    "            'cosine': CosineMeasure(query_vector)\n",
    "        }[measure]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-at-a-time for Vector Space Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VSModel_DAAT(VSModel):\n",
    "    \"\"\"\n",
    "        Implements the DAAT model for the Vector Space model using inverted index method.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def query(query_vector: dict[str, int], k: int, measure: str = 'dot', predicate: Callable[[int], bool] = None, selected_docs: set[int] = None):\n",
    "        # determine simialrity measure for this query\n",
    "        sim = VSModel.get_similarity_measure(measure, query_vector)\n",
    "        \n",
    "        # get iterators for each term and fetch first posting; postings have form (term, tf)\n",
    "        iters = [iter(index[term]) for (term, _) in sim.term_weights]\n",
    "        nexts = [next(iter, None) for iter in iters]\n",
    "\n",
    "        # keep track of all retrieved documents and their score; stored as tuples (doc_id, score)\n",
    "        topk = TopKList(k, sim.term_weights, predicate)\n",
    "        while not all(e is None for e in nexts):\n",
    "            # get smallest value from nexts, ignoring None values\n",
    "            smallest = min(nexts, key = lambda x: x[0] if x is not None else float('inf'))[0]\n",
    "            # create a document vector with only the query terms for the document with id = smallest\n",
    "            doc_query_terms = {sim.term_weights[i][0]: nexts[i][1] for i in range(len(nexts)) if nexts[i] and nexts[i][0] == smallest}\n",
    "            score = sim.similarity(smallest, doc_query_terms)\n",
    "            # assert score == sim.similarity(smallest)\n",
    "            topk.add(smallest, score)\n",
    "            # for each entry in nexts, fetch next item if entry equals smallest\n",
    "            for i, e in enumerate(nexts):\n",
    "                if e and e[0] is smallest:\n",
    "                    nexts[i] = next(iters[i], None)\n",
    "        \n",
    "        # finsihed, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-at-a-time for Vector Space Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random data example\n",
    "### Create inverted index\n",
    "The next section generates random inverted index postings for a set of terms. It simulates the indexing process for Boolean retrieval by associating random document IDs with each term. The `vocabulary` dictionary defines terms and their desired document frequencies (as a %-figure). The generated postings are stored in the `index` dictionary, with each term mapped to a set of corresponding document IDs.\n",
    "\n",
    "* `nDocs = 100`: Defines the total number of documents (document IDs) as 100.\n",
    "* `index = {}`: Initializes an empty dictionary to store the postings for each term.\n",
    "* `DEBUG = False`: A debug flag (we use it later to illustrate code execution).\n",
    "* `vocabulary`: Defines a dictionary where each term is associated with its desired document frequency (expressed as a percentage).\n",
    "* `documents`: List of all documents with each entry holding a dictionary {vector: dict, len: float, norm: float}\n",
    "  - vector holds the term freqeuncies as dictionary (key=term, value=term frequency)\n",
    "  - len is the number of terms in the document (its length)\n",
    "  - norm is the \n",
    "\n",
    "`create_postings(term: str, docFreq: int = None)` takes a term (string) and an optional document frequency (docFreq, integer) as arguments. It generates random postings for the term by creating a set of document IDs. If docFreq is not provided, it generates a random document frequency between 1 and nDocs. The for-loop iterates through each term in the vocabulary dictionary and calls the create_postings function. For each term, it fetches the desired document frequency from the vocabulary (values are percentages) and passes it to create_postings.\n",
    "\n",
    "`is_relevant(doc_id: int)` returns True if document is relevant ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "DEBUG = False\n",
    "nDocs = 40\n",
    "index = {}\n",
    "documents = {}\n",
    "vocabulary = {}\n",
    "\n",
    "# helper function to create random postings with given document frequency\n",
    "def create_postings(term: str, docFreq: int = None):\n",
    "    # create sets with random ids\n",
    "    index[term] = []\n",
    "    vocabulary[term] = {'df': docFreq, 'idf': 0}\n",
    "    # extend feature vectors for documents with a random term frequency\n",
    "    for doc_id in sorted(random.sample(sorted(documents.keys()), docFreq)):\n",
    "        # select a random term frequency for the term\n",
    "        tf = random.randint(1, 10)\n",
    "        index[term].append((doc_id, tf))\n",
    "        documents[doc_id]['vector'][term] = tf\n",
    "\n",
    "# set all feature vectors of documents to empty. We use sets since BIR uses set-of-word model\n",
    "for doc_id in range(1, nDocs + 1):\n",
    "    documents[doc_id] = {'id': doc_id, 'vector': {}, 'len': 0, 'norm': 0}\n",
    "\n",
    "# we use some animal terms to create random documents\n",
    "terms = ['dog', 'cat', 'horse', 'rabit', 'ostrich', 'bear', 'tiger', 'lion', 'bird']\n",
    "\n",
    "# call create_postings for each entry in vocabulary to create the inverted index\n",
    "for term in terms:\n",
    "    create_postings(term, random.randint(nDocs // 10, nDocs // 2))\n",
    "\n",
    "# now calculate the idf for each term and the norm for each document\n",
    "for item in vocabulary.values():\n",
    "    item['idf'] = idf(item['df'], nDocs)\n",
    "    item['idf_bm25'] = idf_bm25(item['df'], nDocs)\n",
    "for doc in documents.values():\n",
    "    doc['len'] = sum([tf for _, tf in doc['vector'].items()])\n",
    "    doc['norm'] = sum([(tf * vocabulary[term]['idf']) ** 2 for term, tf in doc['vector'].items()]) ** 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the postings for each term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog        9    1.41    1.20    [(7, 8), (16, 2), (17, 7), (21, 8), (22, 2), (26, 4), (31, 9), (34, 9), (38, 9)]\n",
      "cat        15   0.94    0.50    [(2, 5), (3, 7), (4, 8), (6, 2), (13, 1), (15, 9), (18, 3), (22, 3), (26, 6), (28, 10), (30, 6), (31, 10), (36, 4), (39, 8), (40, 8)]\n",
      "horse      11   1.23    0.94    [(4, 8), (9, 2), (11, 8), (14, 5), (17, 9), (19, 1), (20, 10), (27, 9), (35, 10), (37, 6), (39, 10)]\n",
      "rabit      5    1.92    1.86    [(4, 10), (20, 10), (28, 6), (30, 7), (31, 1)]\n",
      "ostrich    14   1.01    0.60    [(2, 9), (9, 3), (10, 5), (12, 5), (15, 1), (17, 7), (20, 1), (21, 9), (23, 7), (26, 6), (27, 4), (29, 2), (33, 10), (37, 2)]\n",
      "bear       15   0.94    0.50    [(2, 3), (5, 2), (7, 2), (9, 7), (14, 6), (16, 4), (19, 7), (20, 4), (23, 3), (24, 9), (25, 6), (28, 4), (31, 2), (38, 9), (39, 3)]\n",
      "tiger      8    1.52    1.34    [(10, 5), (13, 6), (16, 5), (22, 3), (30, 7), (32, 6), (37, 4), (38, 9)]\n",
      "lion       12   1.15    0.82    [(1, 5), (2, 6), (9, 2), (12, 5), (16, 10), (17, 6), (22, 9), (27, 2), (30, 4), (32, 5), (34, 7), (39, 7)]\n",
      "bird       11   1.23    0.94    [(3, 4), (4, 2), (10, 9), (13, 5), (19, 9), (30, 5), (32, 1), (36, 6), (37, 1), (38, 2), (40, 7)]\n"
     ]
    }
   ],
   "source": [
    "# print vocabulary with df and idf\n",
    "for term, item in vocabulary.items():\n",
    "    print(\"{term:10} {df:<4d} {idf:<7.2f} {idf_bm25:<7.2f} {postings}\".format(term=term.ljust(10), df=item['df'], idf=item['idf'], idf_bm25=item['idf_bm25'], postings=index[term]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 | 5     5.74    {'lion': 5}\n",
      "   2 | 23    12.63   {'cat': 5, 'ostrich': 9, 'bear': 3, 'lion': 6}\n",
      "   3 | 11    8.22    {'cat': 7, 'bird': 4}\n",
      "   4 | 28    22.99   {'cat': 8, 'horse': 8, 'rabit': 10, 'bird': 2}\n",
      "   5 | 2     1.88    {'bear': 2}\n",
      "   6 | 2     1.88    {'cat': 2}\n",
      "   7 | 10    11.44   {'dog': 8, 'bear': 2}\n",
      "   8 | 0     0.00    {}\n",
      "   9 | 14    7.99    {'horse': 2, 'ostrich': 3, 'bear': 7, 'lion': 2}\n",
      "  10 | 19    14.32   {'ostrich': 5, 'tiger': 5, 'bird': 9}\n",
      "  11 | 8     9.83    {'horse': 8}\n",
      "  12 | 10    7.63    {'ostrich': 5, 'lion': 5}\n",
      "  13 | 12    11.02   {'cat': 1, 'tiger': 6, 'bird': 5}\n",
      "  14 | 11    8.34    {'horse': 5, 'bear': 6}\n",
      "  15 | 10    8.53    {'cat': 9, 'ostrich': 1}\n",
      "  16 | 21    14.54   {'dog': 2, 'bear': 4, 'tiger': 5, 'lion': 10}\n",
      "  17 | 29    17.80   {'dog': 7, 'horse': 9, 'ostrich': 7, 'lion': 6}\n",
      "  18 | 3     2.82    {'cat': 3}\n",
      "  19 | 17    12.93   {'horse': 1, 'bear': 7, 'bird': 9}\n",
      "  20 | 25    23.14   {'horse': 10, 'rabit': 10, 'ostrich': 1, 'bear': 4}\n"
     ]
    }
   ],
   "source": [
    "# print a few documents\n",
    "for doc_id in range(1, 21):\n",
    "    print(\"{id:>4} | {len:<5d} {norm:<7.2f} {terms}\".format(id=doc_id, len=documents[doc_id]['len'], norm=documents[doc_id]['norm'], terms=str(documents[doc_id]['vector'])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bird': 2, 'horse': 1}\n",
      "\n",
      "    r  id score  document\n",
      "-------------------------------------\n",
      "    1  19 (0.81) {'id': 19, 'vector': {'horse': 1, 'bear': 7, 'bird': 9}, 'len': 17, 'norm': 12.929646825221669}\n",
      "    2  36 (0.80) {'id': 36, 'vector': {'cat': 4, 'bird': 6}, 'len': 10, 'norm': 8.277286265141177}\n",
      "    3  10 (0.69) {'id': 10, 'vector': {'ostrich': 5, 'tiger': 5, 'bird': 9}, 'len': 19, 'norm': 14.319172933962935}\n",
      "    4  40 (0.67) {'id': 40, 'vector': {'cat': 8, 'bird': 7}, 'len': 15, 'norm': 11.42978978465472}\n",
      "    5   3 (0.53) {'id': 3, 'vector': {'cat': 7, 'bird': 4}, 'len': 11, 'norm': 8.218329050423852}\n",
      "    6  13 (0.50) {'id': 13, 'vector': {'cat': 1, 'tiger': 6, 'bird': 5}, 'len': 12, 'norm': 11.018215228846154}\n",
      "    7  11 (0.45) {'id': 11, 'vector': {'horse': 8}, 'len': 8, 'norm': 9.829323335330459}\n",
      "    8  35 (0.45) {'id': 35, 'vector': {'horse': 10}, 'len': 10, 'norm': 12.286654169163073}\n",
      "    9  37 (0.45) {'id': 37, 'vector': {'horse': 6, 'ostrich': 2, 'tiger': 4, 'bird': 1}, 'len': 13, 'norm': 9.833063834527302}\n",
      "   10  27 (0.41) {'id': 27, 'vector': {'horse': 9, 'ostrich': 4, 'lion': 2}, 'len': 15, 'norm': 11.988896501047607}\n",
      "\n",
      "            bird 1.2286654169163074\n",
      "           horse 1.2286654169163074\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# initial step for \"bird(2) horse\"\n",
    "query = {'bird': 2, 'horse':1}\n",
    "k = 10\n",
    "print(query)\n",
    "\n",
    "# (optional) enable a predicate for the filtering step\n",
    "predicate = None\n",
    "# predicate = lambda doc_id: doc_id % 2 == 0\n",
    "# predicate = lambda doc_id: doc_id % 2 == 1\n",
    "selected_docs = None\n",
    "# selected_docs = list(range(10))\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = VSModel_DAAT.query(query, k, 'cosine', predicate)\n",
    "print_topk(topk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Search Example\n",
    "### Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'title': 'The Shawshank Redemption',\n",
       "  'year': '1994',\n",
       "  'runtime': 142,\n",
       "  'rating': 9.3,\n",
       "  'genre': 'Drama',\n",
       "  'actors': 'Tim Robbins Morgan Freeman Bob Gunton William Sadler',\n",
       "  'summary': 'Two imprisoned men bond over a number of years finding solace and eventual redemption through acts of common decency'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the imdb data set (1000 movies)\n",
    "from datasets import imdb\n",
    "from utils import stopwords\n",
    "\n",
    "\n",
    "data = imdb.load()\n",
    "data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "nDocs = 0\n",
    "index = {}\n",
    "documents = {}\n",
    "vocabulary = {}\n",
    "stopwords = set([\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
    "    'i', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was',\n",
    "    'were', 'will', 'with'\n",
    "])\n",
    "\n",
    "# helper function to rate if newly encountered document is relevant\n",
    "def is_relevant(doc):\n",
    "    return doc < 6\n",
    "\n",
    "def add_document(text: str):\n",
    "    global nDocs\n",
    "    nDocs += 1\n",
    "    terms = set()\n",
    "    for term in set(text.lower().split(' ')):\n",
    "        if term in stopwords:\n",
    "            continue\n",
    "        terms.add(term)\n",
    "        if term not in vocabulary:\n",
    "            index[term] = [nDocs]\n",
    "            vocabulary[term] = 1\n",
    "        else:\n",
    "            index[term].append(nDocs)\n",
    "            vocabulary[term] += 1\n",
    "    documents.append(terms)\n",
    "\n",
    "add_document(\"Human machine interface for Lab ABC computer applications\")\n",
    "add_document(\"A survey of user opinion of computer system response time\")\n",
    "add_document(\"The EPS user interface management system\")\n",
    "add_document(\"System and human system engineering testing of EPS\")\n",
    "add_document(\"Relation of user perceived response time to error measurement\")\n",
    "\n",
    "add_document(\"The generation of random binary unordered trees\")\n",
    "add_document(\"The intersection graph of paths in trees\")\n",
    "add_document(\"Graph minors IV Widths of trees and well quasi ordering\")\n",
    "add_document(\"Graph minors a survey\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
