{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space retrieval with inverted files\n",
    "\n",
    "## Helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopKList class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappop, heappush, nsmallest\n",
    "from typing import Callable\n",
    "\n",
    "class TopKList:\n",
    "    \"\"\"\n",
    "        Maintains a list of top-k documents. Initializer accepts\n",
    "        a list of tuples (term, weight) to provide information about\n",
    "        weights used by retrieval model. Implements the iter() interface.\n",
    "        Takes an optional predicate(doc: int) function to filter documents\n",
    "        before returning them. For selective predicate, you may have to\n",
    "        adjust the pruning thresholds.\n",
    "    \"\"\"\n",
    "    # set this property to define the pruning threshold, None/False to turn pruning off\n",
    "    # pruning ensures that the number of elements in the heap ranges between\n",
    "    # first and second value of the tuple\n",
    "    PRUNING_THRESHOLDS = (1000, 2000)\n",
    "\n",
    "    def __init__(self, k: int, term_weights: list[tuple[str,float]] = None, predicate: Callable[[int], bool] = None):\n",
    "        self.docs_heap = []\n",
    "        self.k = k\n",
    "        self.predicate = predicate\n",
    "        if term_weights:\n",
    "            self.term_weights = term_weights\n",
    "            self.terms = [term for term, _ in self.term_weights]\n",
    "            self.weights = dict(self.term_weights)\n",
    "    \n",
    "    def add(self, doc: int, score: float):\n",
    "        heappush(self.docs_heap, (-score, doc, {'id': doc, 'score': score}))\n",
    "        # optional pruning if heap grows too large; be careful to not trigger each time for performance\n",
    "        if len(self.docs_heap) > TopKList.PRUNING_THRESHOLDS[1]:\n",
    "            self.docs_heap = nsmallest(TopKList.PRUNING_THRESHOLDS[0], self.docs_heap)\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = 0\n",
    "        while n < self.k and len(self.docs_heap) > 0:\n",
    "            doc = heappop(self.docs_heap)[2]\n",
    "            if self.predicate == None or self.predicate(doc['id']):\n",
    "                yield doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def idf(doc_freq: int, num_docs: int) -> float:\n",
    "    return math.log((num_docs + 1) / (doc_freq + 1))\n",
    "\n",
    "def idf_bm25(doc_freq: int, num_docs: int) -> float:\n",
    "    return math.log((num_docs - doc_freq + 0.5) / (doc_freq + 0.5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this global variables o drive the examples\n",
    "DEBUG = False\n",
    "nDocs = 100\n",
    "index = {}\n",
    "documents = []\n",
    "vocabulary = {}\n",
    "\n",
    "# helper function to display result and get feedback\n",
    "def print_topk(topk):\n",
    "    n = topk.size if n < 0 else n\n",
    "    print(\"\\n  f  r  id score  document\\n-------------------------------------\")\n",
    "    for rank, doc in enumerate(topk):\n",
    "        print(\"  {rank: >3d} {id: >3d} ({score:.2f})\".format(rank = rank + 1, **doc), documents[doc['id']])\n",
    "    print()\n",
    "    for term in sorted(topk.weights.keys(), key = lambda term: -topk.weights[term]):\n",
    "        print(term.rjust(16), topk.weights[term])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Model Implementation\n",
    "\n",
    "### Base class\n",
    "\n",
    "- scoring functions\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class VSModel:\n",
    "    \"\"\"\n",
    "        Generic class for the evaluation of the Vector Space model, inherited by the document-at-a-time (DAAT) and \n",
    "        term-at-a-time (TAAT) implementation. This superclass defines the idf-weights including filtering the most\n",
    "        important terms.\n",
    "    \"\"\"\n",
    "    # set this property to True to remove terms with negative weights\n",
    "    PRUNE_NEGATIVE_WEIGHTS = False\n",
    "\n",
    "    # set this property to remove terms with absolute weights smaller than this value\n",
    "    PRUNE_WEIGHT_THRESHOLD  = False\n",
    "\n",
    "    # set this property to select top-k weights based on absolute values\n",
    "    PRUNE_TOPK = False\n",
    "\n",
    "    @staticmethod\n",
    "    def idf_weight(term: str):\n",
    "        return vocabulary[term]['idf']\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_terms(terms: list[str]) -> list[tuple[str,float]]:\n",
    "        # remove terms not in vocabulary\n",
    "        terms = list(filter(lambda t: t in vocabulary, terms))\n",
    "        # calculate weigths and produce tuples (term, weight)\n",
    "        term_weights = list(map(lambda t: (t, VSModel.idf_weight(t)), terms))\n",
    "        # filter terms with negative weights\n",
    "        if VSModel.PRUNE_NEGATIVE_WEIGHTS:\n",
    "            print('pruning negative weights')\n",
    "            term_weights = list(filter(lambda t: t[1] >= 0, term_weights))\n",
    "        # filter terms with small absolute weights\n",
    "        if VSModel.PRUNE_WEIGHT_THRESHOLD:\n",
    "            print('pruning small weights')\n",
    "            term_weights = list(filter(lambda t: abs(t[1]) > VSModel.PRUNE_WEIGHT_THRESHOLD, term_weights))\n",
    "        # select top-k terms based on absolute values\n",
    "        if VSModel.PRUNE_TOPK:\n",
    "            print('pruning top-k weights')\n",
    "            term_weights = sorted(term_weights, key = lambda t: (-abs(t[1]),len(index[t[0]]),t[0]))[:VSModel.PRUNE_TOPK]\n",
    "        return term_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-at-a-time for BIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VSModel_DAAT(VSModel):\n",
    "    \"\"\"\n",
    "        Implements the DAAT model for the Vector Space model using inverted index method.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def query(terms: list[str], k: int, predicate: Callable[[int], bool] = None):\n",
    "        # filter terms and obtain weights for terms in order of their importance \n",
    "        term_weights = VSModel.filter_terms(terms)\n",
    "        \n",
    "        # get iterators for each term and fetch first posting\n",
    "        iters = [iter(index[term]) for (term, _) in term_weights]\n",
    "        nexts = [next(iter, None) for iter in iters]\n",
    "\n",
    "        # keep track of all retrieved documents and their score; stored as tuples (doc_id, score)\n",
    "        topk = TopKList(k, term_weights, predicate)\n",
    "        while not all(e is None for e in nexts):\n",
    "            # get smallest value from nexts, ignoring None values\n",
    "            smallest = min(nexts, key = lambda x: x if x is not None else float('inf'))\n",
    "            # calculate score with the query terms present in the document\n",
    "            score = VSModel.calculate_score([(term_weights[i][1]) for i in range(len(nexts)) if nexts[i] == smallest])\n",
    "            topk.add(smallest, score)\n",
    "            # for each entry in nexts, fetch next item if entry equals smallest\n",
    "            for i, e in enumerate(nexts):\n",
    "                if e is smallest:\n",
    "                    nexts[i] = next(iters[i], None)\n",
    "        \n",
    "        # finsihed, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-at-a-time for BIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel_TAAT(BIRModel):\n",
    "    \"\"\"\n",
    "        Implements the TAAT model for the BIR model using inverted index method.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def query(terms: list[str], k: int, feedback: Feedback, predicate: Callable[[int], bool] = None):\n",
    "        # filter terms and obtain weights for terms in order of their importance \n",
    "        term_weights = BIRModel.filter_terms(terms, feedback)\n",
    "        doc_scores = {}\n",
    "\n",
    "        # iterate over terms and fetch postings\n",
    "        for (term, weight) in term_weights:\n",
    "            for posting in index[term]:\n",
    "                # if document is not already in doc_scores, add it\n",
    "                if posting not in doc_scores:\n",
    "                    doc_scores[posting] = 0\n",
    "                # add weight to document score\n",
    "                doc_scores[posting] += weight\n",
    "\n",
    "        # we do not need a full sort of doc_scores, but can use the heap in TopKList\n",
    "        topk = TopKList(k, term_weights, predicate)\n",
    "        for doc, score in doc_scores.items():\n",
    "            topk.add(doc, score)\n",
    "        \n",
    "        # finsihed, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random data example\n",
    "### Create inverted index\n",
    "The next section generates random inverted index postings for a set of terms. It simulates the indexing process for Boolean retrieval by associating random document IDs with each term. The `vocabulary` dictionary defines terms and their desired document frequencies (as a %-figure). The generated postings are stored in the `index` dictionary, with each term mapped to a set of corresponding document IDs.\n",
    "\n",
    "* `nDocs = 100`: Defines the total number of documents (document IDs) as 100.\n",
    "* `index = {}`: Initializes an empty dictionary to store the postings for each term.\n",
    "* `DEBUG = False`: A debug flag (we use it later to illustrate code execution).\n",
    "* `vocabulary`: Defines a dictionary where each term is associated with its desired document frequency (expressed as a percentage).\n",
    "* `documents`: List of all documents with each entry holding a dictionary {vector: dict, len: float, norm: float}\n",
    "  - vector holds the term freqeuncies as dictionary (key=term, value=term frequency)\n",
    "  - len is the number of terms in the document (its length)\n",
    "  - norm is the \n",
    "\n",
    "`create_postings(term: str, docFreq: int = None)` takes a term (string) and an optional document frequency (docFreq, integer) as arguments. It generates random postings for the term by creating a set of document IDs. If docFreq is not provided, it generates a random document frequency between 1 and nDocs. The for-loop iterates through each term in the vocabulary dictionary and calls the create_postings function. For each term, it fetches the desired document frequency from the vocabulary (values are percentages) and passes it to create_postings.\n",
    "\n",
    "`is_relevant(doc: int)` returns True if document is relevant ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "DEBUG = False\n",
    "nDocs = 40\n",
    "index = {}\n",
    "documents = []\n",
    "vocabulary = {}\n",
    "\n",
    "# helper function to create random postings with given document frequency\n",
    "def create_postings(term: str, docFreq: int = None):\n",
    "    # create sets with random ids\n",
    "    index[term] = []\n",
    "    vocabulary[term] = {'df': docFreq, 'idf': 0}\n",
    "    # extend feature vectors for documents with a random term frequency\n",
    "    for doc in sorted(random.sample(range(1, nDocs + 1), docFreq)):\n",
    "        # select a random term frequency for the term\n",
    "        tf = random.randint(1, 10)\n",
    "        index[term].append((doc, tf))\n",
    "        documents[doc]['terms'][term] = tf\n",
    "\n",
    "# set all feature vectors of documents to empty. We use sets since BIR uses set-of-word model\n",
    "for doc in range(nDocs + 1):\n",
    "    documents.append({'terms': {}, 'len': 0, 'norm': 0})\n",
    "\n",
    "# we use some animal terms to create random documents\n",
    "terms = ['dog', 'cat', 'horse', 'rabit', 'ostrich', 'bear', 'tiger', 'lion', 'bird']\n",
    "\n",
    "# call create_postings for each entry in vocabulary to create the inverted index\n",
    "for term in terms:\n",
    "    create_postings(term, random.randint(nDocs // 10, nDocs // 2))\n",
    "\n",
    "# now calculate the idf for each term and the norm for each document\n",
    "for item in vocabulary.values():\n",
    "    item['idf'] = idf(item['df'], nDocs)\n",
    "for doc in documents:\n",
    "    doc['len'] = sum([tf for _, tf in doc['terms'].items()])\n",
    "    doc['norm'] = sum([(tf * vocabulary[term]['idf']) ** 2 for term, tf in doc['terms'].items()]) ** 0.5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the postings for each term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog        17   0.82    [(1, 3), (4, 3), (7, 3), (10, 9), (13, 1), (14, 2), (15, 3), (17, 9), (18, 8), (19, 10), (22, 5), (24, 2), (26, 2), (28, 9), (29, 9), (35, 9), (38, 1)]\n",
      "cat        14   1.01    [(3, 5), (6, 1), (7, 4), (9, 9), (12, 9), (13, 3), (18, 1), (21, 10), (23, 10), (25, 5), (26, 5), (28, 9), (34, 2), (36, 8)]\n",
      "horse      10   1.32    [(1, 9), (5, 10), (7, 5), (10, 10), (16, 1), (17, 9), (19, 2), (25, 7), (32, 4), (37, 9)]\n",
      "rabit      20   0.67    [(1, 9), (2, 1), (3, 10), (4, 10), (5, 2), (6, 3), (10, 7), (13, 3), (14, 2), (18, 10), (20, 2), (21, 5), (22, 6), (23, 10), (27, 9), (29, 9), (30, 5), (33, 5), (35, 5), (38, 4)]\n",
      "ostrich    5    1.92    [(8, 3), (21, 3), (32, 10), (33, 5), (39, 8)]\n",
      "bear       4    2.10    [(11, 10), (16, 2), (26, 2), (36, 2)]\n",
      "tiger      12   1.15    [(6, 7), (8, 7), (10, 5), (13, 2), (22, 8), (23, 9), (26, 1), (28, 2), (31, 3), (36, 9), (37, 1), (38, 9)]\n",
      "lion       16   0.88    [(1, 3), (2, 6), (4, 7), (12, 3), (13, 1), (16, 5), (17, 9), (20, 4), (24, 3), (25, 3), (28, 10), (30, 1), (33, 6), (34, 9), (37, 2), (38, 9)]\n",
      "bird       20   0.67    [(1, 10), (2, 4), (3, 4), (4, 8), (6, 3), (8, 3), (9, 7), (10, 6), (12, 1), (16, 6), (18, 9), (19, 5), (20, 10), (21, 5), (22, 5), (26, 9), (29, 6), (30, 6), (35, 3), (40, 9)]\n"
     ]
    }
   ],
   "source": [
    "# print vocabulary with df and idf\n",
    "for term, item in vocabulary.items():\n",
    "    print(\"{term:10} {df:<4d} {idf:<7.2f} {postings}\".format(term=term.ljust(10), df=item['df'], idf=item['idf'], postings=index[term]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 | 34    15.31   {'dog': 3, 'horse': 9, 'rabit': 9, 'lion': 3, 'bird': 10}\n",
      "   2 | 11    5.96    {'rabit': 1, 'lion': 6, 'bird': 4}\n",
      "   3 | 19    8.79    {'cat': 5, 'rabit': 10, 'bird': 4}\n",
      "   4 | 28    10.84   {'dog': 3, 'rabit': 10, 'lion': 7, 'bird': 8}\n",
      "   5 | 12    13.22   {'horse': 10, 'rabit': 2}\n",
      "   6 | 14    8.59    {'cat': 1, 'rabit': 3, 'tiger': 7, 'bird': 3}\n",
      "   7 | 12    8.10    {'dog': 3, 'cat': 4, 'horse': 5}\n",
      "   8 | 13    10.10   {'ostrich': 3, 'tiger': 7, 'bird': 3}\n",
      "   9 | 16    10.19   {'cat': 9, 'bird': 7}\n",
      "  10 | 37    17.29   {'dog': 9, 'horse': 10, 'rabit': 7, 'tiger': 5, 'bird': 6}\n",
      "  11 | 10    21.04   {'bear': 10}\n",
      "  12 | 13    9.45    {'cat': 9, 'lion': 3, 'bird': 1}\n",
      "  13 | 10    4.46    {'dog': 1, 'cat': 3, 'rabit': 3, 'tiger': 2, 'lion': 1}\n",
      "  14 | 4     2.12    {'dog': 2, 'rabit': 2}\n",
      "  15 | 3     2.47    {'dog': 3}\n",
      "  16 | 14    7.41    {'horse': 1, 'bear': 2, 'lion': 5, 'bird': 6}\n",
      "  17 | 27    16.06   {'dog': 9, 'horse': 9, 'lion': 9}\n",
      "  18 | 28    11.20   {'dog': 8, 'cat': 1, 'rabit': 10, 'bird': 9}\n",
      "  19 | 17    9.27    {'dog': 10, 'horse': 2, 'bird': 5}\n",
      "  20 | 16    7.68    {'rabit': 2, 'lion': 4, 'bird': 10}\n"
     ]
    }
   ],
   "source": [
    "# print a few documents\n",
    "for doc in range(20):\n",
    "    print(\"{id:>4} | {len:<5d} {norm:<7.2f} {terms}\".format(id=doc + 1, len=documents[doc + 1]['len'], norm=documents[doc + 1]['norm'], terms=str(documents[doc + 1]['terms'])))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird horse\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'VSModel_DAAT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\home\\work\\mmir\\mmir-unibasel\\chapter03\\VectorSpace_InvertedIndex.ipynb Cell 20\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/home/work/mmir/mmir-unibasel/chapter03/VectorSpace_InvertedIndex.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m predicate \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/home/work/mmir/mmir-unibasel/chapter03/VectorSpace_InvertedIndex.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# predicate = lambda doc: doc % 2 == 0\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/home/work/mmir/mmir-unibasel/chapter03/VectorSpace_InvertedIndex.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# predicate = lambda doc: doc % 2 == 1\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/home/work/mmir/mmir-unibasel/chapter03/VectorSpace_InvertedIndex.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/home/work/mmir/mmir-unibasel/chapter03/VectorSpace_InvertedIndex.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# run query, display result, and get feedback\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/home/work/mmir/mmir-unibasel/chapter03/VectorSpace_InvertedIndex.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m topk \u001b[39m=\u001b[39m VSModel_DAAT\u001b[39m.\u001b[39mquery(query, k, predicate)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/home/work/mmir/mmir-unibasel/chapter03/VectorSpace_InvertedIndex.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m print_topk()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VSModel_DAAT' is not defined"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# initial step for \"bird horse\"\n",
    "query = ['bird', 'horse']\n",
    "k = 20\n",
    "print(' '.join(query))\n",
    "\n",
    "# (optional) enable a predicate for the filtering step\n",
    "predicate = None\n",
    "# predicate = lambda doc: doc % 2 == 0\n",
    "# predicate = lambda doc: doc % 2 == 1\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = VSModel_DAAT.query(query, k, predicate)\n",
    "print_topk()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small document example for DAAT and TAAT\n",
    "### Create inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from the exercise\n",
    "nDocs = 0\n",
    "index = {}\n",
    "documents = [set()]\n",
    "vocabulary = {}\n",
    "stopwords = set([\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
    "    'i', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was',\n",
    "    'were', 'will', 'with'\n",
    "])\n",
    "\n",
    "# helper function to rate if newly encountered document is relevant\n",
    "def is_relevant(doc):\n",
    "    return doc < 6\n",
    "\n",
    "def add_document(text: str):\n",
    "    global nDocs\n",
    "    nDocs += 1\n",
    "    terms = set()\n",
    "    for term in set(text.lower().split(' ')):\n",
    "        if term in stopwords:\n",
    "            continue\n",
    "        terms.add(term)\n",
    "        if term not in vocabulary:\n",
    "            index[term] = [nDocs]\n",
    "            vocabulary[term] = 1\n",
    "        else:\n",
    "            index[term].append(nDocs)\n",
    "            vocabulary[term] += 1\n",
    "    documents.append(terms)\n",
    "\n",
    "add_document(\"Human machine interface for Lab ABC computer applications\")\n",
    "add_document(\"A survey of user opinion of computer system response time\")\n",
    "add_document(\"The EPS user interface management system\")\n",
    "add_document(\"System and human system engineering testing of EPS\")\n",
    "add_document(\"Relation of user perceived response time to error measurement\")\n",
    "\n",
    "add_document(\"The generation of random binary unordered trees\")\n",
    "add_document(\"The intersection graph of paths in trees\")\n",
    "add_document(\"Graph minors IV Widths of trees and well quasi ordering\")\n",
    "add_document(\"Graph minors a survey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interface      2    [1, 3]\n",
      "human          2    [1, 4]\n",
      "lab            1    [1]\n",
      "machine        1    [1]\n",
      "computer       2    [1, 2]\n",
      "applications   1    [1]\n",
      "abc            1    [1]\n",
      "opinion        1    [2]\n",
      "user           3    [2, 3, 5]\n",
      "system         3    [2, 3, 4]\n",
      "time           2    [2, 5]\n",
      "response       2    [2, 5]\n",
      "survey         2    [2, 9]\n",
      "management     1    [3]\n",
      "eps            2    [3, 4]\n",
      "engineering    1    [4]\n",
      "testing        1    [4]\n",
      "measurement    1    [5]\n",
      "relation       1    [5]\n",
      "error          1    [5]\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice \n",
    "\n",
    "# print postings with term and list of documents\n",
    "for term, posting in islice(index.items(), 20):\n",
    "    print(term.ljust(14), str(len(posting)).ljust(4), sorted(posting[:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 {'interface', 'human', 'machine', 'lab', 'computer', 'applications', 'abc'}\n",
      "2 {'opinion', 'computer', 'user', 'system', 'time', 'response', 'survey'}\n",
      "3 {'interface', 'management', 'user', 'system', 'eps'}\n",
      "4 {'human', 'engineering', 'testing', 'system', 'eps'}\n",
      "5 {'measurement', 'relation', 'user', 'time', 'error', 'response', 'perceived'}\n",
      "6 {'trees', 'unordered', 'random', 'binary', 'generation'}\n",
      "7 {'intersection', 'graph', 'paths', 'trees'}\n",
      "8 {'widths', 'well', 'trees', 'graph', 'minors', 'quasi', 'iv', 'ordering'}\n",
      "9 {'graph', 'minors', 'survey'}\n"
     ]
    }
   ],
   "source": [
    "# print all documents\n",
    "print()\n",
    "for doc in range(len(documents) - 1):\n",
    "    print(doc + 1, documents[doc + 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human computer interaction\n",
      "pruning negative weights\n",
      "pruning top-k weights\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1   1 (2.45) {'interface', 'human', 'machine', 'lab', 'computer', 'applications', 'abc'}\n",
      "  +  2   2 (1.22) {'opinion', 'computer', 'user', 'system', 'time', 'response', 'survey'}\n",
      "  +  3   4 (1.22) {'human', 'engineering', 'testing', 'system', 'eps'}\n",
      "\n",
      "        computer 1.2237754316221157\n",
      "           human 1.2237754316221157\n",
      "\n",
      "next feedback: +1, +2, +3, +4, +5, -6, -7, -8, -9\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# initial step for \"cat dog\"\n",
    "query = ['human', 'computer', 'interaction']\n",
    "k = 9\n",
    "feedback = Feedback()\n",
    "print(' '.join(query))\n",
    "\n",
    "# set behavior\n",
    "BIRModel.PRUNE_NEGATIVE_WEIGHTS     = True\n",
    "BIRModel.PRUNE_WEIGHT_THRESHOLD     = False\n",
    "BIRModel.PRUNE_TOPK                 = 5\n",
    "BIRModel.PRUNE_NON_RELEVANT         = False\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback)\n",
    "display_and_get_feedback(k, all_docs = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback step\n",
    "\n",
    "Adjust weights with feedback. Repeat runs (`Ctrl+Enter` to stay on cell) + query expansion with terms from relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m reduce\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(query))\n\u001b[1;32m      3\u001b[0m print_feedback()\n\u001b[1;32m      5\u001b[0m \u001b[39m# run query, display result, and get feedback\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "print(' '.join(query))\n",
    "print_feedback()\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback)\n",
    "display_and_get_feedback(k)\n",
    "\n",
    "query = sorted(set(query) | reduce(lambda terms, doc: terms | documents[doc], feedback.relevant, set()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(10, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
