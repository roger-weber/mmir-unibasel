{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space retrieval with inverted files\n",
    "\n",
    "## Helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopKList class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappop, heappush, nsmallest\n",
    "from typing import Callable\n",
    "\n",
    "class TopKList:\n",
    "    \"\"\"\n",
    "        Maintains a list of top-k documents. Initializer accepts\n",
    "        a list of tuples (term, weight) to provide information about\n",
    "        weights used by retrieval model. Implements the iter() interface.\n",
    "        Takes an optional predicate(doc: int) function to filter documents\n",
    "        before returning them. For selective predicate, you may have to\n",
    "        adjust the pruning thresholds.\n",
    "    \"\"\"\n",
    "    # set this property to define the pruning threshold, None/False to turn pruning off\n",
    "    # pruning ensures that the number of elements in the heap ranges between\n",
    "    # first and second value of the tuple\n",
    "    PRUNING_THRESHOLDS = (1000, 2000)\n",
    "\n",
    "    def __init__(self, k: int, term_weights: list[tuple[str,float]] = None, predicate: Callable[[int], bool] = None):\n",
    "        self.docs_heap = []\n",
    "        self.k = k\n",
    "        self.predicate = predicate\n",
    "        if term_weights:\n",
    "            self.term_weights = term_weights\n",
    "            self.terms = [term for term, _ in self.term_weights]\n",
    "            self.weights = dict(self.term_weights)\n",
    "    \n",
    "    def add(self, doc: int, score: float):\n",
    "        heappush(self.docs_heap, (-score, doc, {'id': doc, 'score': score}))\n",
    "        # optional pruning if heap grows too large; be careful to not trigger each time for performance\n",
    "        if len(self.docs_heap) > TopKList.PRUNING_THRESHOLDS[1]:\n",
    "            self.docs_heap = nsmallest(TopKList.PRUNING_THRESHOLDS[0], self.docs_heap)\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = 0\n",
    "        while n < self.k and len(self.docs_heap) > 0:\n",
    "            doc = heappop(self.docs_heap)[2]\n",
    "            if self.predicate == None or self.predicate(doc['id']):\n",
    "                yield doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def idf(doc_freq: int, num_docs: int) -> float:\n",
    "    return math.log((num_docs + 1) / (doc_freq + 1))\n",
    "\n",
    "def idf_bm25(doc_freq: int, num_docs: int) -> float:\n",
    "    return math.log((num_docs - doc_freq + 0.5) / (doc_freq + 0.5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this global variables o drive the examples\n",
    "DEBUG = False\n",
    "nDocs = 100\n",
    "index = {}\n",
    "documents = []\n",
    "vocabulary = {}\n",
    "\n",
    "# helper function to display result and get feedback\n",
    "def print_topk(topk):\n",
    "    print(\"\\n    r  id score  document\\n-------------------------------------\")\n",
    "    for rank, doc in enumerate(topk):\n",
    "        print(\"  {rank: >3d} {id: >3d} ({score:.2f})\".format(rank = rank + 1, **doc), documents[doc['id']])\n",
    "    print()\n",
    "    for term in sorted(topk.weights.keys(), key = lambda term: -topk.weights[term]):\n",
    "        print(term.rjust(16), topk.weights[term])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Model Implementation\n",
    "\n",
    "### Scoring functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VSMeasure: pass\n",
    "\n",
    "# implements the cosine measure\n",
    "class CosineMeasure(VSMeasure):\n",
    "    def __init__(self, query_vector: dict[str, int]):\n",
    "        self.query_vector = {}\n",
    "        self.term_weights = []\n",
    "        self.query_norm = 0\n",
    "        for term in query_vector.keys():\n",
    "            if term in vocabulary:\n",
    "                idf_2 = vocabulary[term]['idf'] ** 2\n",
    "                self.query_vector[term] = query_vector[term] * idf_2\n",
    "                self.term_weights.append((term, vocabulary[term]['idf']))\n",
    "                self.query_norm += idf_2 * query_vector[term] ** 2\n",
    "            else:\n",
    "                del self.query_vector[term]\n",
    "        self.query_norm = self.query_norm ** 0.5\n",
    "    \n",
    "    def similarity(self, doc: int):\n",
    "        doc_vector = documents[doc]['vector']\n",
    "        dot_product = sum([doc_vector.get(term, 0) * q for (term, q) in self.query_vector.items()])\n",
    "        return dot_product / self.query_norm / documents[doc]['norm']\n",
    "\n",
    "    def update_sum(self, term: str, doc_tf: int, doc: int):\n",
    "        return doc_tf * self.query_vector.get(term, 0) / self.query_norm / documents[doc]['norm']\n",
    "\n",
    "# implements the dot product\n",
    "class DotProduct(VSMeasure):\n",
    "    def __init__(self, query_vector: dict[str, int]):\n",
    "        self.query_vector = {}\n",
    "        self.term_weights = []\n",
    "        for term in query_vector.keys():\n",
    "            if term in vocabulary:\n",
    "                idf_2 = vocabulary[term]['idf'] ** 2\n",
    "                self.query_vector[term] = query_vector[term] * idf_2\n",
    "                self.term_weights.append((term, vocabulary[term]['idf']))\n",
    "            else:\n",
    "                del self.query_vector[term]\n",
    "    \n",
    "    def similarity(self, doc: int):\n",
    "        doc_vector = documents[doc]['vector']\n",
    "        dot_product = sum([doc_vector.get(term, 0) * q for (term, q) in self.query_vector.items()])\n",
    "        return dot_product\n",
    "\n",
    "    def update_sum(self, term: str, doc_tf: int, doc: int):\n",
    "        return doc_tf * self.query_vector.get(term, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class VSModel:\n",
    "    \"\"\"\n",
    "        Generic class for the evaluation of the Vector Space model, inherited by the document-at-a-time (DAAT) and \n",
    "        term-at-a-time (TAAT) implementation. This superclass defines the idf-weights including filtering the most\n",
    "        important terms.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def get_similarity_measure(measure: str, query_vector: dict[str, int]) -> VSMeasure:\n",
    "        return {\n",
    "            'cosine': CosineMeasure(query_vector)\n",
    "        }[measure]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-at-a-time for Vector Space Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VSModel_DAAT(VSModel):\n",
    "    \"\"\"\n",
    "        Implements the DAAT model for the Vector Space model using inverted index method.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def query(query_vector: dict[str, int], k: int, measure: str = 'dot', predicate: Callable[[int], bool] = None, selected_docs: set[int] = None):\n",
    "        # determine simialrity measure for this query\n",
    "        sim = VSModel.get_similarity_measure(measure, query_vector)\n",
    "        \n",
    "        # get iterators for each term and fetch first posting\n",
    "        iters = [iter(index[term]) for (term, _) in sim.term_weights]\n",
    "        nexts = [next(iter, None) for iter in iters]\n",
    "\n",
    "        # keep track of all retrieved documents and their score; stored as tuples (doc_id, score)\n",
    "        topk = TopKList(k, sim.term_weights, predicate)\n",
    "        while not all(e is None for e in nexts):\n",
    "            # get smallest value from nexts, ignoring None values\n",
    "            smallest = min(nexts, key = lambda x: x[0] if x is not None else float('inf'))[0]\n",
    "            # calculate score with the query terms present in the document\n",
    "            score = sum([sim.update_sum(sim.term_weights[i][0], nexts[i][1], smallest) for i in range(len(nexts)) if nexts[i] and nexts[i][0] == smallest])\n",
    "            # assert score == sim.similarity(smallest)\n",
    "            topk.add(smallest, score)\n",
    "            # for each entry in nexts, fetch next item if entry equals smallest\n",
    "            for i, e in enumerate(nexts):\n",
    "                if e and e[0] is smallest:\n",
    "                    nexts[i] = next(iters[i], None)\n",
    "        \n",
    "        # finsihed, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-at-a-time for BIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel_TAAT(BIRModel):\n",
    "    \"\"\"\n",
    "        Implements the TAAT model for the BIR model using inverted index method.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def query(terms: list[str], k: int, feedback: Feedback, predicate: Callable[[int], bool] = None):\n",
    "        # filter terms and obtain weights for terms in order of their importance \n",
    "        term_weights = BIRModel.filter_terms(terms, feedback)\n",
    "        doc_scores = {}\n",
    "\n",
    "        # iterate over terms and fetch postings\n",
    "        for (term, weight) in term_weights:\n",
    "            for posting in index[term]:\n",
    "                # if document is not already in doc_scores, add it\n",
    "                if posting not in doc_scores:\n",
    "                    doc_scores[posting] = 0\n",
    "                # add weight to document score\n",
    "                doc_scores[posting] += weight\n",
    "\n",
    "        # we do not need a full sort of doc_scores, but can use the heap in TopKList\n",
    "        topk = TopKList(k, term_weights, predicate)\n",
    "        for doc, score in doc_scores.items():\n",
    "            topk.add(doc, score)\n",
    "        \n",
    "        # finsihed, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random data example\n",
    "### Create inverted index\n",
    "The next section generates random inverted index postings for a set of terms. It simulates the indexing process for Boolean retrieval by associating random document IDs with each term. The `vocabulary` dictionary defines terms and their desired document frequencies (as a %-figure). The generated postings are stored in the `index` dictionary, with each term mapped to a set of corresponding document IDs.\n",
    "\n",
    "* `nDocs = 100`: Defines the total number of documents (document IDs) as 100.\n",
    "* `index = {}`: Initializes an empty dictionary to store the postings for each term.\n",
    "* `DEBUG = False`: A debug flag (we use it later to illustrate code execution).\n",
    "* `vocabulary`: Defines a dictionary where each term is associated with its desired document frequency (expressed as a percentage).\n",
    "* `documents`: List of all documents with each entry holding a dictionary {vector: dict, len: float, norm: float}\n",
    "  - vector holds the term freqeuncies as dictionary (key=term, value=term frequency)\n",
    "  - len is the number of terms in the document (its length)\n",
    "  - norm is the \n",
    "\n",
    "`create_postings(term: str, docFreq: int = None)` takes a term (string) and an optional document frequency (docFreq, integer) as arguments. It generates random postings for the term by creating a set of document IDs. If docFreq is not provided, it generates a random document frequency between 1 and nDocs. The for-loop iterates through each term in the vocabulary dictionary and calls the create_postings function. For each term, it fetches the desired document frequency from the vocabulary (values are percentages) and passes it to create_postings.\n",
    "\n",
    "`is_relevant(doc: int)` returns True if document is relevant ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "DEBUG = False\n",
    "nDocs = 40\n",
    "index = {}\n",
    "documents = []\n",
    "vocabulary = {}\n",
    "\n",
    "# helper function to create random postings with given document frequency\n",
    "def create_postings(term: str, docFreq: int = None):\n",
    "    # create sets with random ids\n",
    "    index[term] = []\n",
    "    vocabulary[term] = {'df': docFreq, 'idf': 0}\n",
    "    # extend feature vectors for documents with a random term frequency\n",
    "    for doc in sorted(random.sample(range(1, nDocs + 1), docFreq)):\n",
    "        # select a random term frequency for the term\n",
    "        tf = random.randint(1, 10)\n",
    "        index[term].append((doc, tf))\n",
    "        documents[doc]['vector'][term] = tf\n",
    "\n",
    "# set all feature vectors of documents to empty. We use sets since BIR uses set-of-word model\n",
    "for doc in range(nDocs + 1):\n",
    "    documents.append({'vector': {}, 'len': 0, 'norm': 0})\n",
    "\n",
    "# we use some animal terms to create random documents\n",
    "terms = ['dog', 'cat', 'horse', 'rabit', 'ostrich', 'bear', 'tiger', 'lion', 'bird']\n",
    "\n",
    "# call create_postings for each entry in vocabulary to create the inverted index\n",
    "for term in terms:\n",
    "    create_postings(term, random.randint(nDocs // 10, nDocs // 2))\n",
    "\n",
    "# now calculate the idf for each term and the norm for each document\n",
    "for item in vocabulary.values():\n",
    "    item['idf'] = idf(item['df'], nDocs)\n",
    "    item['idf_bm25'] = idf_bm25(item['df'], nDocs)\n",
    "for doc in documents:\n",
    "    doc['len'] = sum([tf for _, tf in doc['vector'].items()])\n",
    "    doc['norm'] = sum([(tf * vocabulary[term]['idf']) ** 2 for term, tf in doc['vector'].items()]) ** 0.5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the postings for each term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog        20   0.67    0.00    [(3, 4), (4, 9), (7, 9), (11, 1), (13, 6), (14, 5), (15, 9), (18, 3), (19, 3), (20, 10), (21, 10), (25, 4), (26, 8), (27, 10), (29, 5), (30, 1), (33, 2), (34, 7), (37, 10), (38, 1)]\n",
      "cat        10   1.32    1.07    [(1, 3), (5, 9), (6, 3), (7, 8), (20, 7), (21, 2), (28, 3), (30, 3), (34, 7), (37, 8)]\n",
      "horse      7    1.63    1.50    [(5, 6), (10, 7), (16, 3), (22, 10), (31, 7), (32, 10), (38, 4)]\n",
      "rabit      8    1.52    1.34    [(14, 1), (21, 6), (22, 10), (28, 5), (30, 6), (32, 5), (33, 4), (38, 2)]\n",
      "ostrich    11   1.23    0.94    [(5, 10), (7, 4), (9, 9), (11, 7), (12, 5), (13, 6), (17, 1), (18, 5), (24, 2), (26, 5), (34, 6)]\n",
      "bear       9    1.41    1.20    [(9, 6), (10, 9), (13, 1), (15, 8), (18, 4), (28, 9), (30, 3), (31, 9), (37, 9)]\n",
      "tiger      14   1.01    0.60    [(5, 3), (6, 2), (7, 1), (9, 8), (12, 9), (14, 3), (21, 5), (22, 10), (23, 9), (25, 4), (26, 5), (34, 9), (37, 4), (38, 4)]\n",
      "lion       19   0.72    0.10    [(2, 4), (3, 8), (7, 5), (9, 7), (10, 1), (11, 3), (14, 1), (17, 8), (19, 9), (23, 9), (24, 9), (25, 2), (27, 9), (30, 9), (32, 8), (35, 9), (36, 6), (38, 10), (40, 6)]\n",
      "bird       12   1.15    0.82    [(1, 7), (5, 2), (10, 5), (11, 9), (14, 6), (17, 9), (18, 7), (24, 8), (25, 6), (26, 2), (36, 3), (39, 1)]\n"
     ]
    }
   ],
   "source": [
    "# print vocabulary with df and idf\n",
    "for term, item in vocabulary.items():\n",
    "    print(\"{term:10} {df:<4d} {idf:<7.2f} {idf_bm25:<7.2f} {postings}\".format(term=term.ljust(10), df=item['df'], idf=item['idf'], idf_bm25=item['idf_bm25'], postings=index[term]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 | 10    8.96    {'cat': 3, 'bird': 7}\n",
      "   2 | 4     2.87    {'lion': 4}\n",
      "   3 | 12    6.34    {'dog': 4, 'lion': 8}\n",
      "   4 | 9     6.02    {'dog': 9}\n",
      "   5 | 30    20.04   {'cat': 9, 'horse': 6, 'ostrich': 10, 'tiger': 3, 'bird': 2}\n",
      "   6 | 5     4.43    {'cat': 3, 'tiger': 2}\n",
      "   7 | 27    13.60   {'dog': 9, 'cat': 8, 'ostrich': 4, 'tiger': 1, 'lion': 5}\n",
      "   8 | 0     0.00    {}\n",
      "   9 | 30    16.85   {'ostrich': 9, 'bear': 6, 'tiger': 8, 'lion': 7}\n",
      "  10 | 22    18.04   {'horse': 7, 'bear': 9, 'lion': 1, 'bird': 5}\n",
      "  11 | 20    13.64   {'dog': 1, 'ostrich': 7, 'lion': 3, 'bird': 9}\n",
      "  12 | 14    10.94   {'ostrich': 5, 'tiger': 9}\n",
      "  13 | 13    8.51    {'dog': 6, 'ostrich': 6, 'bear': 1}\n",
      "  14 | 16    8.40    {'dog': 5, 'rabit': 1, 'tiger': 3, 'lion': 1, 'bird': 6}\n",
      "  15 | 17    12.79   {'dog': 9, 'bear': 8}\n",
      "  16 | 3     4.90    {'horse': 3}\n",
      "  17 | 18    11.89   {'ostrich': 1, 'lion': 8, 'bird': 9}\n",
      "  18 | 19    11.76   {'dog': 3, 'ostrich': 5, 'bear': 4, 'bird': 7}\n",
      "  19 | 12    6.77    {'dog': 3, 'lion': 9}\n",
      "  20 | 17    11.38   {'dog': 10, 'cat': 7}\n"
     ]
    }
   ],
   "source": [
    "# print a few documents\n",
    "for doc in range(20):\n",
    "    print(\"{id:>4} | {len:<5d} {norm:<7.2f} {terms}\".format(id=doc + 1, len=documents[doc + 1]['len'], norm=documents[doc + 1]['norm'], terms=str(documents[doc + 1]['vector'])))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bird': 2, 'horse': 1}\n",
      "\n",
      "    r  id score  document\n",
      "-------------------------------------\n",
      "    1  23 (0.87) {'vector': {'dog': 1, 'cat': 7, 'lion': 1, 'bird': 10}, 'len': 19, 'norm': 14.025126073837832}\n",
      "    2   4 (0.86) {'vector': {'rabit': 2, 'bird': 3}, 'len': 5, 'norm': 4.236190701513045}\n",
      "    3   6 (0.77) {'vector': {'dog': 2, 'cat': 8, 'horse': 5, 'rabit': 3, 'lion': 7, 'bird': 8}, 'len': 33, 'norm': 15.239974756540674}\n",
      "    4  14 (0.74) {'vector': {'dog': 5, 'horse': 6, 'rabit': 4, 'lion': 9, 'bird': 8}, 'len': 32, 'norm': 16.377357309671446}\n",
      "    5  22 (0.73) {'vector': {'rabit': 6, 'bear': 2, 'lion': 7, 'bird': 9}, 'len': 24, 'norm': 15.075793772175903}\n",
      "    6  36 (0.71) {'vector': {'tiger': 5, 'bird': 7}, 'len': 12, 'norm': 11.929040556051872}\n",
      "    7  19 (0.55) {'vector': {'cat': 2, 'tiger': 7, 'bird': 6}, 'len': 15, 'norm': 13.295596288926044}\n",
      "    8  24 (0.54) {'vector': {'dog': 1, 'rabit': 2, 'lion': 10, 'bird': 6}, 'len': 19, 'norm': 13.440831229986518}\n",
      "    9   2 (0.45) {'vector': {'horse': 1, 'rabit': 5, 'ostrich': 4, 'tiger': 9, 'bird': 6}, 'len': 25, 'norm': 16.989935421090834}\n",
      "   10  13 (0.42) {'vector': {'cat': 6, 'lion': 9, 'bird': 4}, 'len': 19, 'norm': 11.71886551555771}\n",
      "   11  16 (0.35) {'vector': {'cat': 3, 'horse': 5, 'rabit': 1}, 'len': 9, 'norm': 5.786601797794573}\n",
      "   12  40 (0.33) {'vector': {'horse': 8, 'tiger': 1, 'lion': 4}, 'len': 13, 'norm': 9.729638396478913}\n",
      "   13  26 (0.33) {'vector': {'cat': 8, 'horse': 10, 'ostrich': 2}, 'len': 20, 'norm': 12.253359573422497}\n",
      "   14  37 (0.29) {'vector': {'dog': 8, 'cat': 6, 'horse': 8}, 'len': 22, 'norm': 11.089934332714751}\n",
      "   15  30 (0.25) {'vector': {'horse': 5, 'rabit': 8}, 'len': 13, 'norm': 8.168522094703105}\n",
      "   16   1 (0.18) {'vector': {'cat': 5, 'horse': 5, 'rabit': 1, 'tiger': 6}, 'len': 17, 'norm': 11.109543121539035}\n",
      "   17  33 (0.17) {'vector': {'cat': 6, 'horse': 3, 'rabit': 6, 'ostrich': 1}, 'len': 16, 'norm': 7.022236683178643}\n",
      "   18  12 (0.12) {'vector': {'dog': 3, 'horse': 2, 'rabit': 2, 'ostrich': 5}, 'len': 12, 'norm': 7.025859655023876}\n",
      "   19   3 (0.05) {'vector': {'dog': 9, 'cat': 8, 'horse': 2, 'rabit': 2, 'bear': 8}, 'len': 29, 'norm': 14.984237628143012}\n",
      "   20  10 (0.04) {'vector': {'dog': 3, 'cat': 7, 'horse': 1, 'ostrich': 2, 'lion': 7}, 'len': 20, 'norm': 9.504680923186546}\n",
      "\n",
      "            bird 1.7310054260226062\n",
      "           horse 1.1545819202215484\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# initial step for \"bird(2) horse\"\n",
    "query = {'bird': 2, 'horse':1}\n",
    "k = 20\n",
    "print(query)\n",
    "\n",
    "# (optional) enable a predicate for the filtering step\n",
    "predicate = None\n",
    "# predicate = lambda doc: doc % 2 == 0\n",
    "# predicate = lambda doc: doc % 2 == 1\n",
    "selected_docs = None\n",
    "# selected_docs = list(range(10))\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = VSModel_DAAT.query(query, k, 'cosine', predicate)\n",
    "print_topk(topk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small document example for DAAT and TAAT\n",
    "### Create inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from the exercise\n",
    "nDocs = 0\n",
    "index = {}\n",
    "documents = [set()]\n",
    "vocabulary = {}\n",
    "stopwords = set([\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
    "    'i', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was',\n",
    "    'were', 'will', 'with'\n",
    "])\n",
    "\n",
    "# helper function to rate if newly encountered document is relevant\n",
    "def is_relevant(doc):\n",
    "    return doc < 6\n",
    "\n",
    "def add_document(text: str):\n",
    "    global nDocs\n",
    "    nDocs += 1\n",
    "    terms = set()\n",
    "    for term in set(text.lower().split(' ')):\n",
    "        if term in stopwords:\n",
    "            continue\n",
    "        terms.add(term)\n",
    "        if term not in vocabulary:\n",
    "            index[term] = [nDocs]\n",
    "            vocabulary[term] = 1\n",
    "        else:\n",
    "            index[term].append(nDocs)\n",
    "            vocabulary[term] += 1\n",
    "    documents.append(terms)\n",
    "\n",
    "add_document(\"Human machine interface for Lab ABC computer applications\")\n",
    "add_document(\"A survey of user opinion of computer system response time\")\n",
    "add_document(\"The EPS user interface management system\")\n",
    "add_document(\"System and human system engineering testing of EPS\")\n",
    "add_document(\"Relation of user perceived response time to error measurement\")\n",
    "\n",
    "add_document(\"The generation of random binary unordered trees\")\n",
    "add_document(\"The intersection graph of paths in trees\")\n",
    "add_document(\"Graph minors IV Widths of trees and well quasi ordering\")\n",
    "add_document(\"Graph minors a survey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interface      2    [1, 3]\n",
      "human          2    [1, 4]\n",
      "lab            1    [1]\n",
      "machine        1    [1]\n",
      "computer       2    [1, 2]\n",
      "applications   1    [1]\n",
      "abc            1    [1]\n",
      "opinion        1    [2]\n",
      "user           3    [2, 3, 5]\n",
      "system         3    [2, 3, 4]\n",
      "time           2    [2, 5]\n",
      "response       2    [2, 5]\n",
      "survey         2    [2, 9]\n",
      "management     1    [3]\n",
      "eps            2    [3, 4]\n",
      "engineering    1    [4]\n",
      "testing        1    [4]\n",
      "measurement    1    [5]\n",
      "relation       1    [5]\n",
      "error          1    [5]\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice \n",
    "\n",
    "# print postings with term and list of documents\n",
    "for term, posting in islice(index.items(), 20):\n",
    "    print(term.ljust(14), str(len(posting)).ljust(4), sorted(posting[:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 {'interface', 'human', 'machine', 'lab', 'computer', 'applications', 'abc'}\n",
      "2 {'opinion', 'computer', 'user', 'system', 'time', 'response', 'survey'}\n",
      "3 {'interface', 'management', 'user', 'system', 'eps'}\n",
      "4 {'human', 'engineering', 'testing', 'system', 'eps'}\n",
      "5 {'measurement', 'relation', 'user', 'time', 'error', 'response', 'perceived'}\n",
      "6 {'trees', 'unordered', 'random', 'binary', 'generation'}\n",
      "7 {'intersection', 'graph', 'paths', 'trees'}\n",
      "8 {'widths', 'well', 'trees', 'graph', 'minors', 'quasi', 'iv', 'ordering'}\n",
      "9 {'graph', 'minors', 'survey'}\n"
     ]
    }
   ],
   "source": [
    "# print all documents\n",
    "print()\n",
    "for doc in range(len(documents) - 1):\n",
    "    print(doc + 1, documents[doc + 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human computer interaction\n",
      "pruning negative weights\n",
      "pruning top-k weights\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1   1 (2.45) {'interface', 'human', 'machine', 'lab', 'computer', 'applications', 'abc'}\n",
      "  +  2   2 (1.22) {'opinion', 'computer', 'user', 'system', 'time', 'response', 'survey'}\n",
      "  +  3   4 (1.22) {'human', 'engineering', 'testing', 'system', 'eps'}\n",
      "\n",
      "        computer 1.2237754316221157\n",
      "           human 1.2237754316221157\n",
      "\n",
      "next feedback: +1, +2, +3, +4, +5, -6, -7, -8, -9\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# initial step for \"cat dog\"\n",
    "query = ['human', 'computer', 'interaction']\n",
    "k = 9\n",
    "feedback = Feedback()\n",
    "print(' '.join(query))\n",
    "\n",
    "# set behavior\n",
    "BIRModel.PRUNE_NEGATIVE_WEIGHTS     = True\n",
    "BIRModel.PRUNE_WEIGHT_THRESHOLD     = False\n",
    "BIRModel.PRUNE_TOPK                 = 5\n",
    "BIRModel.PRUNE_NON_RELEVANT         = False\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback)\n",
    "display_and_get_feedback(k, all_docs = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback step\n",
    "\n",
    "Adjust weights with feedback. Repeat runs (`Ctrl+Enter` to stay on cell) + query expansion with terms from relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m reduce\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(query))\n\u001b[1;32m      3\u001b[0m print_feedback()\n\u001b[1;32m      5\u001b[0m \u001b[39m# run query, display result, and get feedback\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "print(' '.join(query))\n",
    "print_feedback()\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback)\n",
    "display_and_get_feedback(k)\n",
    "\n",
    "query = sorted(set(query) | reduce(lambda terms, doc: terms | documents[doc], feedback.relevant, set()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(10, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bird': 2, 'horse': 1}\n",
      "{'vector': {'dog': 1, 'cat': 7, 'lion': 1, 'bird': 10}, 'len': 19, 'norm': 14.025126073837832}\n",
      "{'bird': 3.4620108520452124, 'horse': 1.1545819202215484}\n",
      "[('bird', 1.7310054260226062), ('horse', 1.1545819202215484)]\n",
      "2.842288448471051\n",
      "0.8684673603499498\n",
      "0.8684673603499498\n"
     ]
    }
   ],
   "source": [
    "sim = CosineMeasure(query)\n",
    "doc = 23\n",
    "\n",
    "print(query)\n",
    "print(documents[doc])\n",
    "print(sim.query_vector)\n",
    "print(sim.term_weights)\n",
    "print(sim.query_norm)\n",
    "print(sim.similarity(doc))\n",
    "# print([sim.update_sum(term, documents[doc]['vector'].get(term, 0)) for term in query.keys()])\n",
    "s = sum([sim.update_sum(term, documents[doc]['vector'].get(term, 0)) for term in query.keys()])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dog': {'df': 19, 'idf': 0.7178397931503168},\n",
       " 'cat': {'df': 13, 'idf': 1.074514737089049},\n",
       " 'horse': {'df': 20, 'idf': 0.6690496289808848},\n",
       " 'rabit': {'df': 10, 'idf': 1.3156767939059373},\n",
       " 'ostrich': {'df': 6, 'idf': 1.7676619176489945},\n",
       " 'bear': {'df': 8, 'idf': 1.5163474893680884},\n",
       " 'tiger': {'df': 19, 'idf': 0.7178397931503168},\n",
       " 'lion': {'df': 16, 'idf': 0.8803587226480917},\n",
       " 'bird': {'df': 4, 'idf': 2.1041341542702074}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
