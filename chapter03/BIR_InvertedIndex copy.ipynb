{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIR retrieval with inverted files\n",
    "\n",
    "## Helper functions for the BIRModel\n",
    "\n",
    "### Feedback class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedback:\n",
    "    def __init__(self):\n",
    "        self.assessed = set()\n",
    "        self.relevant = set()\n",
    "    \n",
    "    def is_initial_step(self) -> bool:\n",
    "        return len(self.assessed) == 0\n",
    "\n",
    "    def add(self, doc: int, relevant: bool) -> None:\n",
    "        self.assessed.add(doc)\n",
    "        if relevant:\n",
    "            self.relevant.add(doc)\n",
    "        \n",
    "    def is_relevant(self, doc: int) -> bool:\n",
    "        return doc in self.relevant\n",
    "    \n",
    "    def is_assessed(self, doc: int) -> bool:\n",
    "        return doc in self.assessed\n",
    "    \n",
    "    def is_not_relevant(self, doc: int) -> bool:\n",
    "        return (doc in self.assessed) and (doc not in self.relevant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopKList class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappop, heappush, nsmallest\n",
    "\n",
    "class TopKList:\n",
    "    def __init__(self, k: int, term_weights: list[tuple[str,float]]):\n",
    "        self.term_weights = term_weights\n",
    "        self.docs_heap = []\n",
    "        self.k = k\n",
    "        self.terms = [term for term, _ in self.term_weights]\n",
    "        self.weights = dict(self.term_weights)\n",
    "    \n",
    "    def add(self, doc: int, score: float):\n",
    "        heappush(self.docs_heap, (-score, doc, {'id': doc, 'score': score}))\n",
    "        # optional pruning if heap grows too large; be careful to not trigger each time for performance\n",
    "        if len(self.docs_heap) > min(10 * self.k, 100):\n",
    "            self.docs_heap = nsmallest(self.k, self.docs_heap)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(min(self.k, len(self.docs_heap))):\n",
    "            yield heappop(self.docs_heap)[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feedback(text = 'feedback'):\n",
    "    print(text + \":\", \", \".join([('+' if feedback.is_relevant(doc) else '-') + str(doc) for doc in sorted(feedback.assessed, key=lambda doc: (not feedback.is_relevant(doc), doc))]))\n",
    "\n",
    "# helper function to display result and get feedback\n",
    "def display_and_get_feedback(n = 8, all_docs = False):\n",
    "    n = topk.size if n < 0 else n\n",
    "    print(\"\\n  f  r  id score  document\\n-------------------------------------\")\n",
    "    for rank, doc in enumerate(topk):\n",
    "        # let's assume user provides feedback for n not yet assessed documents\n",
    "        if not feedback.is_assessed(doc['id']) and n > 0:\n",
    "            n -= 1\n",
    "            feedback.add(doc['id'], is_relevant(doc['id']))\n",
    "        if rank < 10:\n",
    "            relevancy = '+' if feedback.is_relevant(doc['id']) else '-' if feedback.is_assessed(doc['id']) else ' '\n",
    "            print(\"  {relevancy}{rank: >3d} {id: >3d} ({score:.2f})\".format(rank = rank + 1, relevancy = relevancy, **doc), documents[doc['id']])\n",
    "    if all_docs:\n",
    "        for doc in filter(lambda doc: not feedback.is_assessed(doc), range(1, nDocs + 1)):\n",
    "            if n > 0:\n",
    "                n -= 1\n",
    "                feedback.add(doc, is_relevant(doc))\n",
    "    print()\n",
    "    for term in sorted(topk.weights.keys(), key = lambda term: -topk.weights[term]):\n",
    "        print(term.rjust(16), topk.weights[term])\n",
    "    print_feedback(\"\\nnext feedback\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIR Model Implementation\n",
    "\n",
    "### Base class\n",
    "\n",
    "- pruning of terms\n",
    "- setting weigths based on feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BIRModel:\n",
    "    \"\"\"\n",
    "        Generic class for the evaluation of the BIR model, inherited by the document-at-a-time (DAAT) and \n",
    "        term-at-a-time (TAAT) models. This superclass defines the cj-weights including filtering the most\n",
    "        important terms.\n",
    "    \"\"\"\n",
    "    # set this property to True to remove terms with negative weights\n",
    "    PRUNE_NEGATIVE_WEIGHTS = False\n",
    "\n",
    "    # set this property to remove terms with absolute weights smaller than this value\n",
    "    PRUNE_WEIGHT_THRESHOLD  = False\n",
    "\n",
    "    # set this property to select top-k weights based on absolute values\n",
    "    PRUNE_TOPK = False\n",
    "\n",
    "    # set this property to true to prune non-relevant documents from result list\n",
    "    PRUNE_NON_RELEVANT = False\n",
    "\n",
    "    @staticmethod\n",
    "    def cj_weight(term: str, feedback: Feedback):\n",
    "        docFreq = len(index[term])\n",
    "        if feedback.is_initial_step():\n",
    "            rj = 0.5\n",
    "            nj = (docFreq + 0.5) / (len(documents) + 1)\n",
    "            if DEBUG:\n",
    "                print(term, \"rj=\", rj, \"nj=\", nj, \"cj=\", math.log(rj / (1 - rj) * (1 - nj) / nj))\n",
    "        else:\n",
    "            # get postings as set to siplify calculations in Python\n",
    "            docs = set(index[term])\n",
    "            # number of assessed relevant documents which have the term\n",
    "            lj, L = len(feedback.relevant & docs), len(feedback.relevant)\n",
    "            # number of assessed documents which have the term\n",
    "            kj, K = len(feedback.assessed & docs), len(feedback.assessed)\n",
    "            # calculate rj and nj\n",
    "            rj = (lj + 0.5) / (L + 1)\n",
    "            nj = (kj - lj + 0.5) / (K - L + 1)\n",
    "            if DEBUG:\n",
    "                print(term, \"l=\", lj, \"/\", L, \"k=\", kj, \"/\", K, \"rj=\", rj, \"nj=\", nj, \"cj=\", math.log(rj / (1 - rj) * (1 - nj) / nj))\n",
    "        return math.log(rj / (1 - rj) * (1 - nj) / nj)\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_terms(terms: list[str], feedback: Feedback) -> list[tuple[str,float]]:\n",
    "        # remove terms not in vocabulary\n",
    "        terms = list(filter(lambda t: t in vocabulary, terms))\n",
    "        # calculate weigths and produce tuples (term, weight)\n",
    "        term_weights = list(map(lambda t: (t, BIRModel.cj_weight(t, feedback)), terms))\n",
    "        # filter terms with negative weights\n",
    "        if BIRModel.PRUNE_NEGATIVE_WEIGHTS:\n",
    "            print('pruning negative weights')\n",
    "            term_weights = list(filter(lambda t: t[1] >= 0, term_weights))\n",
    "        # filter terms with small absolute weights\n",
    "        if BIRModel.PRUNE_WEIGHT_THRESHOLD:\n",
    "            print('pruning small weights')\n",
    "            term_weights = list(filter(lambda t: abs(t[1]) > BIRModel.PRUNE_WEIGHT_THRESHOLD, term_weights))\n",
    "        # select top-k terms based on absolute values\n",
    "        if BIRModel.PRUNE_TOPK:\n",
    "            print('pruning top-k weights')\n",
    "            term_weights = sorted(term_weights, key = lambda t: (-abs(t[1]),len(index[t[0]]),t[0]))[:BIRModel.PRUNE_TOPK]\n",
    "        return term_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-at-a-time for BIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel_DAAT(BIRModel):\n",
    "    \"\"\"\n",
    "        Implements the DAAT model for the BIR model using inverted index method.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def query(terms: list[str], k: int, feedback: Feedback):\n",
    "        # filter terms and obtain weights for terms in order of their importance \n",
    "        term_weights = BIRModel.filter_terms(terms, feedback)\n",
    "        \n",
    "        # get iterators for each term and fetch first posting\n",
    "        iters = [iter(index[term]) for (term, _) in term_weights]\n",
    "        nexts = [next(iter, None) for iter in iters]\n",
    "\n",
    "        # keep track of all retrieved documents and their score; stored as tuples (doc_id, score)\n",
    "        topk = TopKList(k, term_weights)\n",
    "        while not all(e is None for e in nexts):\n",
    "            # get smallest value from nexts, ignoring None values\n",
    "            smallest = min(nexts, key = lambda x: x if x is not None else float('inf'))\n",
    "            # if we have feedback, make sure document is either relevant or not assessed so far\n",
    "            if not(BIRModel.PRUNE_NON_RELEVANT and feedback.is_not_relevant(smallest)):\n",
    "                # if so, add it to topk\n",
    "                score = sum([term_weights[i][1] for i in range(len(nexts)) if nexts[i] == smallest])\n",
    "                topk.add(smallest, score)\n",
    "            # for each entry in nexts, fetch next item if entry equals smallest\n",
    "            for i, e in enumerate(nexts):\n",
    "                if e is smallest:\n",
    "                    nexts[i] = next(iters[i], None)\n",
    "        \n",
    "        # finsihed, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-at-a-time for BIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIRModel_TAAT(BIRModel):\n",
    "    \"\"\"\n",
    "        Implements the TAAT model for the BIR model using inverted index method.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def query(terms: list[str], k: int, feedback: Feedback):\n",
    "        # filter terms and obtain weights for terms in order of their importance \n",
    "        term_weights = BIRModel.filter_terms(terms, feedback)\n",
    "        doc_scores = {}\n",
    "\n",
    "        # iterate over terms and fetch postings\n",
    "        for (term, weight) in term_weights:\n",
    "            for posting in index[term]:\n",
    "                # if document is not already in doc_scores, add it\n",
    "                if posting not in doc_scores:\n",
    "                    doc_scores[posting] = 0\n",
    "                # add weight to document score\n",
    "                doc_scores[posting] += weight\n",
    "\n",
    "        # we do not need a full sort of doc_scores, but can use the heap in TopKList\n",
    "        topk = TopKList(k, term_weights)\n",
    "        for doc, score in doc_scores.items():\n",
    "            topk.add(doc, score)\n",
    "        \n",
    "        # finsihed, return topk for result iteration\n",
    "        return topk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random data example\n",
    "### Create inverted index\n",
    "The next section generates random inverted index postings for a set of terms. It simulates the indexing process for Boolean retrieval by associating random document IDs with each term. The `vocabulary` dictionary defines terms and their desired document frequencies (as a %-figure). The generated postings are stored in the `index` dictionary, with each term mapped to a set of corresponding document IDs.\n",
    "\n",
    "* `nDocs = 100`: Defines the total number of documents (document IDs) as 100.\n",
    "* `index = {}`: Initializes an empty dictionary to store the postings for each term.\n",
    "* `DEBUG = False`: A debug flag (we use it later to illustrate code execution).\n",
    "* `vocabulary`: Defines a dictionary where each term is associated with its desired document frequency (expressed as a percentage).\n",
    "* `documents`: List of all documents with each entry holding the set of terms in the document\n",
    "\n",
    "`create_postings(term: str, docFreq: int = None)` takes a term (string) and an optional document frequency (docFreq, integer) as arguments. It generates random postings for the term by creating a set of document IDs. If docFreq is not provided, it generates a random document frequency between 1 and nDocs. The for-loop iterates through each term in the vocabulary dictionary and calls the create_postings function. For each term, it fetches the desired document frequency from the vocabulary (values are percentages) and passes it to create_postings.\n",
    "\n",
    "`is_relevant(doc: int)` returns True if document is relevant ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "DEBUG = False\n",
    "nDocs = 100\n",
    "index = {}\n",
    "documents = []\n",
    "\n",
    "# helper function to rate if newly encountered document is relevant\n",
    "def is_relevant(doc):\n",
    "    # we check if bird or ostrich is present in the document\n",
    "    # if 'horse' in documents[doc]:\n",
    "    #     return random.random() < 0.5\n",
    "    # if 'bird' in documents[doc]:\n",
    "    #     return random.random() < 0.5\n",
    "    # if 'ostrich' in documents[doc]:\n",
    "    #     return random.random() < 1.0\n",
    "    return random.random() < 0.8\n",
    "\n",
    "\n",
    "def create_postings(term: str, docFreq: int = None):\n",
    "    # create random postings for the term for ids between 1 and nDocs\n",
    "    if docFreq is None:\n",
    "        docFreq = random.randint(1, nDocs)\n",
    "    # create sets with random ids\n",
    "    index[term] = sorted(random.sample(range(1, nDocs + 1), docFreq))\n",
    "    # extend feature vectors for documents\n",
    "    for doc in index[term]:\n",
    "        documents[doc].add(term)\n",
    "\n",
    "# define vocabulary and create random postings with given document frequency (in percents)\n",
    "vocabulary = {\n",
    "    'dog':       33,\n",
    "    'cat':       28,\n",
    "    'horse':     18,\n",
    "    'rabit':     20,\n",
    "    'ostrich':   50,\n",
    "    'bear':      13,\n",
    "    'tiger':     12,\n",
    "    'lion':      15,\n",
    "    'bird':      30\n",
    "}\n",
    "\n",
    "# set all feature vectors of documents to empty. We use sets since BIR uses set-of-word model\n",
    "for doc in range(nDocs + 1):\n",
    "    documents.append(set())\n",
    "\n",
    "# call create_postings for each entry in vocabulary to create the inverted index\n",
    "for word in vocabulary:\n",
    "    create_postings(word, vocabulary[word] * nDocs // 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the postings for each term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog        33   [4, 9, 10, 12, 15, 16, 17, 18, 19, 20, 24, 26, 30, 31, 35, 38, 44, 55, 58, 64, 66, 70, 72, 74, 76]\n",
      "cat        28   [2, 5, 7, 13, 17, 26, 31, 32, 41, 42, 43, 44, 46, 48, 50, 53, 56, 64, 72, 77, 83, 84, 89, 90, 94]\n",
      "horse      18   [1, 4, 6, 20, 23, 24, 25, 31, 33, 36, 46, 47, 50, 69, 73, 91, 94, 100]\n",
      "rabit      20   [1, 2, 14, 15, 22, 23, 25, 29, 31, 35, 43, 44, 47, 48, 54, 72, 81, 85, 90, 96]\n",
      "ostrich    50   [1, 3, 5, 9, 10, 11, 18, 19, 20, 21, 22, 23, 24, 28, 29, 30, 32, 33, 35, 36, 39, 40, 45, 48, 49]\n",
      "bear       13   [14, 20, 22, 30, 33, 37, 50, 66, 73, 80, 81, 84, 96]\n",
      "tiger      12   [4, 13, 31, 49, 64, 69, 81, 82, 84, 95, 97, 99]\n",
      "lion       15   [2, 9, 16, 17, 39, 43, 44, 48, 54, 56, 60, 61, 86, 95, 98]\n",
      "bird       30   [1, 4, 9, 13, 20, 22, 24, 25, 26, 28, 29, 34, 35, 36, 38, 39, 48, 54, 56, 57, 60, 65, 67, 69, 71]\n"
     ]
    }
   ],
   "source": [
    "# print postings with term and list of documents\n",
    "for term, posting in index.items():\n",
    "    print(term.ljust(10), str(len(posting)).ljust(4), sorted(posting[:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'ostrich', 'rabit', 'bird', 'horse'}\n",
      "2 {'rabit', 'cat', 'lion'}\n",
      "3 {'ostrich'}\n",
      "4 {'dog', 'bird', 'horse', 'tiger'}\n",
      "5 {'ostrich', 'cat'}\n",
      "6 {'horse'}\n",
      "7 {'cat'}\n",
      "8 set()\n",
      "9 {'dog', 'bird', 'lion', 'ostrich'}\n",
      "10 {'dog', 'ostrich'}\n",
      "11 {'ostrich'}\n",
      "12 {'dog'}\n",
      "13 {'bird', 'cat', 'tiger'}\n",
      "14 {'bear', 'rabit'}\n",
      "15 {'dog', 'rabit'}\n",
      "16 {'dog', 'lion'}\n",
      "17 {'dog', 'cat', 'lion'}\n",
      "18 {'dog', 'ostrich'}\n",
      "19 {'dog', 'ostrich'}\n",
      "20 {'dog', 'bear', 'ostrich', 'horse', 'bird'}\n"
     ]
    }
   ],
   "source": [
    "# print a few documents\n",
    "for doc in range(20):\n",
    "    print(doc + 1, documents[doc + 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Initial step without feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird horse\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  -  1   1 (2.36) {'ostrich', 'rabit', 'bird', 'horse'}\n",
      "  +  2   4 (2.36) {'dog', 'bird', 'horse', 'tiger'}\n",
      "  +  3  20 (2.36) {'dog', 'bear', 'ostrich', 'horse', 'bird'}\n",
      "  +  4  24 (2.36) {'dog', 'bird', 'horse', 'ostrich'}\n",
      "  +  5  25 (2.36) {'rabit', 'bird', 'horse'}\n",
      "  +  6  36 (2.36) {'ostrich', 'bird', 'horse'}\n",
      "  +  7  69 (2.36) {'ostrich', 'bird', 'horse', 'tiger'}\n",
      "  +  8  94 (2.36) {'dog', 'horse', 'bird', 'cat'}\n",
      "     9   6 (1.51) {'horse'}\n",
      "    10  23 (1.51) {'ostrich', 'rabit', 'horse'}\n",
      "\n",
      "           horse 1.5070758997725306\n",
      "            bird 0.8519707660865959\n",
      "\n",
      "next feedback: +4, +20, +24, +25, +36, +69, +94, -1\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# initial step for \"cat dog\"\n",
    "query = ['bird', 'horse']\n",
    "k = 20\n",
    "feedback = Feedback()\n",
    "print(' '.join(query))\n",
    "\n",
    "# set pruning behavior\n",
    "BIRModel.PRUNE_NEGATIVE_WEIGHTS     = False\n",
    "BIRModel.PRUNE_WEIGHT_THRESHOLD     = False\n",
    "BIRModel.PRUNE_TOPK                 = False\n",
    "BIRModel.PRUNE_NON_RELEVANT         = True\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback)\n",
    "display_and_get_feedback()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Feedback step\n",
    "\n",
    "Adjust weights with feedback. Repeat runs (`Ctrl+Enter` to stay on cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bird horse\n",
      "feedback: +4, +6, +9, +20, +22, +23, +24, +25, +29, +31, +35, +36, +39, +46, +47, +50, +69, +73, +91, +94, -1, -13, -26, -28, -33, -34, -38, -100\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1   6 (1.49) {'horse'}\n",
      "  +  2  23 (1.49) {'ostrich', 'rabit', 'horse'}\n",
      "  +  3  31 (1.49) {'dog', 'tiger', 'rabit', 'horse', 'cat'}\n",
      "  +  4  46 (1.49) {'horse', 'cat'}\n",
      "  +  5  47 (1.49) {'rabit', 'horse'}\n",
      "  +  6  50 (1.49) {'bear', 'horse', 'cat'}\n",
      "  +  7  73 (1.49) {'ostrich', 'bear', 'horse'}\n",
      "  +  8  91 (1.49) {'horse'}\n",
      "  +  9   4 (0.92) {'dog', 'bird', 'horse', 'tiger'}\n",
      "  + 10  20 (0.92) {'dog', 'bear', 'ostrich', 'horse', 'bird'}\n",
      "\n",
      "           horse 1.4880770554298333\n",
      "            bird -0.5698489642154517\n",
      "\n",
      "next feedback: +4, +6, +9, +20, +22, +23, +24, +25, +29, +31, +35, +36, +39, +46, +47, +50, +69, +73, +91, +94, -1, -13, -26, -28, -33, -34, -38, -100\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(query))\n",
    "print_feedback()\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback)\n",
    "display_and_get_feedback()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small document example for DAAT and TAAT\n",
    "### Create inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from the exercise\n",
    "nDocs = 0\n",
    "index = {}\n",
    "documents = [set()]\n",
    "vocabulary = {}\n",
    "stopwords = set([\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he',\n",
    "    'i', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 'to', 'was',\n",
    "    'were', 'will', 'with'\n",
    "])\n",
    "\n",
    "# helper function to rate if newly encountered document is relevant\n",
    "def is_relevant(doc):\n",
    "    return doc < 6\n",
    "\n",
    "def add_document(text: str):\n",
    "    global nDocs\n",
    "    nDocs += 1\n",
    "    terms = set()\n",
    "    for term in set(text.lower().split(' ')):\n",
    "        if term in stopwords:\n",
    "            continue\n",
    "        terms.add(term)\n",
    "        if term not in vocabulary:\n",
    "            index[term] = [nDocs]\n",
    "            vocabulary[term] = 1\n",
    "        else:\n",
    "            index[term].append(nDocs)\n",
    "            vocabulary[term] += 1\n",
    "    documents.append(terms)\n",
    "\n",
    "add_document(\"Human machine interface for Lab ABC computer applications\")\n",
    "add_document(\"A survey of user opinion of computer system response time\")\n",
    "add_document(\"The EPS user interface management system\")\n",
    "add_document(\"System and human system engineering testing of EPS\")\n",
    "add_document(\"Relation of user perceived response time to error measurement\")\n",
    "\n",
    "add_document(\"The generation of random binary unordered trees\")\n",
    "add_document(\"The intersection graph of paths in trees\")\n",
    "add_document(\"Graph minors IV Widths of trees and well quasi ordering\")\n",
    "add_document(\"Graph minors a survey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interface      2    [1, 3]\n",
      "human          2    [1, 4]\n",
      "lab            1    [1]\n",
      "machine        1    [1]\n",
      "computer       2    [1, 2]\n",
      "applications   1    [1]\n",
      "abc            1    [1]\n",
      "opinion        1    [2]\n",
      "user           3    [2, 3, 5]\n",
      "system         3    [2, 3, 4]\n",
      "time           2    [2, 5]\n",
      "response       2    [2, 5]\n",
      "survey         2    [2, 9]\n",
      "management     1    [3]\n",
      "eps            2    [3, 4]\n",
      "engineering    1    [4]\n",
      "testing        1    [4]\n",
      "measurement    1    [5]\n",
      "relation       1    [5]\n",
      "error          1    [5]\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice \n",
    "\n",
    "# print postings with term and list of documents\n",
    "for term, posting in islice(index.items(), 20):\n",
    "    print(term.ljust(14), str(len(posting)).ljust(4), sorted(posting[:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 {'interface', 'human', 'machine', 'lab', 'computer', 'applications', 'abc'}\n",
      "2 {'opinion', 'computer', 'user', 'system', 'time', 'response', 'survey'}\n",
      "3 {'interface', 'management', 'user', 'system', 'eps'}\n",
      "4 {'human', 'engineering', 'testing', 'system', 'eps'}\n",
      "5 {'measurement', 'relation', 'user', 'time', 'error', 'response', 'perceived'}\n",
      "6 {'trees', 'unordered', 'random', 'binary', 'generation'}\n",
      "7 {'intersection', 'graph', 'paths', 'trees'}\n",
      "8 {'widths', 'well', 'trees', 'graph', 'minors', 'quasi', 'iv', 'ordering'}\n",
      "9 {'graph', 'minors', 'survey'}\n"
     ]
    }
   ],
   "source": [
    "# print all documents\n",
    "print()\n",
    "for doc in range(len(documents) - 1):\n",
    "    print(doc + 1, documents[doc + 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-at-a-time evaluation\n",
    "\n",
    "### Initial step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human computer interaction\n",
      "pruning negative weights\n",
      "pruning top-k weights\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1   1 (2.45) {'interface', 'human', 'machine', 'lab', 'computer', 'applications', 'abc'}\n",
      "  +  2   2 (1.22) {'opinion', 'computer', 'user', 'system', 'time', 'response', 'survey'}\n",
      "  +  3   4 (1.22) {'human', 'engineering', 'testing', 'system', 'eps'}\n",
      "\n",
      "        computer 1.2237754316221157\n",
      "           human 1.2237754316221157\n",
      "\n",
      "next feedback: +1, +2, +3, +4, +5, -6, -7, -8, -9\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# initial step for \"cat dog\"\n",
    "query = ['human', 'computer', 'interaction']\n",
    "k = 9\n",
    "feedback = Feedback()\n",
    "print(' '.join(query))\n",
    "\n",
    "# set behavior\n",
    "BIRModel.PRUNE_NEGATIVE_WEIGHTS     = True\n",
    "BIRModel.PRUNE_WEIGHT_THRESHOLD     = False\n",
    "BIRModel.PRUNE_TOPK                 = 5\n",
    "BIRModel.PRUNE_NON_RELEVANT         = False\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback)\n",
    "display_and_get_feedback(k, all_docs = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback step\n",
    "\n",
    "Adjust weights with feedback. Repeat runs (`Ctrl+Enter` to stay on cell) + query expansion with terms from relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc applications computer engineering eps error human interaction interface lab machine management measurement opinion perceived relation response survey system testing time user\n",
      "feedback: +1, +2, +3, +4, +5, -6, -7, -8, -9\n",
      "pruning negative weights\n",
      "pruning top-k weights\n",
      "\n",
      "  f  r  id score  document\n",
      "-------------------------------------\n",
      "  +  1   2 (6.93) {'opinion', 'computer', 'user', 'system', 'time', 'response', 'survey'}\n",
      "  +  2   3 (6.93) {'interface', 'management', 'user', 'system', 'eps'}\n",
      "  +  3   4 (6.26) {'human', 'engineering', 'testing', 'system', 'eps'}\n",
      "  +  4   1 (3.72) {'interface', 'human', 'machine', 'lab', 'computer', 'applications', 'abc'}\n",
      "  +  5   5 (2.53) {'measurement', 'relation', 'user', 'time', 'error', 'response', 'perceived'}\n",
      "\n",
      "          system 2.5336968139574325\n",
      "            user 2.5336968139574325\n",
      "        computer 1.8607523407150066\n",
      "             eps 1.8607523407150066\n",
      "           human 1.8607523407150066\n",
      "\n",
      "next feedback: +1, +2, +3, +4, +5, -6, -7, -8, -9\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "print(' '.join(query))\n",
    "print_feedback()\n",
    "\n",
    "# run query, display result, and get feedback\n",
    "topk = BIRModel_DAAT.query(query, k, feedback)\n",
    "display_and_get_feedback(k)\n",
    "\n",
    "query = sorted(set(query) | reduce(lambda terms, doc: terms | documents[doc], feedback.relevant, set()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
