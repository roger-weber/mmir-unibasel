{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "import os, json, re\n",
    "\n",
    "def load_text(book_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Load a book from the Gutenberg project.\n",
    "\n",
    "    :param book_id: The id of the book to load.\n",
    "    :return: The text of the book.\n",
    "    \"\"\"\n",
    "    START_MARKERS = ['*** START OF']\n",
    "    END_MARKERS = ['*** END OF']\n",
    "    print(f'loading text from https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt')\n",
    "    with urlopen(f'https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt') as response:\n",
    "        text = []\n",
    "        # ignore lines up to start markers\n",
    "        for line in response:\n",
    "            line = line.decode(\"utf-8-sig\").strip()\n",
    "            if any(line.startswith(token) for token in START_MARKERS):\n",
    "                break\n",
    "        # add all lines up to end markers\n",
    "        for line in response:\n",
    "            line = line.decode(\"utf-8-sig\").strip()\n",
    "            if any(line.startswith(token) for token in END_MARKERS):\n",
    "                break\n",
    "            text.append(line)\n",
    "    return '\\n'.join(text).strip()\n",
    "\n",
    "def from_cache(book_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Load a book from local cache. If not in cache,\n",
    "    return None.\n",
    "\n",
    "    :param book_id: The id of the book to load.\n",
    "    :return: The text of the book.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(f'books/{book_id}.json'):\n",
    "        return None\n",
    "    with open(f'books/{book_id}.json', 'r') as f:\n",
    "        book = json.load(f)\n",
    "    with open(f'books/{book_id}.txt', 'r') as f:\n",
    "        book['text'] = f.read()\n",
    "    return book\n",
    "\n",
    "def to_cache(book_id: int, book: dict) -> None:\n",
    "    \"\"\"\n",
    "    Write book to local cache.\n",
    "\n",
    "    :param book_id: The id of the book to load.\n",
    "    :return: The text of the book.\n",
    "    \"\"\"\n",
    "    # ensure books folder exists\n",
    "    os.makedirs('books', exist_ok=True)\n",
    "    # dump json of book without property text\n",
    "    text = book['text']\n",
    "    book = book.copy()\n",
    "    del book['text']\n",
    "    # write book to cache\n",
    "    with open(f'books/{book_id}.json', 'w') as f:\n",
    "        json.dump(book, f)\n",
    "    print(f'wrote book to cache: books/{book_id}.json')\n",
    "    with open(f'books/{book_id}.txt', 'w') as f:\n",
    "        f.write(text)\n",
    "    print(f'wrote book to cache: books/{book_id}.txt')\n",
    "\n",
    "def get_book_ids() -> list:\n",
    "    \"\"\"\n",
    "    Load a book from the Gutenberg project.\n",
    "\n",
    "    :return: List of ids in cache\n",
    "    \"\"\"\n",
    "    # ensure books folder exists\n",
    "    os.makedirs('books', exist_ok=True)\n",
    "    \n",
    "    # scan file names from *.txt files from the books folder\n",
    "    return [int(file.split('.')[0]) for file in os.listdir('books') if file.endswith('.json')]\n",
    "\n",
    "def load_book(book_id: int) -> dict:\n",
    "    \"\"\"\n",
    "    Load a book from the Gutenberg project.\n",
    "\n",
    "    :param book_id: The id of the book to load.\n",
    "    :return: The text of the book.\n",
    "    \"\"\"\n",
    "    # read meatdata from page\n",
    "    print(f'loading metadata from https://www.gutenberg.org/ebooks/{book_id}')\n",
    "    with urlopen(f'https://www.gutenberg.org/ebooks/{book_id}') as response:\n",
    "        page = BeautifulSoup(response, 'html.parser')\n",
    "    book = { 'id': book_id }\n",
    "    rows = page.find('table', {'class': 'bibrec'}).find_all('tr')\n",
    "    fields = dict([[row.find('th').get_text().lower().strip(), row.find('td').get_text().strip()] for row in rows if row.find('th')])\n",
    "    a = fields['author'].split(',')\n",
    "    book['yearspan'] = a.pop().strip()\n",
    "    book['author'] = ','.join(a)\n",
    "    book['title'] = fields['title']\n",
    "    book['language'] = fields['language']\n",
    "    book['release_date'] = fields['release date']\n",
    "\n",
    "    # load text from Gutenberg project\n",
    "    book['text'] = load_text(book_id)\n",
    "\n",
    "    # create dictionary with book id as key and book text as value\n",
    "    return book\n",
    "\n",
    "def get_book(book_id: int, reload: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Load a book from the Gutenberg project.\n",
    "\n",
    "    :param book_id: The id of the book to load.\n",
    "    :return: The text of the book.\n",
    "    \"\"\"\n",
    "    # check if book is in cache\n",
    "    if not(reload) and (book := from_cache(book_id)):\n",
    "        return book\n",
    "    # fetch book from Gutenberg project\n",
    "    book = load_book(book_id)\n",
    "    to_cache(book_id, book)\n",
    "    return book\n",
    "\n",
    "book = get_book(244, reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove accents and whitespaces from a string.\n",
    "\n",
    "    :param str: The string to clean.\n",
    "    :return: The cleaned string.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s+', ' ', unidecode(text))\n",
    "\n",
    "text = clean_text(book['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.sent_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "def tokenize_text(text: str):\n",
    "    \n",
    "    # lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punctuation from text\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    \n",
    "    # tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # remove stopwords from txt_tokens and word_tokens\n",
    "    from nltk.corpus import stopwords\n",
    "    english_stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in english_stop_words]\n",
    "    \n",
    "    # return your tokens\n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize_text(text = great_gatsby)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sent_detector.tokenize(text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
