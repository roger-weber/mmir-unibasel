{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence-transformers/all-MiniLM-L6-v2 [(model homepage)](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n",
    "\n",
    "- This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
    "- By default, input text longer than 256 word pieces is truncated. \n",
    "\n",
    "## sentence-transformers/all-mpnet-base-v2 [(model homepage)](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)\n",
    "\n",
    "- This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
    "- By default, input text longer than 384 word pieces is truncated. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length: 256\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Max Sequence Length:\", model.max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 2019, 2742, 6251,  102],\n",
       "        [ 101, 2169, 6251, 2003, 4991,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output[0].size()\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertModel' object has no attribute 'max_seq_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\home\\work\\mmir-unibasel\\chapter04\\embeddings.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/home/work/mmir-unibasel/chapter04/embeddings.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/home/work/mmir-unibasel/chapter04/embeddings.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/home/work/mmir-unibasel/chapter04/embeddings.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMax Sequence Length:\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m.\u001b[39;49mmax_seq_length)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/home/work/mmir-unibasel/chapter04/embeddings.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Tokenize sentences\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/home/work/mmir-unibasel/chapter04/embeddings.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m encoded_input \u001b[39m=\u001b[39m tokenizer(sentences, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\env\\bin\\python3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'max_seq_length'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length: 256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 384])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(\"Max Sequence Length:\", model.max_seq_length)\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\home\\work\\mmir-unibasel\\chapter04\\embeddings.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/home/work/mmir-unibasel/chapter04/embeddings.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m embeddings\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passages: 509663\n",
      "Input question: who is albert einstein?\n",
      "Results (after 0.093 seconds):\n",
      "\t0.769\t['Gene Sharp', 'Gene Sharp (January 21, 1928 – January 28, 2018) was an American political scientist, writer and activist. He was the founder of the Albert Einstein Institution, a non-profit organization dedicated to the study of nonviolent action. He was a retired professor of political science at the University of Massachusetts Dartmouth. He was known for his writings on nonviolent struggle.']\n",
      "\t0.704\t['Albert Einstein', 'Albert Einstein (14 March 1879 – 18 April 1955) was a German-born scientist. He worked on theoretical physics. He developed the theory of relativity. He received the Nobel Prize in Physics in 1921 for theoretical physics. His famous equation is formula_1 (E = energy, m = mass, c = speed of light).']\n",
      "\t0.687\t['Hermann Minkowski', \"Hermann Minkowski (22 June 1864 in Kaunas – 12 January 1909 in Göttingen) was a German mathematician of Jewish descent. He was one of Albert Einstein's teachers.\"]\n",
      "\t0.683\t['Jim Peebles', 'Phillip James Edwin Peebles (born April 25, 1935) is a Canadian-American physicist and theoretical cosmologist. He is currently the Albert Einstein Professor Emeritus of Science at Princeton University.']\n",
      "\t0.661\t['Albert Einstein', 'Einstein lived in Princeton and was one of the first members invited to the Institute for Advanced Study, where he worked for the remainder of his life. He is widely considered one of the greatest scientists of all time. His contributions helped lay the foundations for all modern branches of physics, including quantum mechanics and relativity.']\n",
      "\n",
      "\n",
      "========\n",
      "\n",
      "Input question: what is the capital of france?\n",
      "Results (after 0.026 seconds):\n",
      "\t0.832\t['Capital of France', 'The capital of France is Paris. In the course of history, the national capital has been in many locations other than Paris.']\n",
      "\t0.752\t['Versailles, Yvelines', 'Versailles is a French city. It is in the western suburbs of Paris, 17.1\\xa0km. (10.6 miles) from the center of Paris. It is the \"capital\" of the Yvelines département. This city is very important for the History of France because it was formerly the capital of the kingdom of France.']\n",
      "\t0.738\t['Arrondissement of Sarlat-la-Canéda', 'The arrondissement of Sarlat-la-Canéda is an arrondissement of France. It is part of the Dordogne \"département\" in the Nouvelle-Aquitaine region. Its capital is the city of Sarlat-la-Canéda.']\n",
      "\t0.734\t['Arrondissement of Figeac', 'The arrondissement of Figeac is an arrondissement of France. It is part of the Lot \"département\" in the Occitanie region. Its capital is the city of Figeac.']\n",
      "\t0.731\t['Arrondissement of Confolens', 'The arrondissement of Confolens is an arrondissement of France, in the Charente department, Nouvelle-Aquitaine region. Its capital is the city of Confolens.']\n",
      "\n",
      "\n",
      "========\n",
      "\n",
      "Input question: how many states are there in the united states?\n",
      "Results (after 0.022 seconds):\n",
      "\t0.692\t['U.S. state', 'In the United States, a state is a political entity. There are currently 50 of them, and they are bound together in a federation. Each has a government and can make laws over a particular area that the state covers. All states have a shared sovereignty with the U.S. federal government. People who were born or naturalized in states are citizens of both the United States and the state they are in. People can move between states without asking or saying so, unless a court has said they can\\'t. Four states call themselves \"commonwealths\" in their official names. Washington, D.C. is a federal district and not a state nor in a state.']\n",
      "\t0.686\t['United States', 'The United States conquered and bought new lands over time, and grew from the original 13 colonies in the east to the current 50 states, of which 48 of them are joined together to make up the contiguous United States. These states, called the \"lower 48\", can all be reached by road without crossing a border into another country. They go from the Atlantic east to the Pacific in the west. There are two other states which are not joined to the lower 48 states. Alaska can be reached by passing through British Columbia and the Yukon, both of which are part of Canada. Hawaii is in the middle of the Pacific Ocean.']\n",
      "\t0.678\t['Continental United States', 'The continental United States is the area of the United States of America that is located in the continent of North America. It includes 49 of the 50 states (48 of which are located south of Canada and north of Mexico, known as the \"lower 48 states\", the other being Alaska) and the District of Columbia, which contains the federal capital, Washington, D.C. The only state which is not part of this is Hawaii (as they are islands in the Pacific Ocean and not part of North America).']\n",
      "\t0.672\t['Contiguous United States', 'The contiguous United States, or officially the conterminous United States, are the 48 US states that touch one another plus the District of Columbia.']\n",
      "\t0.634\t['United States', 'At 3.79 million square miles (9.83 million km) and with about 331 million people, the United States is the third or fourth-largest country by total area and third-largest by land area and by population.']\n",
      "\n",
      "\n",
      "========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This examples demonstrates the setup for Question-Answer-Retrieval.\n",
    "\n",
    "You can input a query or a question. The script then uses semantic search\n",
    "to find relevant passages in Simple English Wikipedia (as it is smaller and fits better in RAM).\n",
    "\n",
    "As model, we use: nq-distilbert-base-v1\n",
    "\n",
    "It was trained on the Natural Questions dataset, a dataset with real questions from Google Search\n",
    "together with annotated data from Wikipedia providing the answer. For the passages, we encode the\n",
    "Wikipedia article tile together with the individual text passages.\n",
    "\n",
    "Google Colab Example: https://colab.research.google.com/drive/11GunvCqJuebfeTlgbJWkIMT0xJH6PWF1?usp=sharing\n",
    "\"\"\"\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import time\n",
    "import gzip\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# We use the Bi-Encoder to encode all passages, so that we can use it with sematic search\n",
    "model_name = 'nq-distilbert-base-v1'\n",
    "bi_encoder = SentenceTransformer(model_name)\n",
    "top_k = 5  # Number of passages we want to retrieve with the bi-encoder\n",
    "\n",
    "# As dataset, we use Simple English Wikipedia. Compared to the full English wikipedia, it has only\n",
    "# about 170k articles. We split these articles into paragraphs and encode them with the bi-encoder\n",
    "\n",
    "wikipedia_filepath = 'data/simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "if not os.path.exists(wikipedia_filepath):\n",
    "    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)\n",
    "\n",
    "passages = []\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "        for paragraph in data['paragraphs']:\n",
    "            # We encode the passages as [title, text]\n",
    "            passages.append([data['title'], paragraph])\n",
    "\n",
    "# If you like, you can also limit the number of passages you want to use\n",
    "print(\"Passages:\", len(passages))\n",
    "\n",
    "# To speed things up, pre-computed embeddings are downloaded.\n",
    "# The provided file encoded the passages with the model 'nq-distilbert-base-v1'\n",
    "if model_name == 'nq-distilbert-base-v1':\n",
    "    embeddings_filepath = 'data/simplewiki-2020-11-01-nq-distilbert-base-v1.pt'\n",
    "    if not os.path.exists(embeddings_filepath):\n",
    "        util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01-nq-distilbert-base-v1.pt', embeddings_filepath)\n",
    "\n",
    "    corpus_embeddings = torch.load(embeddings_filepath)\n",
    "    corpus_embeddings = corpus_embeddings.float()  # Convert embedding file to float\n",
    "    if torch.cuda.is_available():\n",
    "        corpus_embeddings = corpus_embeddings.to('cuda')\n",
    "else:  # Here, we compute the corpus_embeddings from scratch (which can take a while depending on the GPU)\n",
    "    corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "\n",
    "queries = [\"who is albert einstein?\", \"what is the capital of france?\", \"how many states are there in the united states?\"]\n",
    "\n",
    "for query in queries:\n",
    "\n",
    "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "    start_time = time.time()\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "    hits = hits[0]  # Get the hits for the first query\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Output of top-k hits\n",
    "    print(\"Input question:\", query)\n",
    "    print(\"Results (after {:.3f} seconds):\".format(end_time - start_time))\n",
    "    for hit in hits:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']]))\n",
    "\n",
    "    print(\"\\n\\n========\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\env\\bin\\python3\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-9.33790863e-01,  2.10345602e+00, -1.84392011e+00, -1.33380818e+00,\n",
       "        6.36020994e+00,  8.07110012e-01,  8.93033981e-01,  2.68119192e+00,\n",
       "       -5.75263977e-01, -4.21091139e-01,  9.52303028e+00,  2.18039960e-01,\n",
       "       -3.30847025e+00,  1.02345204e+00,  9.72467065e-01,  3.37657213e+00,\n",
       "        1.68216801e+00,  7.95359969e-01,  1.38530582e-01, -1.84179977e-01,\n",
       "        1.72705996e+00,  1.18366025e-01, -2.57244730e+00,  1.24378395e+00,\n",
       "       -3.93339872e+00, -1.86945093e+00, -1.11058807e+00, -2.70361042e+00,\n",
       "       -2.12335777e+00,  1.53907204e+00,  6.22535944e-01,  5.59828877e-01,\n",
       "       -2.48922992e+00, -1.79261017e+00, -2.97665620e+00,  2.52682066e+00,\n",
       "        2.08242059e-01,  1.08852196e+00,  4.71816635e+00,  1.33117783e+00,\n",
       "       -1.84098017e+00,  1.22467613e+00,  1.01205993e+00, -8.04393947e-01,\n",
       "       -3.04949999e+00,  2.76379204e+00,  3.52020884e+00, -4.47342014e+00,\n",
       "       -1.01268899e+00, -2.11250022e-01, -1.05838799e+00,  1.79123998e-01,\n",
       "        2.60044074e+00, -3.99453545e+00,  4.42636102e-01, -1.41843945e-01,\n",
       "        1.21468377e+00, -5.20800129e-02,  9.57156345e-02,  9.20384049e-01,\n",
       "        2.37173796e+00, -4.20409292e-01, -6.86004043e-01, -1.78114569e+00,\n",
       "       -6.12850070e-01,  2.03716803e+00, -3.20045996e+00, -5.49385929e+00,\n",
       "        2.67424297e+00,  2.29134798e+00, -1.22854817e+00,  2.26762199e+00,\n",
       "       -4.17020178e+00, -9.14151013e-01, -9.24788117e-01,  1.86098802e+00,\n",
       "       -3.86088896e+00,  2.59430814e+00, -3.47292781e+00, -1.17121980e-01,\n",
       "       -5.69626999e+00, -2.79384971e+00,  9.80202854e-01,  1.63766067e-03,\n",
       "        2.98628378e+00,  2.46989936e-01, -1.80647564e+00, -5.81699967e-01,\n",
       "        5.12574387e+00, -1.73071802e+00, -8.34087074e-01, -2.61602020e+00,\n",
       "        2.78380251e+00, -6.65291977e+00, -8.64794292e-03, -2.99107122e+00,\n",
       "       -1.17130196e+00, -1.39044309e+00, -8.54604125e-01, -2.62980509e+00,\n",
       "        2.24500704e+00,  4.58904177e-01,  5.08854008e+00,  5.40887976e+00,\n",
       "       -2.39749044e-01,  5.03860998e+00,  2.01660299e+00,  3.28143924e-01,\n",
       "       -2.19849777e+00,  7.31424987e-01, -1.20136905e+00,  1.98968279e+00,\n",
       "       -1.46387529e+00,  4.21238184e+00,  6.48738027e-01,  1.09414005e+00,\n",
       "       -2.97839212e+00,  1.87850028e-01,  1.16186976e-01,  9.12447751e-01,\n",
       "       -1.14829600e+00, -3.49112964e+00, -5.30358016e-01,  5.03105831e+00,\n",
       "       -2.00906396e+00, -6.49465036e+00,  1.17133081e+00, -2.12881112e+00,\n",
       "        3.48699999e+00, -1.90830612e+00, -3.90004992e+00,  6.55160099e-02,\n",
       "        5.04400539e+00, -5.56320000e+00, -1.13291788e+00, -4.62799054e-03,\n",
       "        1.73496187e+00, -1.52987027e+00,  5.59811974e+00, -4.57182980e+00,\n",
       "       -1.81711805e+00, -1.16233993e+00,  2.38217020e+00,  1.65748191e+00,\n",
       "        4.15263087e-01,  1.21945143e+00, -5.34012032e+00, -1.37788296e+00,\n",
       "       -1.80402011e-01,  2.92009878e+00,  4.35279936e-01,  4.58111000e+00,\n",
       "        1.39510393e+00,  2.09133768e+00, -5.26492953e-01,  1.44286799e+00,\n",
       "        4.17800045e+00,  2.63974786e+00, -1.61625826e+00, -2.17597008e+00,\n",
       "       -3.71490926e-01, -2.52516890e+00,  1.38771832e+00,  2.57902598e+00,\n",
       "       -2.76889014e+00, -2.82961297e+00, -3.64301753e+00,  5.90032721e+00,\n",
       "        2.01601005e+00,  6.05051041e-01,  3.33949351e+00, -1.24907613e+00,\n",
       "        3.46170998e+00, -2.96799187e-02,  1.12302792e+00, -2.48611212e+00,\n",
       "       -2.78890997e-01,  9.31873977e-01, -2.40613604e+00, -1.07877600e+00,\n",
       "       -1.01105189e+00,  2.07009792e+00,  2.56251192e+00, -1.56206000e+00,\n",
       "       -2.61201811e+00, -4.07054812e-01, -2.19154406e+00, -9.37413692e-01,\n",
       "       -3.66960019e-01,  5.05604029e+00, -2.12132025e+00, -1.04096794e+00,\n",
       "       -8.92610073e-01, -1.69610000e+00,  1.17649794e+00, -7.56880268e-02,\n",
       "       -1.83304000e+00, -2.75370032e-01,  1.79921508e+00, -5.55014908e-01,\n",
       "       -3.66593987e-01, -2.62852621e+00, -2.49695206e+00, -5.43954992e+00,\n",
       "        1.50272906e+00,  1.32038200e+00, -4.67379904e+00,  7.63929963e-01,\n",
       "       -2.68350577e+00, -2.25956297e+00,  3.64588809e+00,  1.90094376e+00,\n",
       "       -1.22368205e+00,  2.82528019e+00,  1.67783988e+00,  6.14329934e-01,\n",
       "       -1.49442911e-01, -2.39145184e+00,  1.76500008e-01,  1.17242002e+00,\n",
       "       -2.97851014e+00, -1.09696698e+00,  2.72218394e+00, -2.70103991e-01,\n",
       "       -3.01842004e-01, -1.25370014e+00,  1.29507899e+00,  1.84372008e+00,\n",
       "        3.39965963e+00, -1.56047094e+00,  1.00044596e+00, -6.78356028e+00,\n",
       "        1.11919188e+00,  2.30287290e+00, -2.05922794e+00,  1.15680003e+00,\n",
       "       -1.39835000e+00,  2.68350035e-01,  8.72567952e-01, -2.92965680e-01,\n",
       "       -7.36329973e-01, -3.39359969e-01,  2.25755978e+00,  2.71503997e+00,\n",
       "       -3.01687813e+00, -4.76749897e-01, -5.01949024e+00,  1.87892592e+00,\n",
       "        1.46814108e+00,  4.03211403e+00,  1.95917964e-01, -1.77390003e+00,\n",
       "       -5.36948013e+00, -2.32375312e+00, -7.44782567e-01, -7.09189057e-01,\n",
       "        2.88299036e+00,  1.89967978e+00,  1.01645589e+00, -9.73350033e-02,\n",
       "       -3.12362015e-01,  5.12876987e+00,  3.03164530e+00,  6.93470001e+00,\n",
       "       -6.00199878e-01, -4.12847042e-01, -5.67631066e-01,  9.39304054e-01,\n",
       "       -5.14120960e+00, -5.15159428e-01,  2.55552816e+00, -2.25015211e+00,\n",
       "       -6.07223988e-01, -2.54644704e+00,  4.48849976e-01,  1.73776019e+00,\n",
       "        1.88284206e+00,  1.49152219e+00,  1.28865406e-01,  2.59383178e+00,\n",
       "       -3.55920017e-01, -1.93496203e+00, -4.33688831e+00,  2.53300309e+00,\n",
       "        7.86952209e+00, -4.39289957e-01,  3.07300425e+00, -3.35322201e-01,\n",
       "       -1.58022404e+00,  2.53625989e+00,  2.51943374e+00,  6.32758737e-01,\n",
       "        2.38783669e+00, -3.67040008e-01, -1.93964019e-01,  2.66991186e+00,\n",
       "        7.71844029e-01, -4.70005989e-01, -2.00422239e+00,  2.83102989e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "tokens = nlp(\"this is a sentence. this is another sentence.\")\n",
    "tokens.vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
