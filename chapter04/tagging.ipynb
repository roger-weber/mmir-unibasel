{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS and NER tagging of dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Project Gutenberg, 244: A Study in Scarlet (en), Arthur Conan Doyle\n",
    "text_en = \"\"\"\n",
    "This was a lofty chamber, lined and littered with countless bottles.\n",
    "Broad, low tables were scattered about, which bristled with retorts,\n",
    "test-tubes, and little Bunsen lamps, with their blue flickering flames.\n",
    "There was only one student in the room, who was bending over a distant\n",
    "table absorbed in his work. At the sound of our steps he glanced round\n",
    "and sprang to his feet with a cry of pleasure. “I’ve found it! I’ve\n",
    "found it,” he shouted to my companion, running towards us with a\n",
    "test-tube in his hand. “I have found a re-agent which is precipitated\n",
    "by hæmoglobin, and by nothing else.” Had he discovered a gold mine,\n",
    "greater delight could not have shone upon his features.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\env\\bin\\python3\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "c:\\env\\bin\\python3\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "c:\\env\\bin\\python3\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc54ab1c0a54aa5a62589187e77b2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\env\\bin\\python3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Roger\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb96d1e913c4c3c97efc889493ba7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vblagoje/bert-english-uncased-finetuned-pos were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db71af080bf64b7c81b920379a5420af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b91de361cbd4798b9e12d73b8a9355d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab21a817df3946c29d961638faefb8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6e346438fc4048bcaacf8470752c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b4b28f869b4447b07f32f49dd6e089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/537M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d68135b7de4953b8ec381296f82a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/335 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2caeb414cd3e46208e16c1fd8b4cd160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/843k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88b1f3545db4e24b09e14bc1ff743f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/bpe.codes:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283f69dde71d409e8fc552d84ea49df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/17.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8e7a124aab4737ba92e10bc4067597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Roger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Roger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "\n",
    "nlp_bert = pipeline(\"token-classification\", model=\"vblagoje/bert-english-uncased-finetuned-pos\", aggregation_strategy=\"max\")\n",
    "nlp_bertweet = pipeline(\"token-classification\", model=\"TweebankNLP/bertweet-tb2_ewt-pos-tagging\", aggregation_strategy=\"simple\")\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp_spacy = spacy.load('en_core_web_sm')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "def add_tokens(lookup: dict, name: str, tagged_tokens: list[tuple[str,str]]):\n",
    "  for token, tag in tagged_tokens:\n",
    "    token = token.replace('@@','').lower()\n",
    "    if not token.isalpha():\n",
    "      continue\n",
    "    if not token in lookup:\n",
    "      lookup[token] = defaultdict(str)\n",
    "      lookup[token]['token'] = token\n",
    "    lookup[token][name] = tag\n",
    "\n",
    "# tagset = None for standard, or tagset = 'universal'\n",
    "def get_pos_nltk(text: str, tagset: str=None):\n",
    "  tokens = nltk.word_tokenize(text_en)\n",
    "  return nltk.pos_tag(tokens, tagset=tagset)\n",
    "\n",
    "def get_pos_spacy(text: str):\n",
    "  tokens = nlp_spacy(text)\n",
    "  return [(token.text, token.pos_) for token in tokens]\n",
    "\n",
    "def get_pos_bert(text: str):\n",
    "  tokens = nlp_bert(text)\n",
    "  return [(token['word'], token['entity_group']) for token in tokens]\n",
    "\n",
    "def get_pos_bertweet(text: str):\n",
    "  tokens = nlp_bertweet(text)\n",
    "  return [(token['word'], token['entity_group']) for token in tokens]\n",
    "\n",
    "\n",
    "lookup = {}\n",
    "add_tokens(lookup, 'nltk(standard)', get_pos_nltk(text_en))\n",
    "add_tokens(lookup, 'nltk(universal)', get_pos_nltk(text_en, 'universal'))\n",
    "add_tokens(lookup, 'spaCy', get_pos_spacy(text_en))\n",
    "add_tokens(lookup, 'BERT', get_pos_bert(text_en))\n",
    "add_tokens(lookup, 'BERTweet', get_pos_bertweet(text_en))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| term       | nltk (pos)   | nltk (universal)   | spaCy   | BERT   | BERTweet   |\n",
       "|:-----------|:-------------|:-------------------|:--------|:-------|:-----------|\n",
       "| this       | DT           | DET                | PRON    | PRON   | PRON       |\n",
       "| was        | VBD          | VERB               | AUX     | AUX    | AUX        |\n",
       "| a          | DT           | DET                | DET     | DET    | DET        |\n",
       "| lofty      | JJ           | ADJ                | ADJ     | ADJ    | ADJ        |\n",
       "| chamber    | NN           | NOUN               | NOUN    | NOUN   | NOUN       |\n",
       "| lined      | VBN          | VERB               | VERB    | VERB   | VERB       |\n",
       "| and        | CC           | CONJ               | CCONJ   | CCONJ  | CCONJ      |\n",
       "| littered   | VBN          | VERB               | VERB    | VERB   | VERB       |\n",
       "| with       | IN           | ADP                | ADP     | ADP    | ADP        |\n",
       "| countless  | JJ           | ADJ                | ADJ     | ADJ    | ADJ        |\n",
       "| bottles    | NNS          | NOUN               | NOUN    | NOUN   | NOUN       |\n",
       "| broad      | NNP          | NOUN               | ADJ     | ADJ    | ADJ        |\n",
       "| low        | JJ           | ADJ                | ADJ     | ADJ    | ADJ        |\n",
       "| tables     | NNS          | NOUN               | NOUN    | NOUN   | NOUN       |\n",
       "| were       | VBD          | VERB               | AUX     | AUX    | AUX        |\n",
       "| scattered  | VBN          | VERB               | VERB    | VERB   | VERB       |\n",
       "| about      | IN           | ADP                | ADP     | ADV    | ADV        |\n",
       "| which      | WDT          | DET                | PRON    | PRON   | PRON       |\n",
       "| bristled   | VBD          | VERB               | VERB    | VERB   | VERB       |\n",
       "| retorts    | NNS          | NOUN               | NOUN    | NOUN   | NOUN       |\n",
       "| little     | JJ           | ADJ                | ADJ     | ADJ    | ADJ        |\n",
       "| bunsen     | NNP          | NOUN               | PROPN   |        | PROPN      |\n",
       "| lamps      | NNS          | NOUN               | NOUN    |        | NOUN       |\n",
       "| their      | PRP$         | PRON               | PRON    | PRON   | PRON       |\n",
       "| blue       | JJ           | ADJ                | ADJ     | ADJ    | ADJ        |\n",
       "| flickering | NN           | NOUN               | NOUN    | VERB   | VERB       |\n",
       "| flames     | NNS          | NOUN               | NOUN    | NOUN   | NOUN       |\n",
       "| there      | EX           | DET                | PRON    | PRON   | PRON       |\n",
       "| only       | RB           | ADV                | ADV     | ADV    | ADV        |\n",
       "| one        | CD           | NUM                | NUM     | NUM    | NUM        |\n",
       "| student    | NN           | NOUN               | NOUN    | NOUN   | NOUN       |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from tabulate import tabulate\n",
    "\n",
    "headers = ['term', 'nltk (pos)', 'nltk (universal)', 'spaCy', 'BERT', 'BERTweet']\n",
    "rows = []\n",
    "\n",
    "for e in lookup.values():\n",
    "  rows.append([e['token'], e['nltk(standard)'],e['nltk(universal)'],e['spaCy'],e['BERT'],e['BERTweet']])\n",
    "  if len(rows) > 30:\n",
    "    break\n",
    "\n",
    "Markdown(tabulate(rows, headers, tablefmt='pipe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| tag (nltk)   |   freq |\n",
       "|:-------------|-------:|\n",
       "| NN           |   6906 |\n",
       "| IN           |   5602 |\n",
       "| DT           |   4620 |\n",
       "| PRP          |   4031 |\n",
       "| VBD          |   3459 |\n",
       "| JJ           |   3070 |\n",
       "| ,            |   2959 |\n",
       "| NNP          |   2558 |\n",
       "| .            |   2433 |\n",
       "| RB           |   2171 |\n",
       "| CC           |   1786 |\n",
       "| VB           |   1730 |\n",
       "| NNS          |   1470 |\n",
       "| PRP$         |   1358 |\n",
       "| VBN          |   1233 |\n",
       "| TO           |   1081 |\n",
       "| VBP          |    881 |\n",
       "| VBG          |    709 |\n",
       "| VBZ          |    678 |\n",
       "| MD           |    602 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| tag (nltk, universal)   |   freq |\n",
       "|:------------------------|-------:|\n",
       "| NOUN                    |  10953 |\n",
       "| VERB                    |   9292 |\n",
       "| PRON                    |   5653 |\n",
       "| ADP                     |   5602 |\n",
       "| .                       |   5550 |\n",
       "| DET                     |   5260 |\n",
       "| ADJ                     |   3234 |\n",
       "| ADV                     |   2520 |\n",
       "| CONJ                    |   1786 |\n",
       "| PRT                     |   1359 |\n",
       "| NUM                     |    345 |\n",
       "| X                       |     49 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| tag (spaCy)   |   freq |\n",
       "|:--------------|-------:|\n",
       "| PUNCT         |   8080 |\n",
       "| NOUN          |   7433 |\n",
       "| PRON          |   6745 |\n",
       "| VERB          |   6025 |\n",
       "| ADP           |   5073 |\n",
       "| DET           |   4451 |\n",
       "| SPACE         |   3844 |\n",
       "| AUX           |   3055 |\n",
       "| ADJ           |   2877 |\n",
       "| ADV           |   2265 |\n",
       "| CCONJ         |   1698 |\n",
       "| PROPN         |   1648 |\n",
       "| SCONJ         |   1449 |\n",
       "| PART          |    993 |\n",
       "| NUM           |    390 |\n",
       "| INTJ          |     70 |\n",
       "| X             |     14 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.gutenberg import get_book\n",
    "from collections import Counter\n",
    "\n",
    "book = get_book(244)\n",
    "\n",
    "tokens = nltk.word_tokenize(book.page_content)\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "freqs = nltk.FreqDist(tag for (word, tag) in tagged_tokens).most_common(20)\n",
    "display(Markdown(tabulate(freqs, [\"tag (nltk)\", \"freq\"], tablefmt='pipe')))\n",
    "\n",
    "tagged_tokens = nltk.pos_tag(tokens, tagset=\"universal\")\n",
    "freqs = nltk.FreqDist(tag for (word, tag) in tagged_tokens).most_common(20)\n",
    "display(Markdown(tabulate(freqs, [\"tag (nltk, universal)\", \"freq\"], tablefmt='pipe')))\n",
    "\n",
    "tokens = nlp_spacy(book.page_content)\n",
    "freqs = nltk.FreqDist(t.pos_ for t in tokens).most_common(20)\n",
    "display(Markdown(tabulate(freqs, [\"tag (spaCy)\", \"freq\"], tablefmt='pipe')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\roger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\roger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,1176.0,168.0\" width=\"1176px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"10.2041%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"40%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Jack</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"60%\" x=\"40%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Higgins</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"5.10204%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.04082%\" x=\"10.2041%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.2245%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.12245%\" x=\"12.2449%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">waering</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBG</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.3061%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.44218%\" x=\"18.3673%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Nike</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"21.0884%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.7619%\" x=\"23.8095%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">shoes</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.1905%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.04082%\" x=\"28.5714%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"29.5918%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.80272%\" x=\"30.6122%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">deposits</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.0136%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.12245%\" x=\"37.415%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">£50,000</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"40.4762%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.08163%\" x=\"43.5374%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">with</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"45.5782%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.52381%\" x=\"47.619%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">BestBank</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"52.381%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.72109%\" x=\"57.1429%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58.5034%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.44218%\" x=\"59.8639%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">London</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.585%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.72109%\" x=\"65.3061%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">at</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.6667%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"10.8844%\" x=\"68.0272%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">FACILITY</text></svg><svg width=\"50%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Jermyn</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"50%\" x=\"50%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Street</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.4694%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.7619%\" x=\"78.9116%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">close</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"81.2925%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.72109%\" x=\"83.6735%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.034%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"13.6054%\" x=\"86.3946%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"60%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Piccadilly</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"40%\" x=\"60%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Circus</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"80%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"93.1973%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('Jack', 'NNP'), ('Higgins', 'NNP')]), (',', ','), ('waering', 'VBG'), Tree('PERSON', [('Nike', 'NNP')]), ('shoes', 'NNS'), (',', ','), ('deposits', 'NNS'), ('£50,000', 'VBP'), ('with', 'IN'), Tree('ORGANIZATION', [('BestBank', 'NNP')]), ('in', 'IN'), Tree('GPE', [('London', 'NNP')]), ('at', 'IN'), Tree('FACILITY', [('Jermyn', 'NNP'), ('Street', 'NNP')]), ('close', 'RB'), ('to', 'TO'), Tree('PERSON', [('Piccadilly', 'NNP'), ('Circus', 'NNP')])])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_ner = \"Jack Higgins, wearing Nike shoes, deposits £50,000 with BestBank in London at Jermyn Street close to Piccadilly Circus\"\n",
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "nlp_bert = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"max\")\n",
    "\n",
    "def add_tokens(lookup: dict, name: str, tagged_tokens: list[tuple[str,str]]):\n",
    "  for token, tag in tagged_tokens:\n",
    "    if not token in lookup:\n",
    "      lookup[token] = defaultdict(str)\n",
    "      lookup[token]['token'] = token\n",
    "    lookup[token][name] = tag\n",
    "\n",
    "def get_ner_nltk(text: str):\n",
    "  tokens = nltk.word_tokenize(text)\n",
    "  tagged_tokens = nltk.pos_tag(tokens)\n",
    "  display(nltk.ne_chunk(tagged_tokens))\n",
    "  ner_chunks = [chunk for chunk in nltk.ne_chunk(tagged_tokens) if hasattr(chunk,'label')]\n",
    "  return [(' '.join(c[0] for c in chunk), chunk.label()) for chunk in ner_chunks]\n",
    "\n",
    "def get_ner_spacy(text: str):\n",
    "  tokens = nlp_spacy(text).ents\n",
    "  return [(t.text, t.label_) for t in tokens]\n",
    "\n",
    "def get_ner_bert(text: str):\n",
    "  tokens = nlp_bert(text_ner)\n",
    "  return [(t['word'], t['entity_group']) for t in tokens]\n",
    "\n",
    "lookup = {}\n",
    "add_tokens(lookup, 'nltk', get_ner_nltk(text_ner))\n",
    "add_tokens(lookup, 'spaCy', get_ner_spacy(text_ner))\n",
    "add_tokens(lookup, 'BERT', get_ner_bert(text_ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| term              | nltk         | spaCy   | BERT   |\n",
       "|:------------------|:-------------|:--------|:-------|\n",
       "| 50,000            |              | MONEY   |        |\n",
       "| BestBank          | ORGANIZATION | ORG     | ORG    |\n",
       "| Jack Higgins      | PERSON       | PERSON  | PER    |\n",
       "| Jermyn Street     | FACILITY     | FAC     | LOC    |\n",
       "| London            | GPE          | GPE     | LOC    |\n",
       "| Nike              | PERSON       | ORG     | MISC   |\n",
       "| Piccadilly Circus | PERSON       | ORG     | LOC    |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = ['term', 'nltk', 'spaCy', 'BERT']\n",
    "rows = []\n",
    "\n",
    "for e in lookup.values():\n",
    "  rows.append([e['token'], e['nltk'],e['spaCy'],e['BERT']])\n",
    "  if len(rows) > 30:\n",
    "    break\n",
    "rows.sort()\n",
    "Markdown(tabulate(rows, headers, tablefmt='pipe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
