{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the text examples\n",
    "\n",
    "We fetch some 'nice' chapters from various books in different languages from project Gutenberg. The diversity will demonstrate some of the challenges when tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "I buy my parents' 10% of U.K. startup for $1.4 billion. Dr. Watson's cat called Mrs. Hersley and it was w.r.o.n.g., more to come ...\n",
    "\"\"\".strip()\n",
    "\n",
    "# Project Gutenberg, 244: A Study in Scarlet (en), Arthur Conan Doyle\n",
    "text_en = \"\"\"\n",
    "This was a lofty chamber, lined and littered with countless bottles.\n",
    "Broad, low tables were scattered about, which bristled with retorts,\n",
    "test-tubes, and little Bunsen lamps, with their blue flickering flames.\n",
    "There was only one student in the room, who was bending over a distant\n",
    "table absorbed in his work. At the sound of our steps he glanced round\n",
    "and sprang to his feet with a cry of pleasure. “I’ve found it! I’ve\n",
    "found it,” he shouted to my companion, running towards us with a\n",
    "test-tube in his hand. “I have found a re-agent which is precipitated\n",
    "by hæmoglobin, and by nothing else.” Had he discovered a gold mine,\n",
    "greater delight could not have shone upon his features.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Project Gutenberg, 34811: Buddenbrooks: Verfall einer Familie (de), Thomas Mann\n",
    "text_de = \"\"\"\n",
    "»Ich rechne«, sagte der Konsul trocken. Die Kerze flammte auf, und man\n",
    "sah, wie er gerade aufgerichtet und mit Augen, so kalt und aufmerksam,\n",
    "wie sie während des ganzen Nachmittags noch nicht darein geschaut\n",
    "hatten, fest in die tanzende Flamme blickte. -- »Einerseits: Sie geben\n",
    "33335 an Gotthold und 15000 an die in Frankfurt, und das macht 48335 in\n",
    "Summa. Andererseits: Sie geben nur 25000 an die in Frankfurt, und das\n",
    "bedeutet für die Firma einen Gewinn von 23335. Das ist aber nicht alles.\n",
    "Gesetzt, Sie leisten an Gotthold eine Entschädigungssumme für den Anteil\n",
    "am Hause, so ist das Prinzip durchbrochen, so ist er damals =nicht=\n",
    "endgültig abgefunden worden, so kann er nach Ihrem Tode ein gleich\n",
    "großes Erbe beanspruchen, wie meine Schwester und ich, und dann handelt\n",
    "es sich für die Firma um einen Verlust von Hunderttausenden, mit dem sie\n",
    "nicht rechnen kann, mit dem ich als künftiger alleiniger Inhaber nicht\n",
    "rechnen kann ... Nein, Papa!« beschloß er mit einer energischen\n",
    "Handbewegung und richtete sich noch höher auf. »Ich muß Ihnen abraten,\n",
    "nachzugeben!«\n",
    "\"\"\".strip()\n",
    "\n",
    "# Project Gutenberg, 13951: Les trois mousquetaires (fr), Alexandre Dumas\n",
    "text_fr = \"\"\"\n",
    "D’Artagnan, tout en marchant et en monologuant, était arrivé à quelques\n",
    "pas de l’hôtel d’Aiguillon, et devant cet hôtel il avait aperçu Aramis\n",
    "causant gaiement avec trois gentilshommes des gardes du roi. De son\n",
    "côté, Aramis aperçut d’Artagnan; mais comme il n’oubliait point que\n",
    "c’était devant ce jeune homme que M. de Tréville s’était si fort\n",
    "emporté le matin, et qu’un témoin des reproches que les mousquetaires\n",
    "avaient reçus ne lui était d’aucune façon agréable, il fit semblant de\n",
    "ne pas le voir. D’Artagnan, tout entier au contraire à ses plans de\n",
    "conciliation et de courtoisie, s’approcha des quatre jeunes gens en\n",
    "leur faisant un grand salut accompagné du plus gracieux sourire. Aramis\n",
    "inclina légèrement la tête, mais ne sourit point. Tous quatre, au\n",
    "reste, interrompirent à l’instant même leur conversation.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Project Gutenberg, 27729: Bajki (pl), Adam Mickiewicz\n",
    "text_pl = \"\"\"\n",
    "Powolny bóg wszechżabstwu na króla użycza\n",
    "Małego jako Łokiet Kija Kijowicza.\n",
    "Spadł Kij i pluskiem wszemu obwieścił się błotu.\n",
    "Struchlały żaby na ten majestat łoskotu.\n",
    "Milczą, dzień i noc, ledwie śmiejąc dychać,\n",
    "Nazajutrz jedna drugiej pytają: „Co słychać?\n",
    "Czy niema co od króla?” Aż śmielsze i starsze\n",
    "Ruszają przed oblicze stawić się monarsze.\n",
    "Zrazu zdala, w bojaźni, by się nie narazić;\n",
    "Potem, przemógłszy te strachy,\n",
    "Brat za brat z królem biorą się pod pachy\n",
    "I zaczynają na kark mu włazić.\n",
    "„Toż to taki ma być król?... Najjaśniejszy Bela,\n",
    "Nie wiele z niego będziem mieć wesela;\n",
    "Król, co po karku bezkarnie go gładzim,\n",
    "Niechaj nam abdykuje zaraz, niedołęga!\n",
    "Potrzebna nam jest władza, ale władza tęga!”\n",
    "\"\"\".strip()\n",
    "\n",
    "# Project Gutenberg, 23585: 佛說四十二章經 (zh)\n",
    "text_zh=\"\"\"\n",
    "沙門夜誦迦葉佛遺教經，其聲悲緊，思悔欲退。佛問之曰：汝昔在家，曾為何業？對\n",
    "曰：愛彈琴。佛言：弦緩如何？對曰：不鳴矣！弦急如何？對曰：聲絕矣！急緩得中\n",
    "如何？對曰：諸音普矣！佛言：沙門學道亦然，心若調適，道可得矣。於道若暴，暴\n",
    "即身疲。其身若疲，意即生惱。意若生惱，行即退矣。其行既退，罪必加矣。但清淨\n",
    "安樂，道不失矣。\n",
    "\"\"\".strip()\n",
    "\n",
    "texts = {\n",
    "    'abbreviations': text,\n",
    "    'english': text_en,\n",
    "    'german': text_de,\n",
    "    'french': text_fr,\n",
    "    'polish': text_pl,\n",
    "    'chinese': text_zh\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word based tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 9.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\env\\python310\\lib\\site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (63.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\env\\python310\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\env\\python310\\lib\\site-packages (from packaging>=20.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\env\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\env\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\env\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\env\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\env\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\env\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\env\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2022.9.24)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\env\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\env\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\env\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\env\\python310\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\env\\python310\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.6.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\roger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re, nltk, jieba, spacy\n",
    "nltk.download('punkt')\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def jieba_word(text: str, language: str) -> list[str]:\n",
    "    return [t[0] for t in jieba.tokenize(text) if t[0] != ' ']\n",
    "\n",
    "def python_word(text: str, language: str) -> list[str]:\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    return [token for token in text.split(' ') if token]\n",
    "\n",
    "def nltk_word(text: str, language: str) -> list[str]:\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def spacy_word(text: str, language: str) -> list[str]:\n",
    "    global nlp\n",
    "    return [token.text for token in nlp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='7'>\n"
     ]
    }
   ],
   "source": [
    "print(re.match(r'[\\w\\-]','7h3hj.,,'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1d510ad1704c38a68ca0c806f0f9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='language', options=('abbreviations', 'english', 'french', 'german'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf09a9bca964494816d574a03cb1623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from tabulate import tabulate\n",
    "from itertools import zip_longest\n",
    "from unidecode import unidecode\n",
    "\n",
    "def show_tokens(language: str, alpha: bool, unidecode: bool):\n",
    "    tokens = []\n",
    "    for tokenizer in tokenizers.values():\n",
    "        t = tokenizer(texts[language], language)\n",
    "        if alpha:\n",
    "            t = [token for token in t if re.match(r'[\\w\\-]*', token)]\n",
    "        if unidecode:\n",
    "            t = [unidecode(token) for token in t]\n",
    "        tokens.append(t)\n",
    "    with (out_text := widgets.Output()):\n",
    "        display(Markdown(texts[language]))\n",
    "    with (out_tokens := widgets.Output(layout = {'padding': '0px 50px', 'min_width': '60%'})):\n",
    "        if language == 'chinese':\n",
    "            for i in range(len(tokens)):\n",
    "                nl = '\\n  '\n",
    "                print(f'{list(tokenizers.keys())[i]}:\\n  {nl.join(tokens[i][:10])}\\n\\n')\n",
    "        else:\n",
    "            headers = tokenizers.keys()\n",
    "            rows = [[col[:16] for col in row] for row in zip_longest(*tokens, fillvalue='')]\n",
    "            print(tabulate(rows[:40], headers, tablefmt=\"github\"))\n",
    "    with out_result:\n",
    "        clear_output()\n",
    "        display(widgets.HBox([out_text, out_tokens]))\n",
    "\n",
    "opt_alpha = widgets.Checkbox(description='only words')\n",
    "opt_unidecode = widgets.Checkbox(description='unicode decode')\n",
    "opt_language = widgets.Dropdown(description='language', options=['abbreviations', 'english', 'french', 'german', 'polish', 'chinese'])\n",
    "opt_method = widgets.Dropdown(description='method', options=[\n",
    "    ('python (split on whitespace)', 'python-split'),\n",
    "    ('nltk (word)', 'nltk-word'),\n",
    "    ('spaCy (word)', 'spacy-word'),\n",
    "    ('jieba (chinese)', 'jieba-word'),\n",
    "])\n",
    "tokenizers = {\n",
    "    'python-word': python_word,\n",
    "    'nltk-word': nltk_word,\n",
    "    'spacy-word': spacy_word,\n",
    "    'jieba-word': jieba_word,\n",
    "}\n",
    "\n",
    "out_result = widgets.Output()\n",
    "display(widgets.interactive(show_tokens, language=opt_language, alpha=opt_alpha, unidecode=opt_unidecode))\n",
    "display(out_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_words_tokenize(text: str, k: int) -> list[str]:\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if len(token) <= k:\n",
    "            tokens.append(token)\n",
    "            continue\n",
    "        for i in range(len(token) - k + 1):\n",
    "            tokens.append(token[i:i + k])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ebd3c046fda4074848a0b8851524207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='language', options=('abbreviations', 'english', 'french', 'german'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199540abff3240e99dad922f19ccbed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_K = 11\n",
    "\n",
    "def show_sub_words(language):\n",
    "    tokens = []\n",
    "    for k in range(1, MAX_K, 1):\n",
    "        tokens.append(sub_words_tokenize(texts[language], k))\n",
    "    with (out_text := widgets.Output()):\n",
    "        display(Markdown(texts[language]))\n",
    "    with (out_tokens := widgets.Output(layout = {'padding': '0px 50px', 'min_width': '60%'})):\n",
    "        headers = [str(i) for i in range(1, MAX_K, 1)]\n",
    "        rows = [[f'#{len(t)}' for t in tokens]] + [row for row in zip_longest(*tokens, fillvalue='')]\n",
    "        print(tabulate(rows[:30], headers, tablefmt=\"github\"))\n",
    "    with out_result:\n",
    "        clear_output()\n",
    "        display(widgets.HBox([out_text, out_tokens]))\n",
    "\n",
    "opt_language = widgets.Dropdown(description='language', options=['abbreviations', 'english', 'french', 'german', 'polish', 'chinese'])\n",
    "\n",
    "out_result = widgets.Output()\n",
    "display(widgets.interactive(show_sub_words, language=opt_language))\n",
    "display(out_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-gram extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.gutenberg import get_book\n",
    "book = get_book(244)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive approach: create bi-grams, count frequencies in text, then pick top-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 297),\n",
       " (('in', 'the'), 186),\n",
       " (('to', 'the'), 135),\n",
       " (('to', 'be'), 96),\n",
       " (('and', 'the'), 89),\n",
       " (('upon', 'the'), 88),\n",
       " (('I', 'have'), 81),\n",
       " (('that', 'I'), 76),\n",
       " (('at', 'the'), 75),\n",
       " (('he', 'had'), 73),\n",
       " (('that', 'he'), 73),\n",
       " (('he', 'said'), 72),\n",
       " (('of', 'his'), 69),\n",
       " (('I', 'had'), 63),\n",
       " (('had', 'been'), 63),\n",
       " (('of', 'a'), 63),\n",
       " (('from', 'the'), 60),\n",
       " (('in', 'his'), 59),\n",
       " (('on', 'the'), 58),\n",
       " (('was', 'a'), 57)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk, collections\n",
    "\n",
    "tokens = [token for token in nltk.word_tokenize(book.page_content) if token.isalpha()]\n",
    "counters = collections.Counter(nltk.bigrams(tokens))\n",
    "counters.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/drweb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 297),\n",
       " (('in', 'the'), 186),\n",
       " (('to', 'the'), 135),\n",
       " (('to', 'be'), 96),\n",
       " (('and', 'the'), 89),\n",
       " (('upon', 'the'), 88),\n",
       " (('I', 'have'), 81),\n",
       " (('that', 'I'), 76),\n",
       " (('at', 'the'), 75),\n",
       " (('he', 'had'), 73),\n",
       " (('that', 'he'), 73),\n",
       " (('he', 'said'), 72),\n",
       " (('of', 'his'), 69),\n",
       " (('I', 'had'), 63),\n",
       " (('had', 'been'), 63),\n",
       " (('of', 'a'), 63),\n",
       " (('from', 'the'), 60),\n",
       " (('in', 'his'), 59),\n",
       " (('on', 'the'), 58),\n",
       " (('was', 'a'), 57)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "tokens = [token for token in nltk.word_tokenize(book.page_content) if token.isalpha()]\n",
    "counters = collections.Counter(nltk.bigrams(tokens))\n",
    "counters.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find bi-grams that occur more often together than alone --> PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aboard', 'union'),\n",
       " ('ac', 'nummos'),\n",
       " ('accident', 'design'),\n",
       " ('accordingly', 'troopship'),\n",
       " ('accuracy', 'subjoined'),\n",
       " ('accursed', 'shameless'),\n",
       " ('accused', 'lukewarmness'),\n",
       " ('acres', 'evermore'),\n",
       " ('actions', 'directed'),\n",
       " ('additions', 'succeeding'),\n",
       " ('admired', 'treated'),\n",
       " ('advisability', 'allowing'),\n",
       " ('advocating', 'closer'),\n",
       " ('affectation', 'lounged'),\n",
       " ('agent', 'marking'),\n",
       " ('agree', 'beforehand'),\n",
       " ('airy', 'cheerfully'),\n",
       " ('alertness', 'decision'),\n",
       " ('ambitious', 'title'),\n",
       " ('anatomy', 'chemist')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "tokens = [token.lower() for token in nltk.word_tokenize(book.page_content) if token.isalpha() and token not in stopwords.words('english')]\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "finder.nbest(bigram_measures.pmi, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lauriston', 'gardens'),\n",
       " ('salt', 'lake'),\n",
       " ('scotland', 'yard'),\n",
       " ('halliday', 'private'),\n",
       " ('lake', 'city'),\n",
       " ('private', 'hotel'),\n",
       " ('baker', 'street'),\n",
       " ('brixton', 'road'),\n",
       " ('sprang', 'feet'),\n",
       " ('in', 'meantime'),\n",
       " ('jefferson', 'hope'),\n",
       " ('enoch', 'drebber'),\n",
       " ('old', 'farmer'),\n",
       " ('john', 'rance'),\n",
       " ('joseph', 'stangerson'),\n",
       " ('john', 'ferrier'),\n",
       " ('sherlock', 'holmes'),\n",
       " ('no', 'doubt'),\n",
       " ('young', 'hunter'),\n",
       " ('as', 'spoke')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.apply_freq_filter(5)\n",
    "finder.nbest(bigram_measures.pmi, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
